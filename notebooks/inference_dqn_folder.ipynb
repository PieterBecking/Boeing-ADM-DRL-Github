{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m env_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latest:\n\u001b[0;32m--> 203\u001b[0m     MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../trained_models/dqn/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_3ac-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../trained_models/dqn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menv_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_3ac-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../trained_models/dqn/_perfect_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_3ac-2.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.visualizations import StatePlotter\n",
    "from scripts.utils import load_scenario_data, get_training_metadata, NumpyEncoder\n",
    "from src.config import *\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image as IPImage\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.logger import create_new_id, log_inference_metadata, log_inference_scenario_data, find_corresponding_training_id, get_config_variables, convert_to_serializable, update_id_status\n",
    "import src.config as config\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def run_inference_dqn_single(model_path, scenario_folder, env_type, seed, inference_id):\n",
    "    \"\"\"\n",
    "    Runs inference on a single scenario and logs detailed results.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "        scenario_folder (str): Path to the scenario folder.\n",
    "        env_type (str): Type of environment (\"myopic\" or \"proactive\").\n",
    "        seed (int): Seed for reproducibility.\n",
    "        inference_id (str): Unique ID for the inference session.\n",
    "    \"\"\"\n",
    "    # Load scenario data\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, flights_dict, rotations_dict, alt_aircraft_dict, config_dict, env_type=env_type\n",
    "    )\n",
    "\n",
    "    # Load trained model and configure\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "\n",
    "    # Initialize visualization\n",
    "    state_plotter = StatePlotter(\n",
    "        aircraft_dict=env.aircraft_dict,\n",
    "        flights_dict=env.flights_dict,\n",
    "        rotations_dict=env.rotations_dict,\n",
    "        alt_aircraft_dict=env.alt_aircraft_dict,\n",
    "        start_datetime=env.start_datetime,\n",
    "        end_datetime=env.end_datetime,\n",
    "        uncertain_breakdowns=env.uncertain_breakdowns,\n",
    "    )\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    # Scenario log\n",
    "    scenario_log = {\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        \"steps\": [],\n",
    "        \"total_reward\": 0,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        # Step-level pre-action info\n",
    "        step_info_before_action = {\n",
    "            \"num_cancelled_flights\": len(env.cancelled_flights),\n",
    "            \"num_delayed_flights\": len(env.environment_delayed_flights),\n",
    "            \"num_resolved_conflicts\": len(env.resolved_conflicts),\n",
    "            \"current_datetime\": env.current_datetime.isoformat(),\n",
    "        }\n",
    "\n",
    "        # Action selection\n",
    "        action_mask = obs['action_mask']\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "        action = np.argmax(masked_q_values)\n",
    "\n",
    "        if action_mask[action] == 0:\n",
    "            raise ValueError(f\"Invalid action selected: {action}\")\n",
    "\n",
    "        # Environment step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done_flag = terminated or truncated\n",
    "        action_mapped = env.map_index_to_action(action)\n",
    "\n",
    "        # Log step data\n",
    "        step_log = {\n",
    "            \"step_num\": step_num,\n",
    "            \"action\": action,\n",
    "            \"flight_action\": action_mapped[0],\n",
    "            \"aircraft_action\": action_mapped[1],\n",
    "            \"reward\": reward,\n",
    "            \"total_reward\": total_reward,\n",
    "            \"q_values\": q_values.tolist(),\n",
    "            \"masked_q_values\": masked_q_values.tolist(),\n",
    "            \"action_mask\": action_mask.tolist(),\n",
    "            \"done_flag\": done_flag,\n",
    "            \"info_after_step\": convert_to_serializable(env.info_after_step),\n",
    "            \"step_info_before_action\": step_info_before_action,\n",
    "        }\n",
    "\n",
    "        scenario_log[\"steps\"].append(step_log)\n",
    "        step_num += 1\n",
    "\n",
    "    scenario_log[\"total_reward\"] = total_reward\n",
    "    scenario_log[\"runtime_end\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    # Log scenario data\n",
    "    log_inference_scenario_data(inference_id, scenario_log)\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Steps Taken: {step_num}\")\n",
    "    return scenario_log\n",
    "\n",
    "\n",
    "def run_inference_dqn_folder(model_path, scenario_folder, env_type, seed):\n",
    "    \"\"\"\n",
    "    Runs inference on all scenarios in a folder.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "        scenario_folder (str): Path to the folder containing scenarios.\n",
    "        env_type (str): Type of environment (\"myopic\" or \"proactive\").\n",
    "        seed (int): Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    inference_id = create_new_id(\"inference\")\n",
    "    runtime_start = time.time()\n",
    "\n",
    "    # Load training metadata\n",
    "    training_id = find_corresponding_training_id(model_path, env_type)\n",
    "    training_logs_path = f\"../logs/training/training_{training_id}.json\"\n",
    "    training_metadata = get_training_metadata(training_logs_path, env_type)\n",
    "\n",
    "    # Metadata for inference\n",
    "    inference_metadata = {\n",
    "        \"inference_id\": inference_id,\n",
    "        \"model_path\": model_path,\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        \"training_metadata\": training_metadata,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "    log_inference_metadata(inference_id, inference_metadata)\n",
    "\n",
    "    complete_inference_log = {\n",
    "        \"inference_id\": inference_id,\n",
    "        \"runtime_start\": inference_metadata[\"runtime_start\"],\n",
    "        \"runtime_end\": None,\n",
    "        \"scenarios\": {},\n",
    "    }\n",
    "\n",
    "    for scenario in os.listdir(scenario_folder):\n",
    "        scenario_path = os.path.join(scenario_folder, scenario)\n",
    "        if os.path.isdir(scenario_path):\n",
    "            scenario_log = run_inference_dqn_single(model_path, scenario_path, env_type, seed, inference_id)\n",
    "            complete_inference_log[\"scenarios\"][scenario] = scenario_log\n",
    "\n",
    "    complete_inference_log[\"runtime_end\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "    log_file_path = os.path.join(\"../logs\", \"inference\", f\"inference_{inference_id}.json\")\n",
    "\n",
    "    # Save complete inference log\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        json.dump(complete_inference_log, log_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "    # Mark as done in ids.json\n",
    "    update_id_status(inference_id, \"finished\")\n",
    "\n",
    "    print(f\"Inference log saved to {log_file_path}\")\n",
    "\n",
    "\n",
    "# Main logic to run inference\n",
    "latest = True\n",
    "env_type = \"proactive\"\n",
    "\n",
    "if latest:\n",
    "    MODEL_PATH = f\"../trained_models/dqn/{env_type}_3ac-{max(int(model.split('-')[1].split('.')[0]) for model in os.listdir('../trained_models/dqn') if model.startswith(f'{env_type}_3ac-'))}.zip\"\n",
    "else:\n",
    "    MODEL_PATH = f\"../trained_models/dqn/_perfect_{env_type}_3ac-2.zip\"\n",
    "\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "\n",
    "seed = int(time.time())\n",
    "SCENARIO_FOLDER = \"../data/Locked/alpha/\"\n",
    "print(f\"Environment Type: {env_type}\")\n",
    "\n",
    "run_inference_dqn_folder(MODEL_PATH, SCENARIO_FOLDER, env_type, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
