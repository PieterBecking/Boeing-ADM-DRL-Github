{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path: ../trained_models/dqn/proactive_3ac-25.zip\n",
      "Environment Type: proactive\n",
      "Training ID: 0103\n",
      "Training Config Variables: {'myopic_or_proactive': 'proactive', 'model_type': 'dqn', 'training_id': '0103', 'MODEL_SAVE_PATH': '../trained_models/dqn/', 'N_EPISODES': 10, 'num_scenarios_training': 100, 'results_dir': '../results/dqn/20241128-16-26', 'CROSS_VAL_FLAG': 1, 'CROSS_VAL_INTERVAL': 2, 'np': \"<module 'numpy' from '/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/numpy/__init__.py'>\", 'MAX_AIRCRAFT': 3, 'MAX_FLIGHTS_PER_AIRCRAFT': 12, 'ROWS_STATE_SPACE': 4, 'COLUMNS_STATE_SPACE': 39, 'ACTION_SPACE_SIZE': 52, 'DEPARTURE_AFTER_END_RECOVERY': 1, 'BREAKDOWN_PROBABILITY': 0.9, 'BREAKDOWN_DURATION': 517.9099155552203, 'TIMESTEP_HOURS': 1, 'DUMMY_VALUE': -999, 'RESOLVED_CONFLICT_REWARD': 10000, 'DELAY_MINUTE_PENALTY': 11.5, 'MAX_DELAY_PENALTY': 750, 'NO_ACTION_PENALTY': 0, 'CANCELLED_FLIGHT_PENALTY': 1000, 'LAST_MINUTE_THRESHOLD': 120, 'LAST_MINUTE_FLIGHT_PENALTY': 455, 'AHEAD_BONUS_PER_MINUTE': 0.1, 'TIME_MINUTE_PENALTY': 1, 'MIN_TURN_TIME': 0, 'MIN_BREAKDOWN_PROBABILITY': 0, 'DEBUG_MODE': 0, 'DEBUG_MODE_TRAINING': 0, 'DEBUG_MODE_REWARD': 0, 'DEBUG_MODE_PRINT_STATE': 0, 'DEBUG_MODE_CANCELLED_FLIGHT': 0, 'DEBUG_MODE_VISUALIZATION': 0, 'DEBUG_MODE_BREAKDOWN': 0, 'DEBUG_MODE_ACTION': 0, 'DEBUG_MODE_STOPPING_CRITERIA': 0, 'DEBUG_MODE_SCHEDULING': 0, 'DEBUG_MODE_REWARD_LAST_MINUTE_PENALTY': 0, 'LEARNING_RATE': 0.001, 'GAMMA': 0.99, 'BUFFER_SIZE': 100000, 'BATCH_SIZE': 256, 'TARGET_UPDATE_INTERVAL': 100, 'EPSILON_START': 1.0, 'EPSILON_MIN': 0.025, 'EPSILON_DECAY_RATE': 0.0008164850496046783, 'EXPLORATION_PHASE': 250, 'MAX_TIMESTEPS': 50000, 'LEARNING_STARTS': 1000, 'TRAIN_FREQ': 4, 'NEURAL_NET_STRUCTURE': {'net_arch': [256, 512, 512, 256]}, 'device_info': \"{'device_type': 'MacBook M1'}\", 'TRAINING_FOLDERS_PATH': '../data/Training/3ac-100/', 'TESTING_FOLDERS_PATH': '../data/Training/3ac-100/', 'runtime_start': '2024-11-28T15:27:18.632373Z', 'runtime_start_in_seconds': 1732807638.6323211}\n",
      "Inference Config Variables: {'np': \"<module 'numpy' from '/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/numpy/__init__.py'>\", 'MAX_AIRCRAFT': 3, 'MAX_FLIGHTS_PER_AIRCRAFT': 12, 'ROWS_STATE_SPACE': 4, 'COLUMNS_STATE_SPACE': 39, 'ACTION_SPACE_SIZE': 52, 'DEPARTURE_AFTER_END_RECOVERY': 1, 'BREAKDOWN_PROBABILITY': 0.9, 'BREAKDOWN_DURATION': 513.9644936561494, 'TIMESTEP_HOURS': 1, 'DUMMY_VALUE': -999, 'RESOLVED_CONFLICT_REWARD': 10000, 'DELAY_MINUTE_PENALTY': 11.5, 'MAX_DELAY_PENALTY': 750, 'NO_ACTION_PENALTY': 0, 'CANCELLED_FLIGHT_PENALTY': 1000, 'LAST_MINUTE_THRESHOLD': 120, 'LAST_MINUTE_FLIGHT_PENALTY': 455, 'AHEAD_BONUS_PER_MINUTE': 0.1, 'TIME_MINUTE_PENALTY': 1, 'MIN_TURN_TIME': 0, 'MIN_BREAKDOWN_PROBABILITY': 0, 'DEBUG_MODE': 0, 'DEBUG_MODE_TRAINING': 0, 'DEBUG_MODE_REWARD': 0, 'DEBUG_MODE_PRINT_STATE': 0, 'DEBUG_MODE_CANCELLED_FLIGHT': 0, 'DEBUG_MODE_VISUALIZATION': 0, 'DEBUG_MODE_BREAKDOWN': 0, 'DEBUG_MODE_ACTION': 0, 'DEBUG_MODE_STOPPING_CRITERIA': 0, 'DEBUG_MODE_SCHEDULING': 0, 'DEBUG_MODE_REWARD_LAST_MINUTE_PENALTY': 0}\n",
      "No conflicting variables found between training and current config!\n",
      "Inference metadata logged to ../logs/inference/inference_0133.json\n",
      "seed: 1732809955\n",
      "Step 0:\n",
      "action index:\n",
      "10\n",
      "action mapped:\n",
      "(2, 2)\n",
      "Action taken: (2, 2), Reward: -784.3\n",
      "Step 1:\n",
      "action index:\n",
      "27\n",
      "action mapped:\n",
      "(6, 3)\n",
      "Action taken: (6, 3), Reward: 8184.5\n",
      "Step 2:\n",
      "action index:\n",
      "39\n",
      "action mapped:\n",
      "(9, 3)\n",
      "Action taken: (9, 3), Reward: -180.0\n",
      "Step 3:\n",
      "action index:\n",
      "22\n",
      "action mapped:\n",
      "(5, 2)\n",
      "Action taken: (5, 2), Reward: 9023.6\n",
      "================================================\n",
      "Final state:\n",
      "Total Reward: 16243.8\n",
      "Total Steps: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cb656002c047a0a237507cf3f7e0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Step:', max=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b870950583d74a53b5c786fb5bcc775d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'inference_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 343\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Run the fixed inference loop\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[43mrun_inference_dqn_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCENARIO_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 308\u001b[0m, in \u001b[0;36mrun_inference_dqn_folder\u001b[0;34m(model_path, scenario_folder, env_type, seed)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 308\u001b[0m \u001b[43mrun_inference_dqn_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 257\u001b[0m, in \u001b[0;36mrun_inference_dqn_single\u001b[0;34m(model_path, scenario_folder, env_type, seed)\u001b[0m\n\u001b[1;32m    255\u001b[0m display(slider, output)\n\u001b[1;32m    256\u001b[0m scenario_log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m total_reward\n\u001b[0;32m--> 257\u001b[0m log_inference_scenario_data(\u001b[43minference_id\u001b[49m, scenario_log)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward, step_num\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inference_id' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.visualizations import StatePlotter\n",
    "from scripts.utils import load_scenario_data\n",
    "from src.config import *\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image as IPImage\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.utils import NumpyEncoder\n",
    "from scripts.logger import *\n",
    "\n",
    "from scripts.logger import create_new_id, log_inference_metadata, get_config_variables, find_corresponding_training_id\n",
    "import src.config as config\n",
    "\n",
    "from scripts.utils import load_json, get_training_metadata, check_conflicts_between_training_and_current_config\n",
    "\n",
    "# Load the model and run inference\n",
    "def run_inference_dqn_single(model_path, scenario_folder, env_type, seed):\n",
    "\n",
    "    # Load the scenario data\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "\n",
    "    # Extract necessary data for the environment\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, \n",
    "        flights_dict, \n",
    "        rotations_dict, \n",
    "        alt_aircraft_dict, \n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    # Load the trained model and set the environment\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    print(f\"seed: {seed}\")\n",
    "\n",
    "    # Create StatePlotter object for visualizing the environment state\n",
    "    state_plotter = StatePlotter(\n",
    "        aircraft_dict=env.aircraft_dict,\n",
    "        flights_dict=env.flights_dict,\n",
    "        rotations_dict=env.rotations_dict,\n",
    "        alt_aircraft_dict=env.alt_aircraft_dict,\n",
    "        start_datetime=env.start_datetime,\n",
    "        end_datetime=env.end_datetime,\n",
    "        uncertain_breakdowns=env.uncertain_breakdowns,\n",
    "    )\n",
    "\n",
    "    # Reset the environment for inference\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000  # Set a maximum number of steps to prevent infinite loops\n",
    "\n",
    "    # List to collect images\n",
    "    plots = []\n",
    "\n",
    "        # Scenario-level data\n",
    "    scenario_log = {\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        \"total_reward\": 0,\n",
    "        \"steps\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        # Visualize the current state\n",
    "        print(f\"Step {step_num}:\")\n",
    "\n",
    "        # Extract necessary information from the environment for plotting\n",
    "        swapped_flights = env.swapped_flights\n",
    "        environment_delayed_flights = env.environment_delayed_flights\n",
    "        current_datetime = env.current_datetime\n",
    "\n",
    "        # Retrieve the updated dictionaries from the environment\n",
    "        updated_flights_dict = env.flights_dict\n",
    "        updated_rotations_dict = env.rotations_dict\n",
    "        updated_alt_aircraft_dict = env.alt_aircraft_dict\n",
    "        cancelled_flights = env.penalized_cancelled_flights\n",
    "\n",
    "        if DEBUG_MODE_VISUALIZATION:\n",
    "            print(\"Flights Dict:\")\n",
    "            print(updated_flights_dict)\n",
    "            print(\"Alt Aircraft Dict:\")\n",
    "            print(updated_alt_aircraft_dict)\n",
    "            print(\"Swapped Flights:\")\n",
    "            print(swapped_flights)\n",
    "            print(\"Environment Delayed Flights:\")\n",
    "            print(environment_delayed_flights)\n",
    "            print(\"Cancelled Flights:\")\n",
    "            print(cancelled_flights)\n",
    "            print(\"Unavailabilities:\")\n",
    "            print(env.alt_aircraft_dict)\n",
    "            print(\"Uncertain Breakdowns:\")\n",
    "            for key, value in env.uncertain_breakdowns.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"Current Breakdowns:\")\n",
    "            print(env.current_breakdowns)\n",
    "            print(\"\")\n",
    "\n",
    "        # Update the StatePlotter's dictionaries with the updated ones\n",
    "        state_plotter.alt_aircraft_dict = updated_alt_aircraft_dict\n",
    "        state_plotter.flights_dict = updated_flights_dict\n",
    "        state_plotter.rotations_dict = updated_rotations_dict\n",
    "\n",
    "        if 'reward' not in locals():\n",
    "            reward = 0\n",
    "            action = 0\n",
    "        # Collect the plot as an image\n",
    "        fig = state_plotter.plot_state(\n",
    "            updated_flights_dict, \n",
    "            swapped_flights, \n",
    "            environment_delayed_flights, \n",
    "            cancelled_flights, \n",
    "            current_datetime, \n",
    "            title_appendix=env_type,\n",
    "            show_plot=False,\n",
    "            reward_and_action=(reward, env.map_index_to_action(action), total_reward)\n",
    "        )\n",
    "        # Convert the figure to an image buffer\n",
    "        buf = BytesIO()\n",
    "        fig.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        img = IPImage(data=buf.read(), format='png', embed=True)\n",
    "        plots.append(img)\n",
    "        plt.close(fig)  # Close the figure to prevent automatic display\n",
    "\n",
    "        # Get the action mask from the environment\n",
    "        action_mask = obs['action_mask']\n",
    "\n",
    "        # Convert observation to float32\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "        # Get the action mask from the observation\n",
    "        action_mask = obs.get('action_mask', None)\n",
    "        if action_mask is None:\n",
    "            raise ValueError(\"Action mask is missing in the observation!\")\n",
    "\n",
    "        # Get the Q-values and apply the action mask\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        # Mask invalid actions by setting their Q-values to -inf\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "        # Predict the action using the masked Q-values\n",
    "        action = np.argmax(masked_q_values)\n",
    "\n",
    "        # Verify if the action is valid\n",
    "        if action_mask[action] == 0:\n",
    "            raise ValueError(f\"Invalid action selected by the model: {action}\")\n",
    "\n",
    "        # Take action in the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "        action_mapped = env.map_index_to_action(action)\n",
    "        print(\"action index:\")\n",
    "        print(action)\n",
    "        print(\"action mapped:\")\n",
    "        print(action_mapped)\n",
    "        print(f\"Action taken: {action_mapped}, Reward: {reward}\")\n",
    "\n",
    "        # Combine terminated and truncated flags\n",
    "        done_flag = terminated or truncated\n",
    "\n",
    "        # Log step data\n",
    "        step_log = {\n",
    "            \"step_num\": step_num,\n",
    "            \"action\": action,\n",
    "            \"flight_action\": action_mapped[0],\n",
    "            \"aircraft_action\": action_mapped[1],\n",
    "            \"reward\": reward,\n",
    "            \"total_reward\": total_reward,\n",
    "            \"q_values\": q_values.tolist(),\n",
    "            \"masked_q_values\": masked_q_values.tolist(),\n",
    "            \"action_mask\": action_mask.tolist(),\n",
    "            \"done_flag\": done_flag,\n",
    "        }\n",
    "\n",
    "        scenario_log[\"steps\"].append(step_log)\n",
    "\n",
    "        step_num += 1\n",
    "\n",
    "    print(\"================================================\")\n",
    "    print(\"Final state:\")\n",
    "\n",
    "    # Plot the final state and collect it\n",
    "    fig = state_plotter.plot_state(\n",
    "        updated_flights_dict, \n",
    "        swapped_flights, \n",
    "        environment_delayed_flights, \n",
    "        cancelled_flights, \n",
    "        current_datetime, \n",
    "        title_appendix=env_type,\n",
    "        show_plot=False,\n",
    "        reward_and_action=(reward, env.map_index_to_action(action), total_reward)\n",
    "    )\n",
    "    # Convert the figure to an image buffer\n",
    "    buf = BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    img = IPImage(data=buf.read(), format='png', embed=True)\n",
    "    plots.append(img)\n",
    "    plt.close(fig)  # Close the figure to prevent automatic display\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Total Steps: {step_num}\")\n",
    "\n",
    "    # Create an interactive slider to display the plots\n",
    "    def update_plot(index):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            display(plots[index])\n",
    "\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0, min=0, max=len(plots)-1, step=1, description='Step:'\n",
    "    )\n",
    "    output = widgets.Output()\n",
    "\n",
    "    slider.observe(lambda change: update_plot(change['new']), names='value')\n",
    "\n",
    "    # Display the initial plot\n",
    "    update_plot(0)\n",
    "\n",
    "    # Display the slider and output\n",
    "    display(slider, output)\n",
    "    scenario_log[\"total_reward\"] = total_reward\n",
    "    log_inference_scenario_data(inference_id, scenario_log)\n",
    "    return total_reward, step_num\n",
    "\n",
    "\n",
    "def run_inference_dqn_folder(model_path, scenario_folder, env_type, seed):\n",
    "\n",
    "    inference_config_variables = get_config_variables(config)\n",
    "\n",
    "    # Generate unique ID for training\n",
    "    inference_id = create_new_id(\"inference\")\n",
    "    runtime_start_in_seconds = time.time()\n",
    "\n",
    "    # Get the config variables during training based on the model path and the logs/training/training_{id}.json file\n",
    "    # first find the corresponding training id from the logs/ids.json file\n",
    "    training_id = find_corresponding_training_id(model_path, env_type)\n",
    "    print(f\"Training ID: {training_id}\")\n",
    "\n",
    "    training_logs_path = f\"logs/training/training_{training_id}.json\"\n",
    "    matching_variables, conflicting_variables = check_conflicts_between_training_and_current_config(training_logs_path, env_type, inference_config_variables)\n",
    "\n",
    "    if len(conflicting_variables) > 0:\n",
    "        print(f\"Conflicting Variables: {conflicting_variables}\")\n",
    "        raise ValueError(\"Conflicting variables found between training and current config!\")\n",
    "    elif len(matching_variables) == 0:\n",
    "        raise ValueError(\"No matching variables found between training and current config!\")\n",
    "    else:\n",
    "        print(\"No conflicting variables found between training and current config!\")\n",
    "\n",
    "    inference_metadata = {\n",
    "        \"inference_id\": inference_id,\n",
    "        \"runtime_start_in_seconds\": runtime_start_in_seconds,\n",
    "        \"model_path\": model_path,\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        **matching_variables,\n",
    "\n",
    "    }\n",
    "\n",
    "    # Log the inference metadata\n",
    "    log_inference_metadata(inference_id, inference_metadata)\n",
    "\n",
    "    for scenario in os.listdir(scenario_folder):\n",
    "        scenario_path = os.path.join(scenario_folder, scenario)\n",
    "        # Verify folder and model exist\n",
    "        if not os.path.exists(scenario_path):\n",
    "            raise FileNotFoundError(f\"Scenario folder not found: {scenario_path}\")\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "        run_inference_dqn_single(model_path, scenario_path, env_type, seed)\n",
    "\n",
    "\n",
    "\n",
    "latest = True\n",
    "# env_type = \"myopic\"\n",
    "env_type = \"proactive\"\n",
    "\n",
    "if latest:\n",
    "    MODEL_PATH = f\"../trained_models/dqn/{env_type}_3ac-{max(int(model.split('-')[1].split('.')[0]) for model in os.listdir('../trained_models/dqn') if model.startswith(f'{env_type}_3ac-'))}.zip\"\n",
    "else:\n",
    "    MODEL_PATH = f\"../trained_models/dqn/_perfect_{env_type}_3ac-2.zip\"\n",
    "\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "\n",
    "# seed = 42\n",
    "seed = int(time.time())\n",
    "\n",
    "\n",
    "# PROACTIVE EXAMPLE\n",
    "SCENARIO_FOLDER = \"../data/Locked/alpha/\"\n",
    "\n",
    "# NOT WORKING\n",
    "# MODEL_PATH = \"../trained_models/dqn/myopic_3ac-16.zip\"\n",
    "\n",
    "# ACTUALLY WORKING\n",
    "# MODEL_PATH = \"../trained_models/dqn/myopic_3ac-1.zip\"\n",
    "\n",
    "# Extract the env_type using regex\n",
    "match = re.search(r'/(myopic|proactive)_', MODEL_PATH)\n",
    "# env_type = match.group(1) if match else None\n",
    "\n",
    "print(f\"Environment Type: {env_type}\")\n",
    "\n",
    "# Run the fixed inference loop\n",
    "run_inference_dqn_folder(MODEL_PATH, SCENARIO_FOLDER, env_type, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
