{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Type: proactive\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../logs/training/training_None.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 226\u001b[0m\n\u001b[1;32m    221\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../trained_models/dqn/6ac-100-stochastic-high/42/training_63/myopic-training_63.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 226\u001b[0m \u001b[43mrun_inference_dqn_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCENARIO_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 157\u001b[0m, in \u001b[0;36mrun_inference_dqn_folder\u001b[0;34m(model_path, scenario_folder, env_type, seed)\u001b[0m\n\u001b[1;32m    155\u001b[0m training_id \u001b[38;5;241m=\u001b[39m find_corresponding_training_id(model_path, env_type)\n\u001b[1;32m    156\u001b[0m training_logs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../logs/training/training_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 157\u001b[0m training_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_training_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_logs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Metadata for inference\u001b[39;00m\n\u001b[1;32m    160\u001b[0m inference_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: inference_id,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime_start\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39misoformat() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    168\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/scripts/utils.py:782\u001b[0m, in \u001b[0;36mget_training_metadata\u001b[0;34m(training_logs_path, env_type)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_training_metadata\u001b[39m(training_logs_path, env_type):\n\u001b[0;32m--> 782\u001b[0m     training_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_logs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/scripts/utils.py:778\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(file_path):\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../logs/training/training_None.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.visualizations import StatePlotter\n",
    "from scripts.utils import load_scenario_data, get_training_metadata, NumpyEncoder\n",
    "from src.config import *\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Image as IPImage\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.logger import create_new_id, log_inference_metadata, log_inference_scenario_data, find_corresponding_training_id, get_config_variables, convert_to_serializable, update_id_status\n",
    "import src.config as config\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def run_inference_dqn_single(model_path, scenario_folder, env_type, seed, inference_id):\n",
    "    \"\"\"\n",
    "    Runs inference on a single scenario and logs detailed results.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "        scenario_folder (str): Path to the scenario folder.\n",
    "        env_type (str): Type of environment (\"myopic\" or \"proactive\").\n",
    "        seed (int): Seed for reproducibility.\n",
    "        inference_id (str): Unique ID for the inference session.\n",
    "    \"\"\"\n",
    "    # Load scenario data\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, flights_dict, rotations_dict, alt_aircraft_dict, config_dict, env_type=env_type\n",
    "    )\n",
    "\n",
    "    # Load trained model and configure\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    print(f\"Seed: {seed}\")\n",
    "\n",
    "    # Initialize visualization\n",
    "    state_plotter = StatePlotter(\n",
    "        aircraft_dict=env.aircraft_dict,\n",
    "        flights_dict=env.flights_dict,\n",
    "        rotations_dict=env.rotations_dict,\n",
    "        alt_aircraft_dict=env.alt_aircraft_dict,\n",
    "        start_datetime=env.start_datetime,\n",
    "        end_datetime=env.end_datetime,\n",
    "        uncertain_breakdowns=env.uncertain_breakdowns,\n",
    "    )\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    # Scenario log\n",
    "    scenario_log = {\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        \"steps\": [],\n",
    "        \"total_reward\": 0,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        # Step-level pre-action info\n",
    "        step_info_before_action = {\n",
    "            \"num_cancelled_flights\": len(env.cancelled_flights),\n",
    "            \"num_delayed_flights\": len(env.environment_delayed_flights),\n",
    "            \"num_resolved_conflicts\": len(env.resolved_conflicts),\n",
    "            \"current_datetime\": env.current_datetime.isoformat(),\n",
    "        }\n",
    "\n",
    "        # Action selection\n",
    "        action_mask = obs['action_mask']\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "        action = np.argmax(masked_q_values)\n",
    "\n",
    "        if action_mask[action] == 0:\n",
    "            raise ValueError(f\"Invalid action selected: {action}\")\n",
    "\n",
    "        # Environment step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done_flag = terminated or truncated\n",
    "        action_mapped = env.map_index_to_action(action)\n",
    "\n",
    "        # Log step data\n",
    "        step_log = {\n",
    "            \"step_num\": step_num,\n",
    "            \"action\": action,\n",
    "            \"flight_action\": action_mapped[0],\n",
    "            \"aircraft_action\": action_mapped[1],\n",
    "            \"reward\": reward,\n",
    "            \"total_reward\": total_reward,\n",
    "            \"q_values\": q_values.tolist(),\n",
    "            \"masked_q_values\": masked_q_values.tolist(),\n",
    "            \"action_mask\": action_mask.tolist(),\n",
    "            \"done_flag\": done_flag,\n",
    "            \"info_after_step\": convert_to_serializable(env.info_after_step),\n",
    "            \"step_info_before_action\": step_info_before_action,\n",
    "        }\n",
    "\n",
    "        scenario_log[\"steps\"].append(step_log)\n",
    "        step_num += 1\n",
    "\n",
    "    scenario_log[\"total_reward\"] = total_reward\n",
    "    scenario_log[\"runtime_end\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "    # Log scenario data\n",
    "    log_inference_scenario_data(inference_id, scenario_log)\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(f\"Steps Taken: {step_num}\")\n",
    "    return scenario_log\n",
    "\n",
    "\n",
    "def run_inference_dqn_folder(model_path, scenario_folder, env_type, seed):\n",
    "    \"\"\"\n",
    "    Runs inference on all scenarios in a folder.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "        scenario_folder (str): Path to the folder containing scenarios.\n",
    "        env_type (str): Type of environment (\"myopic\" or \"proactive\").\n",
    "        seed (int): Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    inference_id = create_new_id(\"inference\")\n",
    "    runtime_start = time.time()\n",
    "\n",
    "    # Load training metadata\n",
    "    training_id = find_corresponding_training_id(model_path, env_type)\n",
    "    training_logs_path = f\"../logs/training/training_{training_id}.json\"\n",
    "    training_metadata = get_training_metadata(training_logs_path, env_type)\n",
    "\n",
    "    # Metadata for inference\n",
    "    inference_metadata = {\n",
    "        \"inference_id\": inference_id,\n",
    "        \"model_path\": model_path,\n",
    "        \"scenario_folder\": scenario_folder,\n",
    "        \"env_type\": env_type,\n",
    "        \"seed\": seed,\n",
    "        \"training_metadata\": training_metadata,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    }\n",
    "\n",
    "    log_inference_metadata(inference_id, inference_metadata)\n",
    "\n",
    "    complete_inference_log = {\n",
    "        \"inference_id\": inference_id,\n",
    "        \"runtime_start\": inference_metadata[\"runtime_start\"],\n",
    "        \"runtime_end\": None,\n",
    "        \"scenarios\": {},\n",
    "    }\n",
    "\n",
    "    for scenario in os.listdir(scenario_folder):\n",
    "        scenario_path = os.path.join(scenario_folder, scenario)\n",
    "        if os.path.isdir(scenario_path):\n",
    "            scenario_log = run_inference_dqn_single(model_path, scenario_path, env_type, seed, inference_id)\n",
    "            complete_inference_log[\"scenarios\"][scenario] = scenario_log\n",
    "\n",
    "    complete_inference_log[\"runtime_end\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "    log_file_path = os.path.join(\"../logs\", \"inference\", f\"inference_{inference_id}.json\")\n",
    "\n",
    "    # Save complete inference log\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        json.dump(complete_inference_log, log_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "    # Mark as done in ids.json\n",
    "    update_id_status(inference_id, \"finished\")\n",
    "\n",
    "    print(f\"Inference log saved to {log_file_path}\")\n",
    "\n",
    "\n",
    "# Main logic to run inference\n",
    "latest = True\n",
    "env_type = \"proactive\"\n",
    "\n",
    "# if latest:\n",
    "#     MODEL_PATH = f\"../trained_models/dqn/{env_type}_3ac-{max(int(model.split('-')[1].split('.')[0]) for model in os.listdir('../trained_models/dqn') if model.startswith(f'{env_type}_3ac-'))}.zip\"\n",
    "# else:\n",
    "#     MODEL_PATH = f\"../trained_models/dqn/_perfect_{env_type}_3ac-2.zip\"\n",
    "\n",
    "# print(f\"Model Path: {MODEL_PATH}\")\n",
    "\n",
    "\n",
    "\n",
    "seed = int(time.time())\n",
    "SCENARIO_FOLDER = \"../data/Locked/alpha/\"\n",
    "\n",
    "# PROACTIVE EXAMPLE\n",
    "SCENARIO_FOLDER = \"../data/Training/6ac-100-stochastic-high/Scenario_01\"\n",
    "\n",
    "# NOT WORKING\n",
    "# MODEL_PATH = \"../trained_models/dqn/myopic_3ac-16.zip\"\n",
    "\n",
    "# ACTUALLY WORKING\n",
    "MODEL_PATH = \"../trained_models/dqn/6ac-100-stochastic-high/42/training_63/myopic-training_63.zip\"\n",
    "\n",
    "\n",
    "print(f\"Environment Type: {env_type}\")\n",
    "\n",
    "run_inference_dqn_folder(MODEL_PATH, SCENARIO_FOLDER, env_type, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
