{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scripts.utils import *\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.logger import configure\n",
    "from scripts.utils import *\n",
    "from src.environment import AircraftDisruptionEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO-specific hyperparameters\n",
    "PPO_LEARNING_RATE = 0.0003\n",
    "PPO_N_STEPS = 2048\n",
    "PPO_BATCH_SIZE = 64\n",
    "PPO_N_EPOCHS = 10\n",
    "PPO_GAMMA = 0.99\n",
    "PPO_CLIP_RANGE = 0.2\n",
    "\n",
    "# Shared constants\n",
    "TRAINING_FOLDERS_PATH = '../data/Training/1k-3ac-12f-1dis-F/'\n",
    "TESTING_FOLDERS_PATH = '../data/Testing/1k-3ac-12f-1dis-F/'\n",
    "N_EPISODES = 500\n",
    "MODEL_SAVE_PATH = '../trained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "CUDA available: False\n",
      "Number of GPUs available: 0\n",
      "cuDNN enabled: True\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Device info: {'device_type': 'MacBook M1'}\n",
      "Training on 500000 days of data (500 episodes of 1000 scenarios)\n",
      "Models will be saved to:\n",
      "   ../trained_models/myopic_1k-3ac-12f-1dis-F-500k-1.zip\n",
      "   ../trained_models/proactive_1k-3ac-12f-1dis-F-500k-1.zip\n",
      "Results directory created at: ../results/20241120-06-49\n"
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = initialize_device()\n",
    "\n",
    "# Check device capabilities\n",
    "check_device_capabilities()\n",
    "\n",
    "# Get device-specific information\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "# Verify training folders and gather training data\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "# print(f\"Training folders: {training_folders}\")\n",
    "\n",
    "# Calculate training days and model naming\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "        f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "last_folder = os.path.basename(os.path.normpath(TRAINING_FOLDERS_PATH))\n",
    "model_name = last_folder\n",
    "model_version = get_model_version(model_name)\n",
    "MODEL_SAVE_PATH = f'../trained_models/'\n",
    "MODEL_SAVE_NAME = f'{model_name}-{formatted_days}-{model_version}.zip'\n",
    "print(f\"Models will be saved to:\")\n",
    "print(f\"   {MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME}\")\n",
    "print(f\"   {MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = create_results_directory()\n",
    "print(f\"Results directory created at: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training for Myopic agent...\n",
      "Logging to /var/folders/m6/gwyqzldd12bg_s3mrl40tp6r0000gn/T/SB3-2024-11-20-06-49-00-087313\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error while checking key=action_mask: The observation returned by the `reset()` method does not match the data type (cannot cast) of the given observation space Box(0, 1, (52,), uint8). Expected: uint8, actual dtype: float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:296\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43m_check_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:219\u001b[0m, in \u001b[0;36m_check_obs\u001b[0;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m observation_space\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m obs\u001b[38;5;241m.\u001b[39mshape, (\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe observation returned by the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method does not match the shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the given observation space \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, actual shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(obs\u001b[38;5;241m.\u001b[39mdtype, observation_space\u001b[38;5;241m.\u001b[39mdtype), (\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe observation returned by the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method does not match the data type (cannot cast) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the given observation space \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, actual dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n",
      "\u001b[0;31mAssertionError\u001b[0m: The observation returned by the `reset()` method does not match the data type (cannot cast) of the given observation space Box(0, 1, (52,), uint8). Expected: uint8, actual dtype: float32",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting PPO training for Myopic agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m     myopic_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting PPO training for Proactive agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     proactive_model \u001b[38;5;241m=\u001b[39m train_ppo_agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproactive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 74\u001b[0m, in \u001b[0;36mtrain_ppo_agent\u001b[0;34m(env_type)\u001b[0m\n\u001b[1;32m     64\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m load_scenario_data(dummy_scenario_folder)\n\u001b[1;32m     65\u001b[0m env \u001b[38;5;241m=\u001b[39m Float32AircraftDisruptionEnv(\n\u001b[1;32m     66\u001b[0m     data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maircraft\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m     data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflights\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     env_type\u001b[38;5;241m=\u001b[39menv_type\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure the environment is compatible with SB3\u001b[39;00m\n\u001b[1;32m     76\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Initialize the PPO model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:473\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m \u001b[43m_check_returned_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_render_check:\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:298\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    296\u001b[0m             _check_obs(obs[key], observation_space\u001b[38;5;241m.\u001b[39mspaces[key], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 298\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     _check_obs(obs, observation_space, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error while checking key=action_mask: The observation returned by the `reset()` method does not match the data type (cannot cast) of the given observation space Box(0, 1, (52,), uint8). Expected: uint8, actual dtype: float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from scripts.utils import *\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Constants\n",
    "PPO_LEARNING_RATE = 0.0003\n",
    "PPO_N_STEPS = 2048\n",
    "PPO_BATCH_SIZE = 64\n",
    "PPO_N_EPOCHS = 10\n",
    "PPO_GAMMA = 0.99\n",
    "PPO_CLIP_RANGE = 0.2\n",
    "N_EPISODES = 500\n",
    "MODEL_SAVE_PATH = '../trained_models/'\n",
    "TRAINING_FOLDERS_PATH = '../data/Training/1k-3ac-12f-1dis-F/'\n",
    "TESTING_FOLDERS_PATH = '../data/Testing/1k-3ac-12f-1dis-F/'\n",
    "\n",
    "# Custom environment wrapper to enforce float32 observations\n",
    "class Float32AircraftDisruptionEnv(AircraftDisruptionEnv):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self.cast_to_float32(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self.cast_to_float32(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    @staticmethod\n",
    "    def cast_to_float32(obs):\n",
    "        \"\"\"\n",
    "        Recursively cast all observation components to float32.\n",
    "        \"\"\"\n",
    "        if isinstance(obs, dict):\n",
    "            return {key: Float32AircraftDisruptionEnv.cast_to_float32(value) for key, value in obs.items()}\n",
    "        elif isinstance(obs, np.ndarray):\n",
    "            return obs.astype(np.float32)\n",
    "        else:\n",
    "            return np.array(obs, dtype=np.float32)\n",
    "\n",
    "# Training function\n",
    "def train_ppo_agent(env_type):\n",
    "    # Prepare logging\n",
    "    logger = configure()\n",
    "\n",
    "    # List all the scenario folders\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    # Initialize the environment using the first scenario folder\n",
    "    dummy_scenario_folder = scenario_folders[0]\n",
    "    data_dict = load_scenario_data(dummy_scenario_folder)\n",
    "    env = Float32AircraftDisruptionEnv(\n",
    "        data_dict['aircraft'],\n",
    "        data_dict['flights'],\n",
    "        data_dict['rotations'],\n",
    "        data_dict['alt_aircraft'],\n",
    "        data_dict['config'],\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    check_env(env)  # Ensure the environment is compatible with SB3\n",
    "\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # Initialize the PPO model\n",
    "    model = PPO(\n",
    "        policy=\"MultiInputPolicy\",  # Supports dict-based observations\n",
    "        env=vec_env,\n",
    "        learning_rate=PPO_LEARNING_RATE,\n",
    "        n_steps=PPO_N_STEPS,\n",
    "        batch_size=PPO_BATCH_SIZE,\n",
    "        n_epochs=PPO_N_EPOCHS,\n",
    "        gamma=PPO_GAMMA,\n",
    "        clip_range=PPO_CLIP_RANGE,\n",
    "        verbose=1,\n",
    "        device=\"auto\"\n",
    "    )\n",
    "\n",
    "    model._logger = logger\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(N_EPISODES):\n",
    "        for scenario_folder in scenario_folders:\n",
    "            # Reload the environment with a new scenario\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            env = Float32AircraftDisruptionEnv(\n",
    "                data_dict['aircraft'],\n",
    "                data_dict['flights'],\n",
    "                data_dict['rotations'],\n",
    "                data_dict['alt_aircraft'],\n",
    "                data_dict['config'],\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(DummyVecEnv([lambda: env]))\n",
    "\n",
    "            # Train the model\n",
    "            model.learn(total_timesteps=PPO_N_STEPS)\n",
    "\n",
    "        print(f\"{env_type}: Episode {episode + 1}/{N_EPISODES} completed.\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model_save_name = f'{env_type}_ppo_with_casting.zip'\n",
    "    model.save(os.path.join(MODEL_SAVE_PATH, model_save_name))\n",
    "    print(f\"Model saved to {os.path.join(MODEL_SAVE_PATH, model_save_name)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the myopic and proactive PPO agents\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting PPO training for Myopic agent...\")\n",
    "    myopic_model = train_ppo_agent('myopic')\n",
    "\n",
    "    print(\"Starting PPO training for Proactive agent...\")\n",
    "    proactive_model = train_ppo_agent('proactive')\n",
    "\n",
    "    print(\"Training completed for both agents.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
