{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scripts.utils import *\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure  # Import the configure function\n",
    "from stable_baselines3.common.utils import polyak_update, set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from scripts.utils import NumpyEncoder\n",
    "from scripts.logger import *\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration phase: 125000 timesteps\n",
      "Total timesteps: 2500000\n"
     ]
    }
   ],
   "source": [
    "# Constants and Training Settings\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 256\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "MAX_TIMESTEPS = 50000\n",
    "\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 512, 512, 256])\n",
    "LEARNING_STARTS = 1000\n",
    "TRAIN_FREQ = 4\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.025\n",
    "PERCENTAGE_MIN = 90\n",
    "# EXPLORATION_PHASE = 1000\n",
    "\n",
    "\n",
    "EPSILON_TYPE = \"exponential\"\n",
    "# EPSILON_TYPE = \"linear\"\n",
    "\n",
    "\n",
    "if EPSILON_TYPE == \"linear\":\n",
    "    EPSILON_MIN = 0\n",
    "\n",
    "N_EPISODES = 500\n",
    "cross_val_flag = True\n",
    "CROSS_VAL_INTERVAL = N_EPISODES/100\n",
    "\n",
    "# TRAINING_FOLDERS_PATH = \"../data/Locked/alpha/\"\n",
    "# TRAINING_FOLDERS_PATH = \"../data/Training/3ac-10/\"\n",
    "# TRAINING_FOLDERS_PATH = \"../data/Training/temp/\"\n",
    "TRAINING_FOLDERS_PATH = \"../data/Training/3ac-1000-steady/\"\n",
    "TESTING_FOLDERS_PATH = \"../data/Training/3ac-1-steady/\"\n",
    "\n",
    "\n",
    "# extract number of scenarios in training and testing folders\n",
    "num_scenarios_training = len(os.listdir(TRAINING_FOLDERS_PATH))\n",
    "exploration_phase_percentage = 5\n",
    "num_timesteps_per_scenario = 5\n",
    "\n",
    "total_timesteps = N_EPISODES * num_scenarios_training * num_timesteps_per_scenario\n",
    "EXPLORATION_PHASE = int(exploration_phase_percentage / 100 * total_timesteps)\n",
    "print(f\"Exploration phase: {EXPLORATION_PHASE} timesteps\")\n",
    "\n",
    "print(f\"Total timesteps: {total_timesteps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESTIMATED VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating...\n",
      "Estimated total timesteps:  2924000.0\n",
      "Calculated EPSILON_DECAY_RATE: 1.4017629784594682e-06\n",
      "\n",
      "Testing...\n",
      "Epsilon reaches its minimum value at 90.00% of total timesteps.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON DECAY RATE:  1.4017629784594682e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "\n",
    "print(\"Calculating...\")\n",
    "# Calculate total timesteps 10 times and take the average\n",
    "total_timesteps_list = [calculate_total_training_timesteps(TRAINING_FOLDERS_PATH, N_EPISODES) for _ in range(1)]\n",
    "estimated_total_timesteps = sum(total_timesteps_list) / len(total_timesteps_list)\n",
    "\n",
    "print(\"Estimated total timesteps: \", estimated_total_timesteps)\n",
    "\n",
    "# Calculate the decay rate or linear rate\n",
    "EPSILON_DECAY_RATE = calculate_epsilon_decay_rate(\n",
    "    estimated_total_timesteps,\n",
    "    EPSILON_START,\n",
    "    EPSILON_MIN,\n",
    "    PERCENTAGE_MIN,\n",
    "    EPSILON_TYPE\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Testing...\")\n",
    "simulate_and_plot_epsilon_decay(\n",
    "    EPSILON_START, EPSILON_MIN, EPSILON_DECAY_RATE, estimated_total_timesteps, EPSILON_TYPE\n",
    ")\n",
    "\n",
    "EPSILON_DECAY_RATE = EPSILON_DECAY_RATE\n",
    "print(\"EPSILON DECAY RATE: \", EPSILON_DECAY_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "CUDA available: False\n",
      "Number of GPUs available: 0\n",
      "cuDNN enabled: True\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Device info: {'device_type': 'MacBook M1'}\n",
      "Training on 500000 days of data (500 episodes of 1000 scenarios)\n",
      "Results directory created at: ../results/dqn/20241206-18-36\n"
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = initialize_device()\n",
    "\n",
    "# Check device capabilities\n",
    "check_device_capabilities()\n",
    "\n",
    "# Get device-specific information\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "# Verify training folders and gather training data\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "# print(f\"Training folders: {training_folders}\")\n",
    "\n",
    "# Calculate training days and model naming\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "        f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "MODEL_SAVE_PATH = f'../trained_models/dqn/'\n",
    "\n",
    "# myopic_model_name = \"myopic_3ac\"\n",
    "# proactive_model_name = \"proactive_3ac\"\n",
    "# myopic_model_version = get_model_version(myopic_model_name, myopic_proactive=\"myopic\", drl_type=\"dqn\")\n",
    "# proactive_model_version = get_model_version(proactive_model_name, myopic_proactive=\"proactive\", drl_type=\"dqn\")\n",
    "\n",
    "# myopic_model_path = f\"{MODEL_SAVE_PATH}{myopic_model_name}-{myopic_model_version}.zip\"\n",
    "# proactive_model_path = f\"{MODEL_SAVE_PATH}{proactive_model_name}-{proactive_model_version}.zip\"\n",
    "# print(f\"Models will be saved to:\")\n",
    "# print(f\"   {myopic_model_path}\")\n",
    "# print(f\"   {proactive_model_path}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = create_results_directory(append_to_name='dqn')\n",
    "print(f\"Results directory created at: {results_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.logger import create_new_id, get_config_variables\n",
    "import src.config as config\n",
    "\n",
    "# Training both agents: myopic and proactive\n",
    "\n",
    "def train_dqn_agent(env_type):\n",
    "    log_data = {}  # Main dictionary to store all logs\n",
    "\n",
    "    config_variables = get_config_variables(config)\n",
    "\n",
    "    # Generate unique ID for training\n",
    "    training_id = create_new_id(\"training\")\n",
    "    runtime_start_in_seconds = time.time()\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model_path = f\"../trained_models/dqn/myopic-{training_id}.zip\"\n",
    "        print(f\"Models will be saved to: {model_path}\")\n",
    "        model_path_and_name = model_path\n",
    "    elif env_type == \"proactive\":\n",
    "        model_path = f\"../trained_models/dqn/proactive-{training_id}.zip\"\n",
    "        print(f\"Models will be saved to: {model_path}\")\n",
    "        model_path_and_name = model_path\n",
    "    # Metadata for logging\n",
    "    training_metadata = {\n",
    "        \"myopic_or_proactive\": env_type,\n",
    "        \"model_type\": \"dqn\",\n",
    "        \"training_id\": training_id,\n",
    "        \"MODEL_SAVE_PATH\": model_path,\n",
    "        \"N_EPISODES\": N_EPISODES,\n",
    "        \"num_scenarios_training\": num_scenarios_training,\n",
    "        \"results_dir\": results_dir,\n",
    "        \"CROSS_VAL_FLAG\": cross_val_flag,\n",
    "        \"CROSS_VAL_INTERVAL\": CROSS_VAL_INTERVAL,\n",
    "        **config_variables,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"BUFFER_SIZE\": BUFFER_SIZE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"TARGET_UPDATE_INTERVAL\": TARGET_UPDATE_INTERVAL,\n",
    "        \"EPSILON_START\": EPSILON_START,\n",
    "        \"EPSILON_MIN\": EPSILON_MIN,\n",
    "        \"EPSILON_DECAY_RATE\": EPSILON_DECAY_RATE,\n",
    "        \"EXPLORATION_PHASE\": EXPLORATION_PHASE,\n",
    "        \"MAX_TIMESTEPS\": MAX_TIMESTEPS,\n",
    "        \"LEARNING_STARTS\": LEARNING_STARTS,\n",
    "        \"TRAIN_FREQ\": TRAIN_FREQ,\n",
    "        \"NEURAL_NET_STRUCTURE\": NEURAL_NET_STRUCTURE,\n",
    "        \"device_info\": str(get_device_info(device)),\n",
    "        \"TRAINING_FOLDERS_PATH\": TRAINING_FOLDERS_PATH,\n",
    "        \"TESTING_FOLDERS_PATH\": TESTING_FOLDERS_PATH,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"runtime_start_in_seconds\": runtime_start_in_seconds,\n",
    "    }\n",
    "\n",
    "    log_data['metadata'] = training_metadata\n",
    "    log_data['episodes'] = {}\n",
    "    log_data['cross_validation'] = {}\n",
    "\n",
    "    best_reward_avg = float('-inf')\n",
    "    # Initialize variables\n",
    "    rewards = {}\n",
    "    good_rewards = {}\n",
    "    test_rewards = []\n",
    "    epsilon_values = []\n",
    "    total_timesteps = 0  # Added to track total timesteps\n",
    "    consecutive_drops = 0  # Track consecutive performance drops\n",
    "    best_test_reward = float('-inf')  # Track best test performance\n",
    "    action_sequences = {\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder): {\n",
    "            \"best_actions\": [],\n",
    "            \"best_reward\": float('-inf'),\n",
    "            \"worst_actions\": [],\n",
    "            \"worst_reward\": float('inf')\n",
    "        }\n",
    "        for folder in training_folders\n",
    "    }\n",
    "    \n",
    "    def cross_validate_on_test_data(model, current_episode, log_data):\n",
    "        cross_val_data = {\n",
    "            \"episode\": current_episode,\n",
    "            \"scenarios\": [],\n",
    "            \"avg_test_reward\": 0,\n",
    "        }\n",
    "\n",
    "        test_scenario_folders = [\n",
    "            os.path.join(TESTING_FOLDERS_PATH, folder)\n",
    "            for folder in os.listdir(TESTING_FOLDERS_PATH)\n",
    "            if os.path.isdir(os.path.join(TESTING_FOLDERS_PATH, folder))\n",
    "        ]\n",
    "        total_test_reward = 0\n",
    "        for test_scenario_folder in test_scenario_folders:\n",
    "            scenario_data = {\n",
    "                \"scenario_folder\": test_scenario_folder,\n",
    "                \"total_reward\": 0,\n",
    "                \"steps\": []\n",
    "            }\n",
    "            # Load data\n",
    "            data_dict = load_scenario_data(test_scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            # Update the environment with the new scenario (by reinitializing it)\n",
    "            from src.environment import AircraftDisruptionEnv\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "            # Evaluate the model on the test scenario without training\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            timesteps = 0\n",
    "\n",
    "            while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "                # Get the action mask from the environment\n",
    "                action_mask = obs['action_mask']\n",
    "\n",
    "                # Convert observation to float32\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "                # Preprocess observation and get Q-values\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "                # Apply the action mask (set invalid actions to -np.inf)\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "                # Select the action with the highest masked Q-value\n",
    "                action = np.argmax(masked_q_values)\n",
    "\n",
    "                # Take the selected action in the environment\n",
    "                result = env.step(action)\n",
    "\n",
    "                # Unpack the result\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "                done_flag = terminated or truncated\n",
    "\n",
    "                # Accumulate the reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the current observation\n",
    "                obs = obs_next\n",
    "\n",
    "                scenario_data[\"steps\"].append({\n",
    "                    \"step_number\": timesteps + 1,\n",
    "                    \"action\": action,\n",
    "                    \"flight_action\": env.map_index_to_action(action)[0],\n",
    "                    \"aircraft_action\": env.map_index_to_action(action)[1],\n",
    "                    \"reward\": reward,\n",
    "                    \"total_timestep\": total_timesteps,\n",
    "                    \"time_in_scenario\": timesteps,\n",
    "                    \"epsilon\": \"1.0 at cross-validation\",\n",
    "                    \"action_reason\": \"exploitation at cross-validation\",\n",
    "                    \"action_mask\": action_mask,\n",
    "                    \"action_mask_sum\": np.sum(action_mask),\n",
    "                    \"len_action_mask\": len(action_mask),\n",
    "                    \"masked_q_values\": masked_q_values,\n",
    "                    \"q_values\": q_values,\n",
    "                    \"info_after_step\": env.info_after_step,\n",
    "                    # Assuming impact_of_action is defined elsewhere\n",
    "                    # \"impact_of_action\": impact_of_action,\n",
    "                })\n",
    "\n",
    "                timesteps += 1\n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            total_test_reward += total_reward\n",
    "\n",
    "            scenario_data[\"total_reward\"] = total_reward\n",
    "            cross_val_data[\"scenarios\"].append(scenario_data)\n",
    "\n",
    "        # Compute average test reward\n",
    "        avg_test_reward = total_test_reward / len(test_scenario_folders)\n",
    "        cross_val_data[\"avg_test_reward\"] = avg_test_reward\n",
    "        test_rewards.append((current_episode, avg_test_reward))\n",
    "        print(f\"cross-val done at episode {current_episode}\")\n",
    "\n",
    "        # Store cross-validation data in log_data\n",
    "        log_data['cross_validation'][current_episode] = cross_val_data\n",
    "\n",
    "        return avg_test_reward  # Return the average test reward\n",
    "\n",
    "    # List all the scenario folders in Data/Training\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    total_timesteps = 0  # Reset total_timesteps for each agent\n",
    "\n",
    "    # Initialize the DQN\n",
    "    dummy_scenario_folder = scenario_folders[0]\n",
    "    data_dict = load_scenario_data(dummy_scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    from src.environment import AircraftDisruptionEnv\n",
    "\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict,\n",
    "        flights_dict,\n",
    "        rotations_dict,\n",
    "        alt_aircraft_dict,\n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        policy='MultiInputPolicy',\n",
    "        env=env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "        verbose=0,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model._logger = logger\n",
    "\n",
    "    # Training loop over the number of episodes\n",
    "    for episode in range(N_EPISODES):\n",
    "        # create a dictionary in the rewards dictionary for the current episode\n",
    "        rewards[episode] = {}\n",
    "        # create a dictionary in the action_sequences dictionary for the current episode\n",
    "        action_sequences[episode] = {}\n",
    "        # Log the start of the episode\n",
    "        episode_data = {\n",
    "            \"episode_number\": episode + 1,\n",
    "            \"epsilon_start\": epsilon,\n",
    "            \"scenarios\": {},\n",
    "        }\n",
    "        # Cycle through all the scenario folders\n",
    "        for scenario_folder in scenario_folders:\n",
    "            scenario_data = {\n",
    "                \"scenario_folder\": scenario_folder,\n",
    "                \"steps\": [],\n",
    "                \"total_reward\": 0,\n",
    "            }\n",
    "            # create a dictionary in the rewards dictionary for the current scenario\n",
    "            rewards[episode][scenario_folder] = {}\n",
    "            # create a dictionary in the action_sequences dictionary for the current scenario\n",
    "            action_sequences[episode][scenario_folder] = []\n",
    "            best_reward = float('-inf')\n",
    "            best_action_sequence = []\n",
    "            # Load the data for the current scenario\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            # Update the environment with the new scenario (by reinitializing it)\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "            # Reset the environment\n",
    "            obs, _ = env.reset()  # Extract the observation (obs) and ignore the info (_)\n",
    "\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            timesteps = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "\n",
    "                # Get cancelled flights\n",
    "                num_cancelled_flights_before_step = len(env.cancelled_flights)  # Returns a set of flight IDs\n",
    "\n",
    "                # Get delayed flights and their delays (in minutes)\n",
    "                num_delayed_flights_before_step = len(env.environment_delayed_flights)  # Returns a dict of {flight_id: delay_minutes}\n",
    "\n",
    "                # Get flights that have already been penalized for delays\n",
    "                num_penalized_delays_before_step = len(env.penalized_delays)  # Returns a dict of {flight_id: delay_minutes}\n",
    "\n",
    "                # Get flights that have already been penalized for being cancelled\n",
    "                num_penalized_cancelled_before_step = len(env.penalized_cancelled_flights)  # Returns a set of flight IDs\n",
    "\n",
    "                model.exploration_rate = epsilon\n",
    "\n",
    "                # Get the action mask from the environment\n",
    "                action_mask = obs['action_mask']\n",
    "\n",
    "                # Convert observation to float32\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "                # Preprocess observation and get Q-values\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "                # Apply the action mask (set invalid actions to -np.inf)\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "                current_seed = int(time.time() * 1e9) % (2**32 - 1)\n",
    "                # print(current_seed)\n",
    "                np.random.seed(current_seed)\n",
    "                \n",
    "                brute_force_flag = False\n",
    "                # Select an action\n",
    "                action_reason = \"None\"\n",
    "                if np.random.rand() < epsilon or brute_force_flag:\n",
    "                    # Exploration: choose a random valid action\n",
    "                    valid_actions = np.where(action_mask == 1)[0]\n",
    "                    action = np.random.choice(valid_actions)\n",
    "                    action_reason = \"exploration\"\n",
    "                else:\n",
    "                    # Exploitation: choose the action with the highest masked Q-value\n",
    "                    action = np.argmax(masked_q_values)\n",
    "                    action_reason = \"exploitation\"\n",
    "\n",
    "                # Take the selected action in the environment\n",
    "                result = env.step(action)\n",
    "\n",
    "                # Unpack the result (5 values)\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "                # Store the reward for the current timestep\n",
    "                rewards[episode][scenario_folder][timesteps] = reward\n",
    "                action_sequences[episode][scenario_folder].append(action)\n",
    "\n",
    "                # Combine the terminated and truncated flags into a single done flag\n",
    "                done_flag = terminated or truncated\n",
    "\n",
    "                # Store the action\n",
    "                action_sequence.append(action)\n",
    "\n",
    "                # Add the transition to the replay buffer\n",
    "                model.replay_buffer.add(\n",
    "                    obs=obs,\n",
    "                    next_obs=obs_next,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    done=done_flag,\n",
    "                    infos=[info]\n",
    "                )\n",
    "\n",
    "                # Update the current observation\n",
    "                obs = obs_next\n",
    "\n",
    "                if total_timesteps < EXPLORATION_PHASE:\n",
    "                    # Hold epsilon at 1 during the exploration phase\n",
    "                    epsilon = EPSILON_START\n",
    "                else:\n",
    "                    # Update epsilon after the exploration phase\n",
    "                    epsilon = max(EPSILON_MIN, epsilon * (1 - EPSILON_DECAY_RATE))\n",
    "                    \n",
    "                epsilon_values.append((episode + 1, epsilon))\n",
    "\n",
    "                timesteps += 1\n",
    "                total_timesteps += 1  # Update total_timesteps\n",
    "\n",
    "                # Training\n",
    "                if total_timesteps > model.learning_starts and total_timesteps % TRAIN_FREQ == 0:\n",
    "                    # Perform a training step\n",
    "                    model.train(gradient_steps=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "                # Update target network\n",
    "                if total_timesteps % model.target_update_interval == 0:\n",
    "                    polyak_update(model.q_net.parameters(), model.q_net_target.parameters(), model.tau)\n",
    "                    # Copy batch norm stats\n",
    "                    polyak_update(model.batch_norm_stats, model.batch_norm_stats_target, 1.0)\n",
    "\n",
    "                num_cancelled_flights_after_step = len(env.cancelled_flights)\n",
    "                num_delayed_flights_after_step = len(env.environment_delayed_flights)\n",
    "                num_penalized_delays_after_step = len(env.penalized_delays)\n",
    "                num_penalized_cancelled_after_step = len(env.penalized_cancelled_flights)\n",
    "\n",
    "                impact_of_action = {\n",
    "                    \"num_cancelled_flights\": num_cancelled_flights_after_step - num_cancelled_flights_before_step,\n",
    "                    \"num_delayed_flights\": num_delayed_flights_after_step - num_delayed_flights_before_step,\n",
    "                    \"num_penalized_delays\": num_penalized_delays_after_step - num_penalized_delays_before_step,\n",
    "                    \"num_penalized_cancelled\": num_penalized_cancelled_after_step - num_penalized_cancelled_before_step,\n",
    "                }\n",
    "\n",
    "                scenario_data[\"steps\"].append({\n",
    "                    \"step_number\": timesteps,\n",
    "                    \"action\": action,\n",
    "                    \"flight_action\": env.map_index_to_action(action)[0],\n",
    "                    \"aircraft_action\": env.map_index_to_action(action)[1],\n",
    "                    \"reward\": reward,\n",
    "                    \"total_timestep\": total_timesteps,\n",
    "                    \"time_in_scenario\": timesteps,\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"action_reason\": action_reason,\n",
    "                    \"impact_of_action\": impact_of_action,\n",
    "                    \"done_flag\": done_flag,\n",
    "                    \"action_mask_sum\": np.sum(action_mask),\n",
    "                    \"len_action_mask\": len(action_mask),\n",
    "                    \"info_after_step\": env.info_after_step,\n",
    "                    \"masked_q_values\": masked_q_values,\n",
    "                    \"q_values\": q_values,\n",
    "                    \"action_mask\": action_mask,\n",
    "                })\n",
    "                \n",
    "                # Check if the episode is done\n",
    "                if done_flag:\n",
    "                    break\n",
    "            \n",
    "            total_reward = 0\n",
    "            for _, reward in rewards[episode][scenario_folder].items():\n",
    "                total_reward += reward\n",
    "            # Store the total reward for the episode with the scenario specified\n",
    "            rewards[episode][scenario_folder] = total_reward\n",
    "\n",
    "            if total_reward > 10000:\n",
    "                print(f\"Total reward for scenario {scenario_folder}: {total_reward}\")\n",
    "                print(f\"Action sequence: {action_sequence}\")\n",
    "\n",
    "            # save the action sequence\n",
    "            action_sequences[episode][scenario_folder] = action_sequence\n",
    "\n",
    "            # Summarize scenario\n",
    "            scenario_data[\"total_reward\"] = total_reward\n",
    "            episode_data[\"scenarios\"][scenario_folder] = scenario_data\n",
    "\n",
    "        # Perform cross-validation at specified intervals\n",
    "        if cross_val_flag:\n",
    "            # Initialize best_test_reward and current_test_reward at first cross validation\n",
    "            if (episode + 1) % CROSS_VAL_INTERVAL == 0:\n",
    "                current_test_reward = cross_validate_on_test_data(model, episode + 1, log_data)\n",
    "                if not hasattr(train_dqn_agent, 'best_test_reward'):\n",
    "                    train_dqn_agent.best_test_reward = current_test_reward\n",
    "                best_test_reward = train_dqn_agent.best_test_reward\n",
    "            \n",
    "                # Early stopping logic\n",
    "                if current_test_reward < best_test_reward:\n",
    "                    consecutive_drops += 1\n",
    "                    print(f\"Performance drop {consecutive_drops}/5 (current: {current_test_reward:.2f}, best: {best_test_reward:.2f})\")\n",
    "                    if consecutive_drops >= 500:\n",
    "                        print(f\"Early stopping triggered at episode {episode + 1} due to 5 consecutive drops in test performance\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_drops = 0\n",
    "                    train_dqn_agent.best_test_reward = current_test_reward\n",
    "                    best_test_reward = current_test_reward\n",
    "\n",
    "        # Calculate the average reward for this batch of episodes\n",
    "        avg_reward_for_this_batch = 0\n",
    "        for i in range(len(scenario_folders)):\n",
    "            avg_reward_for_this_batch += rewards[episode][scenario_folders[i]]\n",
    "        avg_reward_for_this_batch /= len(scenario_folders)\n",
    "\n",
    "        # Append the avg reward for this batch to rewards\n",
    "        rewards[episode + 1] = avg_reward_for_this_batch\n",
    "\n",
    "        good_rewards[episode + 1] = avg_reward_for_this_batch\n",
    "        if avg_reward_for_this_batch > best_reward_avg:\n",
    "            best_reward_avg = avg_reward_for_this_batch\n",
    "        print(f\"{env_type}: ({episode + 1}/{N_EPISODES}) {epsilon:.2f} best avg: {best_reward_avg:.2f} - rewards avg: {avg_reward_for_this_batch:.2f}\")\n",
    "        # print(f\"Total avg rewards for this batch: {avg_reward_for_this_batch}\")\n",
    "\n",
    "        # Summarize episode\n",
    "        episode_data[\"avg_reward\"] = avg_reward_for_this_batch\n",
    "        log_data['episodes'][episode + 1] = episode_data\n",
    "\n",
    "    # Save the model after training\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"../trained_models/dqn/myopic-{training_id}.zip\")\n",
    "    else:\n",
    "        model.save(f\"../trained_models/dqn/proactive-{training_id}.zip\")\n",
    "\n",
    "    runtime_end_in_seconds = time.time()\n",
    "    runtime_in_seconds = runtime_end_in_seconds - runtime_start_in_seconds\n",
    "\n",
    "    actual_total_timesteps = total_timesteps\n",
    "    actual_exploration_timesteps = EXPLORATION_PHASE\n",
    "    actual_exploration_percentage = actual_exploration_timesteps / total_timesteps\n",
    "    actual_exploitation_timesteps = total_timesteps - EXPLORATION_PHASE\n",
    "\n",
    "    training_summary = {\n",
    "        \"runtime_seconds\": runtime_in_seconds,\n",
    "        \"total_timesteps\": total_timesteps,\n",
    "        \"final_rewards\": good_rewards,\n",
    "        \"episodes_trained\": episode + 1,\n",
    "        \"actual_total_timesteps\": actual_total_timesteps,\n",
    "        \"actual_exploration_timesteps\": actual_exploration_timesteps,\n",
    "        \"actual_exploration_percentage\": actual_exploration_percentage,\n",
    "        \"actual_exploitation_timesteps\": actual_exploitation_timesteps,\n",
    "    }\n",
    "    log_data['training_summary'] = training_summary\n",
    "\n",
    "    # Include all other relevant data\n",
    "    log_data['average_batch_episode_rewards'] = good_rewards\n",
    "    log_data['test_rewards'] = test_rewards\n",
    "    log_data['epsilon_values'] = epsilon_values\n",
    "    log_data['action_sequences'] = action_sequences\n",
    "    log_data['rewards'] = rewards\n",
    "\n",
    "    # Write all data to the log file at once\n",
    "    log_file_path = os.path.join(\"../logs\", \"training\", f\"training_{training_id}.json\")\n",
    "    \n",
    "    # make log_data serializable\n",
    "    log_data = convert_to_serializable(log_data)\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        json.dump(log_data, log_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "    finalize_training_log(training_id, training_summary, model_path)\n",
    "\n",
    "    # Return collected data\n",
    "    return rewards, test_rewards, total_timesteps, epsilon_values, good_rewards, action_sequences, model_path_and_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be saved to: ../trained_models/dqn/myopic-0396.zip\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Logging to /var/folders/m6/gwyqzldd12bg_s3mrl40tp6r0000gn/T/SB3-2024-12-06-18-36-10-270023\n",
      "myopic: (1/500) 1.00 best avg: -1180.80 - rewards avg: -1180.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the myopic DQN agents\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results_myopic \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Unpack the results\u001b[39;00m\n\u001b[1;32m      8\u001b[0m rewards_myopic, test_rewards_myopic, total_timesteps_myopic, epsilon_values_myopic, good_rewards_myopic, action_sequences_myopic, model_path_and_name_myopic \u001b[38;5;241m=\u001b[39m results_myopic\n",
      "Cell \u001b[0;32mIn[5], line 318\u001b[0m, in \u001b[0;36mtrain_dqn_agent\u001b[0;34m(env_type)\u001b[0m\n\u001b[1;32m    315\u001b[0m obs \u001b[38;5;241m=\u001b[39m {key: np\u001b[38;5;241m.\u001b[39marray(value, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Preprocess observation and get Q-values\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    319\u001b[0m q_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mq_net(obs_tensor)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Apply the action mask (set invalid actions to -np.inf)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:276\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_tensor, vectorized_env\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:487\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(obs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized type of observation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:487\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39mas_tensor(obs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized type of observation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Main code to train both agents and plot results\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train the myopic DQN agents\n",
    "results_myopic = train_dqn_agent('myopic')\n",
    "\n",
    "# Unpack the results\n",
    "rewards_myopic, test_rewards_myopic, total_timesteps_myopic, epsilon_values_myopic, good_rewards_myopic, action_sequences_myopic, model_path_and_name_myopic = results_myopic\n",
    "\n",
    "\n",
    "# Save the myopic rewards and good rewards\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "good_rewards_file = os.path.join(results_dir, \"good_rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "with open(good_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(good_rewards_myopic, file)\n",
    "\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "print(f\"Good rewards saved to {good_rewards_file}\")\n",
    "print(f\"Myopic model saved to \\n{model_path_and_name_myopic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_proactive = train_dqn_agent('proactive')\n",
    "\n",
    "# Unpack the results\n",
    "rewards_proactive, test_rewards_proactive, total_timesteps_proactive, epsilon_values_proactive, good_rewards_proactive, action_sequences_proactive, model_path_and_name_proactive = results_proactive\n",
    "\n",
    "# Save the proactive rewards and good rewards\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "good_rewards_file = os.path.join(results_dir, \"good_rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "with open(good_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(good_rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "runtime_in_seconds = runtime.total_seconds()\n",
    "\n",
    "print(good_rewards_myopic)\n",
    "print(good_rewards_proactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(f\"Myopic model saved to \\n{model_path_and_name_myopic}\")\n",
    "print(f\"Proactive model saved to \\n{model_path_and_name_proactive}\")\n",
    "# Copy the src/environment.py to the results folder under the name used_env.py\n",
    "shutil.copy('../src/environment.py', os.path.join(results_dir, 'used_e_file.py'))\n",
    "shutil.copy('../src/config.py', os.path.join(results_dir, 'used_c_file.py'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the good rewards from the good_rewards_myopic and good_rewards_proactive\n",
    "good_rewards_myopic_values = good_rewards_myopic.values()   \n",
    "good_rewards_proactive_values = good_rewards_proactive.values()\n",
    "\n",
    "print(good_rewards_myopic_values)\n",
    "\n",
    "epsilon_values_myopic_values = [epsilon for episode, epsilon in epsilon_values_myopic]\n",
    "epsilon_values_proactive_values = [epsilon for episode, epsilon in epsilon_values_proactive]\n",
    "\n",
    "episode_numbers_myopic = [episode for episode, epsilon in epsilon_values_myopic]\n",
    "episode_numbers_proactive = [episode for episode, epsilon in epsilon_values_proactive]\n",
    "\n",
    "# Plot the good rewards and epsilon values\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(good_rewards_myopic_values, label='Myopic Rewards', color='C0')\n",
    "ax1.plot(good_rewards_proactive_values, label='Proactive Rewards', color='C1')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_title('Rewards per Episode')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episode_numbers_myopic, epsilon_values_myopic_values, label='Myopic Epsilon Values', color='C2')\n",
    "ax2.plot(episode_numbers_proactive, epsilon_values_proactive_values, label='Proactive Epsilon Values', color='C3')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'good_rewards_per_episode_batch.png'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the good rewards from the good_rewards_myopic and good_rewards_proactive\n",
    "good_rewards_myopic_values = good_rewards_myopic.values()   \n",
    "good_rewards_proactive_values = good_rewards_proactive.values()\n",
    "\n",
    "window_size = 20\n",
    "\n",
    "# # Smooth out the rewards with a window of 100\n",
    "# window = np.ones(window_size) / window_size\n",
    "# smoothed_good_rewards_myopic_values = np.convolve(np.array(list(good_rewards_myopic_values)), window, mode='same')\n",
    "# smoothed_good_rewards_proactive_values = np.convolve(np.array(list(good_rewards_proactive_values)), window, mode='same')\n",
    "\n",
    "# ==== Gaussian Filter ====\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "smoothed_good_rewards_myopic_values = gaussian_filter1d(np.array(list(good_rewards_myopic_values)), sigma=window_size / 2)\n",
    "smoothed_good_rewards_proactive_values = gaussian_filter1d(np.array(list(good_rewards_proactive_values)), sigma=window_size / 2)\n",
    "\n",
    "# # ==== Univariate Spline ====\n",
    "# from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "# x_myopic = np.arange(len(good_rewards_myopic_values))\n",
    "# x_proactive = np.arange(len(good_rewards_proactive_values))\n",
    "\n",
    "# spline_myopic = UnivariateSpline(x_myopic, list(good_rewards_myopic_values), s=window_size)\n",
    "# spline_proactive = UnivariateSpline(x_proactive, list(good_rewards_proactive_values), s=window_size)\n",
    "\n",
    "# smoothed_good_rewards_myopic_values = spline_myopic(x_myopic)\n",
    "# smoothed_good_rewards_proactive_values = spline_proactive(x_proactive)\n",
    "\n",
    "# # ==== Savitzky-Golay Filter ====\n",
    "# from scipy.signal import savgol_filter\n",
    "\n",
    "# # window_size must be odd and greater than the polynomial order\n",
    "# polynomial_order = 2\n",
    "# window_size = max(3, window_size | 1)  # Ensure it's odd\n",
    "# smoothed_good_rewards_myopic_values = savgol_filter(list(good_rewards_myopic_values), window_size, polynomial_order)\n",
    "# smoothed_good_rewards_proactive_values = savgol_filter(list(good_rewards_proactive_values), window_size, polynomial_order)\n",
    "\n",
    "\n",
    "# # ==== Butterworth Filter ====\n",
    "# from scipy.signal import butter, filtfilt\n",
    "\n",
    "# # Design a low-pass filter\n",
    "# b, a = butter(N=2, Wn=0.1)  # Adjust Wn for cutoff frequency\n",
    "# smoothed_good_rewards_myopic_values = filtfilt(b, a, list(good_rewards_myopic_values))\n",
    "# smoothed_good_rewards_proactive_values = filtfilt(b, a, list(good_rewards_proactive_values))\n",
    "\n",
    "\n",
    "# epsilon_values_myopic_values = [epsilon for episode, epsilon in epsilon_values_myopic]\n",
    "# epsilon_values_proactive_values = [epsilon for episode, epsilon in epsilon_values_proactive]\n",
    "\n",
    "# episode_numbers_myopic = [episode for episode, epsilon in epsilon_values_myopic]\n",
    "# episode_numbers_proactive = [episode for episode, epsilon in epsilon_values_proactive]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the smoothed good rewards and epsilon values\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(smoothed_good_rewards_myopic_values, label='Myopic Rewards', color='blue')\n",
    "ax1.plot(smoothed_good_rewards_proactive_values, label='Proactive Rewards', color='orange')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_title('Rewards per Episode')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episode_numbers_myopic, epsilon_values_myopic_values, label='Myopic Epsilon Values', color='C2')\n",
    "ax2.plot(episode_numbers_proactive, epsilon_values_proactive_values, label='Proactive Epsilon Values', color='C3')\n",
    "ax2.set_ylabel('Epsilon')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'smoothed_good_rewards_per_episode_batch.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Process training rewards per episode for myopic\n",
    "avg_rewards_per_episode_myopic = []\n",
    "episodes_myopic = []\n",
    "\n",
    "for episode in sorted(rewards_myopic.keys()):\n",
    "    # Calculate the total reward across all scenarios for this episode\n",
    "    total_reward = sum(rewards_myopic[episode].values()) if isinstance(rewards_myopic[episode], dict) else rewards_myopic[episode]\n",
    "    avg_rewards_per_episode_myopic.append(total_reward)\n",
    "    episodes_myopic.append(episode + 1)  # Convert 0-based to 1-based indexing\n",
    "\n",
    "# Process training rewards per episode for proactive\n",
    "avg_rewards_per_episode_proactive = []\n",
    "episodes_proactive = []\n",
    "\n",
    "for episode in sorted(rewards_proactive.keys()):\n",
    "    # Calculate the total reward across all scenarios for this episode\n",
    "    total_reward = sum(rewards_proactive[episode].values()) if isinstance(rewards_proactive[episode], dict) else rewards_proactive[episode]\n",
    "    avg_rewards_per_episode_proactive.append(total_reward)\n",
    "    episodes_proactive.append(episode + 1)  # Convert 0-based to 1-based indexing\n",
    "\n",
    "# Extract test rewards for myopic\n",
    "test_episodes_myopic = [ep + 1 for ep, _ in test_rewards_myopic]  # Add 1 for 1-based indexing\n",
    "test_avg_rewards_myopic = [reward for _, reward in test_rewards_myopic]\n",
    "\n",
    "# Extract test rewards for proactive\n",
    "test_episodes_proactive = [ep + 1 for ep, _ in test_rewards_proactive]\n",
    "test_avg_rewards_proactive = [reward for _, reward in test_rewards_proactive]\n",
    "\n",
    "# Plot 1: Myopic (Training and Testing)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Myopic Rewards')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'myopic_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Proactive (Training and Testing)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Proactive Rewards')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'proactive_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Training Rewards (Myopic and Proactive)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Training Rewards (Myopic vs Proactive)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'training_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Testing Rewards (Myopic and Proactive)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Testing Rewards (Myopic vs Proactive)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'testing_only.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionaries to store rewards for each scenario\n",
    "scenario_rewards_myopic = defaultdict(list)\n",
    "scenario_rewards_proactive = defaultdict(list)\n",
    "\n",
    "# Group rewards by scenario for the myopic model\n",
    "for episode, scenarios in rewards_myopic.items():\n",
    "    if isinstance(scenarios, dict):  # Check if rewards are stored per scenario\n",
    "        for scenario, reward in scenarios.items():\n",
    "            scenario_id = scenario[-2:]  # Extract the last two characters of the scenario\n",
    "            scenario_rewards_myopic[scenario_id].append(reward)\n",
    "\n",
    "# Group rewards by scenario for the proactive model\n",
    "for episode, scenarios in rewards_proactive.items():\n",
    "    if isinstance(scenarios, dict):  # Check if rewards are stored per scenario\n",
    "        for scenario, reward in scenarios.items():\n",
    "            scenario_id = scenario[-2:]  # Extract the last two characters of the scenario\n",
    "            scenario_rewards_proactive[scenario_id].append(reward)\n",
    "\n",
    "# Calculate the average reward for each scenario for both models\n",
    "avg_rewards_per_scenario_myopic = {scenario: np.mean(rewards) for scenario, rewards in scenario_rewards_myopic.items()}\n",
    "avg_rewards_per_scenario_proactive = {scenario: np.mean(rewards) for scenario, rewards in scenario_rewards_proactive.items()}\n",
    "\n",
    "\n",
    "# Get all unique scenarios\n",
    "all_scenarios = set(avg_rewards_per_scenario_myopic.keys()).union(avg_rewards_per_scenario_proactive.keys())\n",
    "sorted_scenarios = sorted(all_scenarios)\n",
    "\n",
    "# Extract the sorted average rewards for both models\n",
    "sorted_avg_rewards_myopic = [avg_rewards_per_scenario_myopic.get(scenario, 0) for scenario in sorted_scenarios]\n",
    "sorted_avg_rewards_proactive = [avg_rewards_per_scenario_proactive.get(scenario, 0) for scenario in sorted_scenarios]\n",
    "\n",
    "# Plot a grouped bar chart with scenarios on the x-axis and average rewards on the y-axis\n",
    "x = np.arange(len(sorted_scenarios))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width / 2, sorted_avg_rewards_myopic, width, label='Myopic', color='blue')\n",
    "plt.bar(x + width / 2, sorted_avg_rewards_proactive, width, label='Proactive', color='orange')\n",
    "\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Scenario')\n",
    "plt.xticks(x, sorted_scenarios)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_scenario.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the new folder within results_dir\n",
    "scenario_results_dir = os.path.join(results_dir, 'plots', 'reward-plots-per-scenario')\n",
    "os.makedirs(scenario_results_dir, exist_ok=True)\n",
    "\n",
    "# Get all unique scenario IDs from both models\n",
    "all_scenario_ids = set(scenario_rewards_myopic.keys()).union(scenario_rewards_proactive.keys())\n",
    "\n",
    "# Iterate over all scenarios\n",
    "for scenario_id in all_scenario_ids:\n",
    "    # Get rewards lists for both models\n",
    "    rewards_list_myopic = scenario_rewards_myopic.get(scenario_id, [])\n",
    "    rewards_list_proactive = scenario_rewards_proactive.get(scenario_id, [])\n",
    "    \n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot for Myopic model\n",
    "    if rewards_list_myopic:\n",
    "        window_myopic = min(10, len(rewards_list_myopic))  # Adjust window size as needed\n",
    "        smoothed_rewards_myopic = np.convolve(rewards_list_myopic, np.ones(window_myopic) / window_myopic, mode='valid')\n",
    "        plt.plot(range(len(rewards_list_myopic)), rewards_list_myopic, label='Myopic Reward', color='blue', alpha=0.3)\n",
    "        plt.plot(range(len(smoothed_rewards_myopic)), smoothed_rewards_myopic, label='Myopic Smoothed Reward', color='blue')\n",
    "    \n",
    "    # Plot for Proactive model\n",
    "    if rewards_list_proactive:\n",
    "        window_proactive = min(10, len(rewards_list_proactive))  # Adjust window size as needed\n",
    "        smoothed_rewards_proactive = np.convolve(rewards_list_proactive, np.ones(window_proactive) / window_proactive, mode='valid')\n",
    "        plt.plot(range(len(rewards_list_proactive)), rewards_list_proactive, label='Proactive Reward', color='orange', alpha=0.3)\n",
    "        plt.plot(range(len(smoothed_rewards_proactive)), smoothed_rewards_proactive, label='Proactive Smoothed Reward', color='orange')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Reward per Episode for Scenario {scenario_id}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = f'average_reward_for_scenario_{scenario_id}.png'\n",
    "    plt.savefig(os.path.join(scenario_results_dir, plot_filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total_timesteps_proactive:\", total_timesteps_proactive)\n",
    "print(\"total_timesteps_myopic:\", total_timesteps_myopic)\n",
    "\n",
    "# Extract epsilon values and their corresponding timesteps\n",
    "timesteps_myopic = [i for i, _ in enumerate(epsilon_values_myopic)]\n",
    "epsilon_values_myopic_only = [epsilon for _, epsilon in epsilon_values_myopic]\n",
    "\n",
    "timesteps_proactive = [i for i, _ in enumerate(epsilon_values_proactive)]\n",
    "epsilon_values_proactive_only = [epsilon for _, epsilon in epsilon_values_proactive]\n",
    "\n",
    "# Plot the epsilon values over the episodes for both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for Proactive model\n",
    "plt.plot(timesteps_proactive, epsilon_values_proactive_only, label='Proactive Epsilon', color='C1', linestyle='--')\n",
    "plt.plot(timesteps_myopic, epsilon_values_myopic_only, label='Myopic Epsilon', color='C0', linestyle=':')\n",
    "\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Epsilon Value')\n",
    "plt.title('Epsilon Value over Timesteps for Both Models')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'epsilon_value_over_timesteps_both_models.png'))\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
