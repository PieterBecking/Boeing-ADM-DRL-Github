{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "import os\n",
    "import torch as th\n",
    "print(th.backends.mps.is_available())\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "print(gym.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from scripts.utils import *\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# vectorized environment\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "print(MAX_AIRCRAFT)\n",
    "import warnings\n",
    "\n",
    "from stable_baselines3.common.utils import polyak_update\n",
    "from stable_baselines3.common.logger import configure  # Import the configure function\n",
    "\n",
    "\n",
    "import stable_baselines3\n",
    "print(stable_baselines3.__version__)\n",
    "\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "import gym\n",
    "print(\"Stable Baselines3 version:\", stable_baselines3.__version__)\n",
    "print(\"Gym version:\", gym.__version__)\n",
    "\n",
    "import math\n",
    "import subprocess\n",
    "import re\n",
    "import subprocess\n",
    "import platform\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Settings\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64*4\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_DECAY_RATE = 0.000005\n",
    "MAX_TIMESTEPS = 50000         # maximum number of timesteps per episode (not relevant here)\n",
    "\n",
    "LEARNING_STARTS = 0 # means that the model will start learning after 1000 timesteps\n",
    "TRAIN_FREQ = 4 # means that the model will be trained every 4 timesteps\n",
    "\n",
    "N_EPISODES = 100          # number of episodes PER TRAINING SCENARIO\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256*2,256]) # hidden layer sizes\n",
    "\n",
    "\n",
    "\n",
    "TRAINING_FOLDERS_PATH = '../data/Training/1k-3ac-12f-1dis-F/'\n",
    "TESTING_FOLDERS_PATH = '../data/Testing/1k-3ac-12f-1dis-F/'\n",
    "\n",
    "\n",
    "# print info on the device used like macbook pro m1\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    device = th.device('cuda')\n",
    "elif th.backends.mps.is_available():\n",
    "    device = th.device('mps')\n",
    "else:\n",
    "    device = th.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# code to see what hardware is available\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Number of GPUs available\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "# Get the name of the current GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU name:\", torch.cuda.get_device_name())\n",
    "\n",
    "# Check if cuDNN is enabled\n",
    "print(\"cuDNN enabled:\", torch.backends.cudnn.enabled)\n",
    "\n",
    "# Convert device to a string representation for comparison\n",
    "device_str = str(device).lower()  # Convert to string and lowercase\n",
    "\n",
    "print(\"Device:\", device_str)\n",
    "\n",
    "if device_str == 'mps':\n",
    "    print(\"Using MacBook M1\")\n",
    "    macbook_info = get_macbook_info()\n",
    "    gpu_info = get_gpu_info()\n",
    "    device_info = {**macbook_info, **gpu_info}\n",
    "elif device_str == 'cuda':\n",
    "    print(\"Using GPU\")\n",
    "    # Get gpu name and short info\n",
    "    device_info = get_gpu_info()\n",
    "    \n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device_info = get_macbook_info()\n",
    "\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "\n",
    "\n",
    "# Verify folders exists\n",
    "if not os.path.exists(TRAINING_FOLDERS_PATH):\n",
    "    raise FileNotFoundError(f'Training folder not found at {TRAINING_FOLDERS_PATH}')\n",
    "\n",
    "# print all folders in the training folder\n",
    "training_folders = []\n",
    "for folder in os.listdir(TRAINING_FOLDERS_PATH):\n",
    "    if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder)):\n",
    "        training_folders.append(folder)\n",
    "\n",
    "\n",
    "        \n",
    "last_folder = os.path.basename(os.path.normpath(TRAINING_FOLDERS_PATH))\n",
    "\n",
    "num_days_trained_on = N_EPISODES * len(training_folders)\n",
    "print(f'Training on {num_days_trained_on} days of data ({N_EPISODES} episodes of {len(training_folders)} scenarios)')\n",
    "\n",
    "# Function to format the number of days\n",
    "def format_days(days):\n",
    "    if days >= 1000000:\n",
    "        return f\"{math.floor(days/1000000)}M\"\n",
    "    elif days >= 1000:\n",
    "        return f\"{math.floor(days/1000)}k\"\n",
    "    else:\n",
    "        return str(days)\n",
    "\n",
    "how_much = format_days(num_days_trained_on)\n",
    "\n",
    "model_name = last_folder\n",
    "print('Model name:', model_name)\n",
    "model_version = get_model_version(model_name)\n",
    "MODEL_SAVE_PATH = f'../trained_models/{model_name}-{how_much}-{model_version}.zip'\n",
    "\n",
    "print('Model will be saved to:', MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "# Create a new results directory with thwe current datetime\n",
    "now = datetime.now()\n",
    "folder_name = now.strftime('%Y%m%d-%H-%M')\n",
    "results_dir = os.path.join('..', 'results', folder_name)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# create a new folder within results_dir called plots\n",
    "os.makedirs(os.path.join(results_dir, 'plots'), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated epsilon decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environment_myopic import AircraftDisruptionEnv\n",
    "\n",
    "# Initialize variables\n",
    "total_timesteps_per_batch = 0\n",
    "timesteps_per_scenario = []\n",
    "\n",
    "# List all the scenario folders in Data/Training\n",
    "scenario_folders = [\n",
    "    os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "    for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "    if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "]\n",
    "\n",
    "# Simulate one batch with a random agent\n",
    "for scenario_folder in scenario_folders:\n",
    "\n",
    "    # Load the data for the current scenario\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize the environment with the new scenario\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict,\n",
    "        flights_dict,\n",
    "        rotations_dict,\n",
    "        alt_aircraft_dict,\n",
    "        config_dict\n",
    "    )\n",
    "\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    timesteps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Random action from valid actions\n",
    "        action_mask = obs['action_mask']\n",
    "        valid_actions = np.where(action_mask == 1)[0]\n",
    "        action = np.random.choice(valid_actions)\n",
    "\n",
    "        # Take the action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        timesteps += 1\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Accumulate timesteps\n",
    "    total_timesteps_per_batch += timesteps\n",
    "    timesteps_per_scenario.append(timesteps)\n",
    "\n",
    "# Estimate total timesteps for the entire training process\n",
    "total_timesteps_estimate = total_timesteps_per_batch * N_EPISODES\n",
    "\n",
    "print(f\"Estimated total timesteps for training: {total_timesteps_estimate}\")\n",
    "\n",
    "# Generate epsilon values over the estimated total timesteps\n",
    "epsilon_values_estimate = []\n",
    "epsilon = EPSILON_START\n",
    "min_epsilon_reached_at = 0\n",
    "\n",
    "for t in range(int(total_timesteps_estimate)):\n",
    "    epsilon = max(EPSILON_MIN, epsilon * (1 - EPSILON_DECAY_RATE))\n",
    "    epsilon_values_estimate.append(epsilon)\n",
    "    \n",
    "    # Record when epsilon reaches the minimum value\n",
    "    if epsilon == EPSILON_MIN:\n",
    "        min_epsilon_reached_at = t\n",
    "        # Extend the list with EPSILON_MIN for the remaining timesteps\n",
    "        epsilon_values_estimate.extend([EPSILON_MIN] * (int(total_timesteps_estimate) - t - 1))\n",
    "        break\n",
    "\n",
    "# Calculate the percentage of timesteps where epsilon reaches its minimum value\n",
    "percentage_min_epsilon_reached = (min_epsilon_reached_at / total_timesteps_estimate) * 100\n",
    "print(f\"Epsilon reaches its minimum value at {percentage_min_epsilon_reached:.2f}% of total timesteps.\")\n",
    "\n",
    "# Plot the estimated epsilon decay curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilon_values_estimate)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Epsilon Value')\n",
    "plt.title('Estimated Epsilon Decay over Timesteps')\n",
    "plt.show()\n",
    "\n",
    "# Interrupt the code if epsilon reaches its minimum value below 70% of the total timesteps\n",
    "#if percentage_min_epsilon_reached < 70:\n",
    "#    raise RuntimeError(\"Epsilon reaches its minimum value below 70% of total timesteps. Aborting the process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment_myopic import AircraftDisruptionEnv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "# Initialize variables\n",
    "rewards = []\n",
    "test_rewards = []\n",
    "epsilon_values = []\n",
    "total_timesteps = 0\n",
    "action_sequences = {\n",
    "    os.path.join(TRAINING_FOLDERS_PATH, folder): {\n",
    "        \"best_actions\": [],\n",
    "        \"best_reward\": float('-inf'),\n",
    "        \"worst_actions\": [],\n",
    "        \"worst_reward\": float('inf')\n",
    "    }\n",
    "    for folder in training_folders\n",
    "}\n",
    "\n",
    "# Variable to control how often cross-validation occurs\n",
    "CROSS_VAL_INTERVAL = 10  # Perform cross-validation every 5 episodes\n",
    "\n",
    "def cross_validate_on_test_data(model, current_episode):\n",
    "    test_scenario_folders = [\n",
    "        os.path.join(TESTING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TESTING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TESTING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "    total_test_reward = 0\n",
    "    for test_scenario_folder in test_scenario_folders:\n",
    "        # Load data\n",
    "        data_dict = load_scenario_data(test_scenario_folder)\n",
    "        aircraft_dict = data_dict['aircraft']\n",
    "        flights_dict = data_dict['flights']\n",
    "        rotations_dict = data_dict['rotations']\n",
    "        alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "        config_dict = data_dict['config']\n",
    "\n",
    "        # Update the environment with the new scenario (by reinitializing it)\n",
    "        env = AircraftDisruptionEnv(\n",
    "            aircraft_dict,\n",
    "            flights_dict,\n",
    "            rotations_dict,\n",
    "            alt_aircraft_dict,\n",
    "            config_dict\n",
    "        )\n",
    "        model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "        # Evaluate the model on the test scenario without training\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        done_flag = False\n",
    "        total_reward = 0\n",
    "        timesteps = 0\n",
    "\n",
    "        while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "            # Get the action mask from the environment\n",
    "            action_mask = obs['action_mask']\n",
    "\n",
    "            # Convert observation to float32\n",
    "            obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "            # Preprocess observation and get Q-values\n",
    "            obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "            q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "            # Apply the action mask (set invalid actions to -inf)\n",
    "            masked_q_values = q_values.copy()\n",
    "            masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "            # Select the action with the highest masked Q-value\n",
    "            action = np.argmax(masked_q_values)\n",
    "\n",
    "            # Take the selected action in the environment\n",
    "            result = env.step(action)\n",
    "\n",
    "            # Unpack the result\n",
    "            obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "            done_flag = terminated or truncated\n",
    "\n",
    "            # Accumulate the reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the current observation\n",
    "            obs = obs_next\n",
    "\n",
    "            timesteps += 1\n",
    "\n",
    "            if done_flag:\n",
    "                break\n",
    "\n",
    "        total_test_reward += total_reward\n",
    "\n",
    "    # Compute average test reward\n",
    "    avg_test_reward = total_test_reward / len(test_scenario_folders)\n",
    "    test_rewards.append((current_episode, avg_test_reward))\n",
    "\n",
    "def train_dqn_agent():\n",
    "    # List all the scenario folders in Data/Training\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    global total_timesteps\n",
    "\n",
    "    # Initialize the DQN\n",
    "    dummy_scenario_folder = scenario_folders[0]\n",
    "    data_dict = load_scenario_data(dummy_scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict,\n",
    "        flights_dict,\n",
    "        rotations_dict,\n",
    "        alt_aircraft_dict,\n",
    "        config_dict\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        policy='MultiInputPolicy',\n",
    "        env=env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "        verbose=0,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model._logger = logger\n",
    "\n",
    "    # Training loop over the number of episodes\n",
    "    for episode in range(N_EPISODES):\n",
    "        # Cycle through all the scenario folders\n",
    "        for scenario_folder in scenario_folders:\n",
    "            if DEBUG_MODE_TRAINING:\n",
    "                print(f\"Training on scenario {scenario_folder}\")\n",
    "\n",
    "            # Load the data for the current scenario\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            # Update the environment with the new scenario (by reinitializing it)\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict\n",
    "            )\n",
    "            model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "            # Reset the environment\n",
    "            obs, _ = env.reset()  # Extract the observation (obs) and ignore the info (_)\n",
    "\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            timesteps = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "                model.exploration_rate = epsilon\n",
    "\n",
    "                # Get the action mask from the environment\n",
    "                action_mask = obs['action_mask']\n",
    "\n",
    "                # Convert observation to float32\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "                # Preprocess observation and get Q-values\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "                # Apply the action mask (set invalid actions to -inf)\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "                # Select an action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    # Exploration: choose a random valid action\n",
    "                    valid_actions = np.where(action_mask == 1)[0]\n",
    "                    action = np.random.choice(valid_actions)\n",
    "                else:\n",
    "                    # Exploitation: choose the action with the highest masked Q-value\n",
    "                    action = np.argmax(masked_q_values)\n",
    "\n",
    "                # Take the selected action in the environment\n",
    "                result = env.step(action)\n",
    "\n",
    "                # Unpack the result (5 values)\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "                # Combine the terminated and truncated flags into a single done flag\n",
    "                done_flag = terminated or truncated\n",
    "\n",
    "                # Store the action\n",
    "                action_sequence.append(action)\n",
    "\n",
    "                # Accumulate the reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Add the transition to the replay buffer\n",
    "                model.replay_buffer.add(\n",
    "                    obs=obs,\n",
    "                    next_obs=obs_next,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    done=done_flag,\n",
    "                    infos=[info]\n",
    "                )\n",
    "\n",
    "                # Update the current observation\n",
    "                obs = obs_next\n",
    "\n",
    "                # Update epsilon (exploration rate)\n",
    "                epsilon = max(EPSILON_MIN, epsilon * (1 - EPSILON_DECAY_RATE))\n",
    "                epsilon_values.append(epsilon)\n",
    "\n",
    "                timesteps += 1\n",
    "                total_timesteps += 1\n",
    "\n",
    "                # Training\n",
    "                if total_timesteps > model.learning_starts and total_timesteps % TRAIN_FREQ == 0:\n",
    "                    # Perform a training step\n",
    "                    model.train(gradient_steps=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "                # Update target network\n",
    "                if total_timesteps % model.target_update_interval == 0:\n",
    "                    polyak_update(model.q_net.parameters(), model.q_net_target.parameters(), model.tau)\n",
    "                    # Copy batch norm stats\n",
    "                    polyak_update(model.batch_norm_stats, model.batch_norm_stats_target, 1.0)\n",
    "\n",
    "                # Check if the episode is done\n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            # Store the total reward for the episode with the scenario specified\n",
    "            rewards.append((episode, scenario_folder, total_reward))\n",
    "\n",
    "            # Update the worst and best action sequences\n",
    "            if total_reward < action_sequences[scenario_folder][\"worst_reward\"]:\n",
    "                action_sequences[scenario_folder][\"worst_actions\"] = action_sequence\n",
    "                action_sequences[scenario_folder][\"worst_reward\"] = total_reward\n",
    "\n",
    "            if total_reward > action_sequences[scenario_folder][\"best_reward\"]:\n",
    "                action_sequences[scenario_folder][\"best_actions\"] = action_sequence\n",
    "                action_sequences[scenario_folder][\"best_reward\"] = total_reward\n",
    "\n",
    "        # Perform cross-validation at specified intervals\n",
    "        if (episode + 1) % CROSS_VAL_INTERVAL == 0:\n",
    "            cross_validate_on_test_data(model, episode + 1)\n",
    "\n",
    "        print(f\"({episode + 1}/{N_EPISODES})\")\n",
    "\n",
    "    # Save the model after training\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "\n",
    "train_dqn_agent()\n",
    "\n",
    "# Output total timesteps\n",
    "print(total_timesteps)\n",
    "\n",
    "# Output scenario action sequences and rewards\n",
    "for scenario, data in action_sequences.items():\n",
    "    print(f\"Scenario: {scenario}, Worst Reward: {data['worst_reward']}, Best Reward: {data['best_reward']}\")\n",
    "    print(f\"Worst Action Sequence: {data['worst_actions']}\")\n",
    "    print(f\"Best Action Sequence: {data['best_actions']}\")\n",
    "\n",
    "    # Save the action sequences to a CSV file\n",
    "    save_best_and_worst_to_csv(\n",
    "        scenario,\n",
    "        MODEL_SAVE_PATH,\n",
    "        data['worst_actions'],\n",
    "        data['best_actions'],\n",
    "        data['worst_reward'],\n",
    "        data['best_reward']\n",
    "    )\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "runtime_in_seconds = runtime.total_seconds()\n",
    "\n",
    "# create new folder within results_dir called action_sequences\n",
    "os.makedirs(os.path.join(results_dir, 'action_sequences'), exist_ok=True)\n",
    "\n",
    "# Collect hyperparameters and other details\n",
    "hyperparameters = {\n",
    "    'LEARNING_RATE': LEARNING_RATE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'BUFFER_SIZE': BUFFER_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'TARGET_UPDATE_INTERVAL': TARGET_UPDATE_INTERVAL,\n",
    "    'EPSILON_START': EPSILON_START,\n",
    "    'EPSILON_MIN': EPSILON_MIN,\n",
    "    'EPSILON_DECAY_RATE': EPSILON_DECAY_RATE,\n",
    "    'MAX_TIMESTEPS': MAX_TIMESTEPS,\n",
    "    'LEARNING_STARTS': LEARNING_STARTS,\n",
    "    'TRAIN_FREQ': TRAIN_FREQ,\n",
    "    'N_EPISODES': N_EPISODES,\n",
    "    'NEURAL_NET_STRUCTURE': NEURAL_NET_STRUCTURE,\n",
    "    'TRAINING_FOLDERS_PATH': TRAINING_FOLDERS_PATH,\n",
    "    'model_name': model_name,\n",
    "    'model_version': model_version,\n",
    "    'MODEL_SAVE_PATH': MODEL_SAVE_PATH,\n",
    "    'runtime_in_seconds': runtime_in_seconds,\n",
    "    'runtime_in_hh:mm:ss': str(runtime) if runtime_in_seconds > 0 else \"0:00:00\",\n",
    "    'total_timesteps': total_timesteps,\n",
    "    'CROSS_VAL_INTERVAL': CROSS_VAL_INTERVAL,\n",
    "\n",
    "    # values from config.py\n",
    "    'MAX_AIRCRAFT': MAX_AIRCRAFT,\n",
    "    'MAX_FLIGHTS_PER_AIRCRAFT': MAX_FLIGHTS_PER_AIRCRAFT,\n",
    "    'TIMESTEP_HOURS': TIMESTEP_HOURS,\n",
    "    'DUMMY_VALUE': DUMMY_VALUE,\n",
    "    'RESOLVED_CONFLICT_REWARD': RESOLVED_CONFLICT_REWARD,\n",
    "    'DELAY_MINUTE_PENALTY': DELAY_MINUTE_PENALTY,\n",
    "    'MAX_DELAY_PENALTY': MAX_DELAY_PENALTY,\n",
    "    'NO_ACTION_PENALTY': NO_ACTION_PENALTY,\n",
    "    'CANCELLED_FLIGHT_PENALTY': CANCELLED_FLIGHT_PENALTY,\n",
    "    'MIN_TURN_TIME': MIN_TURN_TIME,\n",
    "\n",
    "    # device info\n",
    "    'device': device,\n",
    "    'device_info': device_info\n",
    "\n",
    "}\n",
    "\n",
    "# Save hyperparameters to CSV\n",
    "hyperparameters_df = pd.DataFrame(list(hyperparameters.items()), columns=['Parameter', 'Value'])\n",
    "hyperparameters_df.to_csv(os.path.join(results_dir, 'hyperparameters.csv'), index=False)\n",
    "\n",
    "# Save the action sequences to CSV files in the results directory\n",
    "for scenario, data in action_sequences.items():\n",
    "    scenario_name = os.path.basename(scenario)\n",
    "    worst_actions_df = pd.DataFrame(data['worst_actions'], columns=['Action'])\n",
    "    best_actions_df = pd.DataFrame(data['best_actions'], columns=['Action'])\n",
    "    worst_actions_df.to_csv(os.path.join(results_dir, 'action_sequences', f'{scenario_name}_worst_actions.csv'), index=False)\n",
    "    best_actions_df.to_csv(os.path.join(results_dir, 'action_sequences', f'{scenario_name}_best_actions.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated input\n",
    "print(f\"num_days_trained_on {num_days_trained_on}\")\n",
    "rewards_per_day = len(rewards) // num_days_trained_on\n",
    "\n",
    "# Print out the structure of the rewards list\n",
    "print(f\"Trained on {num_days_trained_on} days of data ({len(training_folders)} unique scenarios)\")\n",
    "\n",
    "# Process training rewards per episode\n",
    "episode_rewards = {}\n",
    "for episode, scenario_folder, total_reward in rewards:\n",
    "    if episode not in episode_rewards:\n",
    "        episode_rewards[episode] = []\n",
    "    episode_rewards[episode].append(total_reward)\n",
    "\n",
    "avg_rewards_per_episode = []\n",
    "episodes = []\n",
    "for episode in sorted(episode_rewards.keys()):\n",
    "    avg_reward = np.mean(episode_rewards[episode])\n",
    "    avg_rewards_per_episode.append(avg_reward)\n",
    "    episodes.append(episode + 1)  # episode numbers start from 0, so add 1\n",
    "\n",
    "# Extract test rewards\n",
    "test_episodes = [ep for ep, _ in test_rewards]\n",
    "test_avg_rewards = [reward for _, reward in test_rewards]\n",
    "\n",
    "# Plot the average rewards over the episodes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episodes, avg_rewards_per_episode, label='Training Average Reward')\n",
    "\n",
    "# Plot test rewards\n",
    "plt.plot(test_episodes, test_avg_rewards, label='Test Average Reward', marker='o', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_episode.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to store rewards for each scenario\n",
    "scenario_rewards = defaultdict(list)\n",
    "\n",
    "# Iterate through the rewards list and group by scenario (last two characters)\n",
    "for _, scenario, reward in rewards:\n",
    "    scenario_id = scenario[-2:]  # Get the last two characters (e.g., '01', '02', etc.)\n",
    "    scenario_rewards[scenario_id].append(reward)\n",
    "\n",
    "# calculate the average reward for each scenario\n",
    "avg_rewards_per_batch_per_scenario = {scenario: np.mean(rewards) for scenario, rewards in scenario_rewards.items()}\n",
    "sorted_scenarios = sorted(avg_rewards_per_batch_per_scenario.keys())\n",
    "\n",
    "# Extract the sorted average rewards\n",
    "sorted_avg_rewards_per_batch = [avg_rewards_per_batch_per_scenario[scenario] for scenario in sorted_scenarios]\n",
    "\n",
    "# Plot a bar chart with scenarios on the x-axis and average rewards on the y-axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_scenarios, sorted_avg_rewards_per_batch)\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Scenario')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_scenario.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the new folder within results_dir\n",
    "scenario_results_dir = os.path.join(results_dir, 'plots', 'reward-plots-per-scenario')\n",
    "os.makedirs(scenario_results_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over all scenarios in scenario_rewards\n",
    "for scenario_id, rewards_list in scenario_rewards.items():\n",
    "    # Set the smoothing window (adjust the size as needed)\n",
    "    window = 1000\n",
    "    # Ensure the window size does not exceed the length of the rewards list\n",
    "    if len(rewards_list) < window:\n",
    "        window = len(rewards_list)\n",
    "    smoothed_rewards = np.convolve(rewards_list, np.ones(window) / window, mode='same')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rewards_list, label='Reward')\n",
    "    plt.plot(smoothed_rewards, label='Smoothed Average Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward per Episode for Scenario ' + scenario_id)\n",
    "    plot_filename = f'average_reward_for_scenario_{scenario_id}.png'\n",
    "    plt.savefig(os.path.join(scenario_results_dir, plot_filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total_timesteps\", total_timesteps)\n",
    "# Plot the epsilon values over the episodes 10, 6\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(epsilon_values)), epsilon_values)\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Epsilon Value')\n",
    "plt.title('Epsilon Value over Timesteps')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'epsilon_value_over_timesteps.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
