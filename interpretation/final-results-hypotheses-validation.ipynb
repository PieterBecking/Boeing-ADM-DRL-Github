{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario ID: 4\n",
      "Data Folder: ../data/Testing/6ac-700-diverse/\n",
      "                          Scenario  ScenarioSlack  TotalFlights  \\\n",
      "0    deterministic_na_Scenario_001       0.349048            21   \n",
      "1    deterministic_na_Scenario_002       0.369713            21   \n",
      "2    deterministic_na_Scenario_003       0.420513            13   \n",
      "3    deterministic_na_Scenario_004       0.441493            16   \n",
      "4    deterministic_na_Scenario_005       0.484896            14   \n",
      "..                             ...            ...           ...   \n",
      "695        mixed_high_Scenario_096       0.479683            15   \n",
      "696        mixed_high_Scenario_097       0.460082            13   \n",
      "697        mixed_high_Scenario_098       0.424324            20   \n",
      "698        mixed_high_Scenario_099       0.407037            15   \n",
      "699        mixed_high_Scenario_100       0.428228            18   \n",
      "\n",
      "     TotalFlightMinutes  FullyDisruptedCount  UncertainCount  \\\n",
      "0                  4101                    2               0   \n",
      "1                  3517                    2               0   \n",
      "2                  2712                    2               0   \n",
      "3                  3217                    2               0   \n",
      "4                  2967                    2               0   \n",
      "..                  ...                  ...             ...   \n",
      "695                3278                    1               1   \n",
      "696                2624                    1               1   \n",
      "697                3834                    1               1   \n",
      "698                3202                    1               1   \n",
      "699                3808                    1               1   \n",
      "\n",
      "     AvgAircraftProbability  AvgUncertaintyProbability  TotalAircraft  \n",
      "0                  0.333333                   0.000000              6  \n",
      "1                  0.333333                   0.000000              6  \n",
      "2                  0.333333                   0.000000              6  \n",
      "3                  0.333333                   0.000000              6  \n",
      "4                  0.333333                   0.000000              6  \n",
      "..                      ...                        ...            ...  \n",
      "695                0.309682                   0.858095              6  \n",
      "696                0.270772                   0.624630              6  \n",
      "697                0.310857                   0.865141              6  \n",
      "698                0.302196                   0.813178              6  \n",
      "699                0.262952                   0.577712              6  \n",
      "\n",
      "[700 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "seeds = []\n",
    "for i in range(1, 101):\n",
    "    seeds.append(i)\n",
    "\n",
    "\n",
    "def time_to_minutes(timestr):\n",
    "    # Handle '+1' suffix by removing it before parsing\n",
    "    timestr = timestr.split('+')[0]  # Remove '+1' if present\n",
    "    hh, mm = timestr.split(':')\n",
    "    return int(hh) * 60 + int(mm)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_slack_for_scenario(scenario_data):\n",
    "    \"\"\"\n",
    "    Calculate the slack metric for the given scenario.\n",
    "    \n",
    "    Slack is defined as:\n",
    "        Slack = 1 - (total flight minutes in recovery period / total recovery period aircraft-minutes)\n",
    "    \n",
    "    A slack of 1 means no flights during recovery period.\n",
    "    A slack of 0 means flights occupy the entire recovery period.\n",
    "    \"\"\"\n",
    "    def time_to_minutes(timestr):\n",
    "        # Handle '+1' suffix by removing it before parsing\n",
    "        timestr = timestr.split('+')[0]  # Remove '+1' if present\n",
    "        hh, mm = timestr.split(':')\n",
    "        return int(hh) * 60 + int(mm)\n",
    "\n",
    "    \n",
    "    # Extract scenario start/end times\n",
    "    # We assume the same date for start and end for simplicity.\n",
    "    recovery_start_time_str = scenario_data[\"recovery_start_time\"]  \n",
    "    recovery_end_time_str = scenario_data[\"recovery_end_time\"]      \n",
    "    \n",
    "    recovery_start_minutes = time_to_minutes(recovery_start_time_str)\n",
    "    recovery_end_minutes = time_to_minutes(recovery_end_time_str)\n",
    "    total_recovery_period_minutes = recovery_end_minutes - recovery_start_minutes\n",
    "    \n",
    "    total_aircraft = scenario_data[\"total_aircraft\"]\n",
    "    \n",
    "    # Calculate total flight minutes within the recovery period\n",
    "    flights = scenario_data[\"flights\"]\n",
    "    total_flights = len(flights)\n",
    "    total_flight_minutes_in_recovery = 0\n",
    "    total_flight_minutes_total = 0\n",
    "    \n",
    "    for flight_id, flight_data in flights.items():\n",
    "        dep_time_str = flight_data[\"DepTime\"]  \n",
    "        arr_time_str = flight_data[\"ArrTime\"] \n",
    "        \n",
    "        dep_minutes = time_to_minutes(dep_time_str)\n",
    "        arr_minutes = time_to_minutes(arr_time_str)\n",
    "        \n",
    "        total_flight_minutes_total += arr_minutes - dep_minutes\n",
    "        overlap_start = max(dep_minutes, recovery_start_minutes)\n",
    "        overlap_end = min(arr_minutes, recovery_end_minutes)\n",
    "        \n",
    "        if overlap_end > overlap_start:\n",
    "            flight_overlap = overlap_end - overlap_start\n",
    "        else:\n",
    "            flight_overlap = 0\n",
    "        \n",
    "        total_flight_minutes_in_recovery += flight_overlap\n",
    "    \n",
    "    # Calculate total aircraft-minutes available during the recovery period\n",
    "    total_recovery_aircraft_minutes = total_recovery_period_minutes * total_aircraft\n",
    "    \n",
    "    # Slack calculation\n",
    "    if total_recovery_aircraft_minutes == 0:\n",
    "        slack = 1.0\n",
    "    else:\n",
    "        slack = 1 - (total_flight_minutes_in_recovery / total_recovery_aircraft_minutes)\n",
    "    \n",
    "    return slack, total_flights, total_flight_minutes_total\n",
    "\n",
    "\n",
    "def extract_disruption_stats(scenario_data):\n",
    "    \"\"\"\n",
    "    Extract disruption statistics:\n",
    "    - Count of fully disrupted (prob = 1.0)\n",
    "    - Count of uncertain disruptions (0 < prob < 1.0)\n",
    "    - Average probability across all aircraft (where an aircraft's probability is the max disruption probability it faces, \n",
    "      with 1.0 for fully disrupted and 0.0 if no disruption)\n",
    "    - Average uncertainty probability (average of all disruptions where 0<prob<1.0, excluding 0 and 1)\n",
    "    \"\"\"\n",
    "    disruptions_info = scenario_data.get('disruptions', {})\n",
    "    disruptions_list = disruptions_info.get('disruptions', [])\n",
    "    total_aircraft = disruptions_info.get('total_aircraft', 0)\n",
    "\n",
    "    if total_aircraft == 0:\n",
    "        # No aircraft or no disruptions\n",
    "        return 0, 0, 0.0, 0.0\n",
    "\n",
    "    fully_disrupted_count = sum(1 for d in disruptions_list if d.get('probability', 0.0) == 1.0)\n",
    "    uncertain_disruptions = [d for d in disruptions_list if 0.0 < d.get('probability', 0.0) < 1.0]\n",
    "    uncertain_count = len(uncertain_disruptions)\n",
    "\n",
    "    aircraft_ids = scenario_data.get('aircraft_ids', [])\n",
    "    ac_prob_map = {ac: 0.0 for ac in aircraft_ids}  \n",
    "    \n",
    "    for d in disruptions_list:\n",
    "        ac_id = d.get('aircraft_id')\n",
    "        p = d.get('probability', 0.0)\n",
    "        # Keep the max probability for that aircraft\n",
    "        if ac_id in ac_prob_map:\n",
    "            ac_prob_map[ac_id] = max(ac_prob_map[ac_id], p)\n",
    "\n",
    "    avg_ac_prob = sum(ac_prob_map.values()) / total_aircraft if total_aircraft > 0 else 0.0\n",
    "\n",
    "    # Average uncertainty probability (only consider disruptions where 0<prob<1)\n",
    "    if len(uncertain_disruptions) > 0:\n",
    "        avg_uncertainty_prob = np.mean([d['probability'] for d in uncertain_disruptions])\n",
    "    else:\n",
    "        avg_uncertainty_prob = 0.0\n",
    "\n",
    "    return fully_disrupted_count, uncertain_count, avg_ac_prob, avg_uncertainty_prob, total_aircraft\n",
    "\n",
    "# Path to the scenarios folder\n",
    "scenario_folder_path = \"../logs/scenarios/\"\n",
    "latest_folder = max(\n",
    "    [f for f in os.listdir(scenario_folder_path) if f.startswith(\"scenario_folder_\")],\n",
    "    key=lambda x: int(x.split('_')[-1].replace('.json', ''))\n",
    ")\n",
    "\n",
    "# latest_folder = \"scenario_folder_scenario_74.json\" # Training/6ac-10-superdiverse\n",
    "\n",
    "# latest_folder = \"scenario_folder_scenario_77.json\" # Training/6ac-10000-superdiverse\n",
    "latest_folder = \"scenario_folder_scenario_4.json\" # Testing/6ac-700-diverse\n",
    "\n",
    "file_path = os.path.join(scenario_folder_path, latest_folder)\n",
    "\n",
    "# Extract scenario ID\n",
    "scenario_id = file_path.split('_')[-1].split('.')[0]\n",
    "print(f\"Scenario ID: {scenario_id}\")\n",
    "\n",
    "# Load the JSON data\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the scenarios from the JSON data\n",
    "scenarios = data['outputs']\n",
    "\n",
    "\n",
    "# Extract the data_folder (not strictly necessary for slack calculation, but we print it for context)\n",
    "data_folder = data['data_folder']\n",
    "print(f\"Data Folder: {data_folder}\")\n",
    "\n",
    "# Calculate slack and disruption stats for each scenario and store in a list of dicts\n",
    "results = []\n",
    "for scenario_name, scenario_data in scenarios.items():\n",
    "    scenario_slack, total_flights, total_flight_minutes_total = calculate_slack_for_scenario(scenario_data)\n",
    "    fully_disrupted_count, uncertain_count, avg_ac_prob, avg_uncertain_prob, total_aircraft = extract_disruption_stats(scenario_data)\n",
    "    results.append({\n",
    "        \"Scenario\": scenario_name,\n",
    "        \"ScenarioSlack\": scenario_slack,\n",
    "        \"TotalFlights\": total_flights,\n",
    "        \"TotalFlightMinutes\": total_flight_minutes_total,\n",
    "        \"FullyDisruptedCount\": fully_disrupted_count,\n",
    "        \"UncertainCount\": uncertain_count,\n",
    "        \"AvgAircraftProbability\": avg_ac_prob,\n",
    "        \"AvgUncertaintyProbability\": avg_uncertain_prob,\n",
    "        \"TotalAircraft\": total_aircraft\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "scenarios_df = pd.DataFrame(results)\n",
    "print(scenarios_df)\n",
    "\n",
    "# Save the slack results to CSV\n",
    "# output_file = os.path.join(scenario_folder_path, f\"scenario_slack_metrics_{scenario_id}.csv\")\n",
    "# scenarios_df.to_csv(output_file, index=False)\n",
    "# print(f\"Slack metrics saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "<b>DONE: </b>MERGED DATASET\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Results (After Merging):\n",
      "                              Scenario            Model  Seed  TotalReward  \\\n",
      "139999     stochastic_low_Scenario_093  Greedy Reactive   100       -217.6   \n",
      "130773          mixed_low_Scenario_063  Greedy Reactive    74      24786.2   \n",
      "130772          mixed_low_Scenario_063  Greedy Reactive    73      24786.2   \n",
      "130771          mixed_low_Scenario_063  Greedy Reactive    72      19799.3   \n",
      "130770          mixed_low_Scenario_063  Greedy Reactive    71      19799.3   \n",
      "...                                ...              ...   ...          ...   \n",
      "222263  stochastic_medium_Scenario_085     DQN Reactive    64       9612.9   \n",
      "222262  stochastic_medium_Scenario_085     DQN Reactive    63       -217.0   \n",
      "222261  stochastic_medium_Scenario_085     DQN Reactive    62       -217.0   \n",
      "222268  stochastic_medium_Scenario_085     DQN Reactive    69       9612.9   \n",
      "35000      stochastic_low_Scenario_091     DQN Reactive     1        -90.8   \n",
      "\n",
      "        TotalDelays  TotalCancelledFlights  ScenarioTime  ScenarioSteps  \\\n",
      "139999          0.0                      0      0.005096              8   \n",
      "130773          0.0                      1      0.105604              6   \n",
      "130772          0.0                      1      0.103904              6   \n",
      "130771          0.0                      2      0.116886              6   \n",
      "130770          0.0                      2      0.109301              6   \n",
      "...             ...                    ...           ...            ...   \n",
      "222263        300.0                      1      0.029779              8   \n",
      "222262          0.0                      0      0.031224              8   \n",
      "222261          0.0                      0      0.030857              8   \n",
      "222268        300.0                      1      0.032664              8   \n",
      "35000           0.0                      0      0.027116              5   \n",
      "\n",
      "        ScenarioResolvedConflicts  SolutionSlack  ...  Reward_time_penalty  \\\n",
      "139999                          0       0.000000  ...               -216.0   \n",
      "130773                          1       0.642236  ...               -126.0   \n",
      "130772                          1       0.642236  ...               -126.0   \n",
      "130771                          1       0.581781  ...               -126.0   \n",
      "130770                          1       0.581781  ...               -126.0   \n",
      "...                           ...            ...  ...                  ...   \n",
      "222263                          1       0.458873  ...               -216.0   \n",
      "222262                          0       0.000000  ...               -216.0   \n",
      "222261                          0       0.000000  ...               -216.0   \n",
      "222268                          1       0.458873  ...               -216.0   \n",
      "35000                           0       0.000000  ...                -90.0   \n",
      "\n",
      "        Reward_final_conflict_resolution_reward  ScenarioSlack  TotalFlights  \\\n",
      "139999                                        0       0.366092            16   \n",
      "130773                                    30000       0.346429            15   \n",
      "130772                                    30000       0.346429            15   \n",
      "130771                                    30000       0.346429            15   \n",
      "130770                                    30000       0.346429            15   \n",
      "...                                         ...            ...           ...   \n",
      "222263                                    30000       0.386979            18   \n",
      "222262                                        0       0.386979            18   \n",
      "222261                                        0       0.386979            18   \n",
      "222268                                    30000       0.386979            18   \n",
      "35000                                         0       0.464530            14   \n",
      "\n",
      "        TotalFlightMinutes  FullyDisruptedCount  UncertainCount  \\\n",
      "139999                3309                    0               2   \n",
      "130773                3294                    1               1   \n",
      "130772                3294                    1               1   \n",
      "130771                3294                    1               1   \n",
      "130770                3294                    1               1   \n",
      "...                    ...                  ...             ...   \n",
      "222263                3531                    0               2   \n",
      "222262                3531                    0               2   \n",
      "222261                3531                    0               2   \n",
      "222268                3531                    0               2   \n",
      "35000                 2506                    0               2   \n",
      "\n",
      "        AvgAircraftProbability  AvgUncertaintyProbability  TotalAircraft  \n",
      "139999                0.106346                   0.319038              6  \n",
      "130773                0.237689                   0.426137              6  \n",
      "130772                0.237689                   0.426137              6  \n",
      "130771                0.237689                   0.426137              6  \n",
      "130770                0.237689                   0.426137              6  \n",
      "...                        ...                        ...            ...  \n",
      "222263                0.222157                   0.666470              6  \n",
      "222262                0.222157                   0.666470              6  \n",
      "222261                0.222157                   0.666470              6  \n",
      "222268                0.222157                   0.666470              6  \n",
      "35000                 0.080172                   0.240516              6  \n",
      "\n",
      "[280000 rows x 26 columns]\n",
      "Inference results with scenario info saved to ../logs/scenarios/scenario_inference_metrics_4.csv\n",
      "==== Columns: ====\n",
      "Index(['Scenario', 'Model', 'Seed', 'TotalReward', 'TotalDelays',\n",
      "       'TotalCancelledFlights', 'ScenarioTime', 'ScenarioSteps',\n",
      "       'ScenarioResolvedConflicts', 'SolutionSlack', 'TailSwaps',\n",
      "       'ActualDisruptedFlights', 'Reward_delay_penalty_total',\n",
      "       'Reward_cancel_penalty', 'Reward_inaction_penalty',\n",
      "       'Reward_proactive_bonus', 'Reward_time_penalty',\n",
      "       'Reward_final_conflict_resolution_reward', 'ScenarioSlack',\n",
      "       'TotalFlights', 'TotalFlightMinutes', 'FullyDisruptedCount',\n",
      "       'UncertainCount', 'AvgAircraftProbability', 'AvgUncertaintyProbability',\n",
      "       'TotalAircraft'],\n",
      "      dtype='object')\n",
      "==== amount of rows: ====\n",
      "280000\n",
      "==== Models: ====\n",
      "['Greedy Reactive' 'DQN Proactive-N' 'DQN Proactive-U' 'DQN Reactive']\n",
      "===== len(seeds) =====\n",
      "100\n",
      "===== len(scenarios) =====\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "scenario_folder_path = \"../logs/scenarios/\"\n",
    "# unpack results_df\n",
    "results_df = pd.read_csv(os.path.join(scenario_folder_path, f\"t_results_df_in_rapport_seeds_{len(seeds)}.csv\"))\n",
    "\n",
    "\n",
    "# Merge scenario-level info from scenarios_df into results_df\n",
    "merged_df = results_df.merge(scenarios_df, on='Scenario', how='left')\n",
    "\n",
    "# Sort models in desired order\n",
    "model_order = ['proactive', 'myopic', 'reactive', 'greedy_reactive']\n",
    "merged_df['Model_Type'] = merged_df['Model'].str.extract('(' + '|'.join(model_order) + ')')\n",
    "merged_df = merged_df.sort_values('Model_Type')\n",
    "merged_df[\"Model\"] = merged_df[\"Model_Type\"]\n",
    "merged_df = merged_df.drop('Model_Type', axis=1)\n",
    "merged_df_backup = merged_df.copy()\n",
    "\n",
    "\n",
    "# Update model names in merged_df\n",
    "merged_df['Model'] = merged_df['Model'].apply(lambda x: \n",
    "    'DQN Proactive-U' if x.startswith('proactive') else\n",
    "    'DQN Proactive-N' if x.startswith('myopic') else \n",
    "    'DQN Reactive' if x.startswith('reactive') else\n",
    "    'Greedy Reactive' if x.startswith('greedy_reactive') else\n",
    "    x\n",
    ")\n",
    "\n",
    "print(\"Inference Results (After Merging):\")\n",
    "print(merged_df)\n",
    "\n",
    "# Save the merged results to CSV\n",
    "merged_output_file = os.path.join(scenario_folder_path, f\"scenario_inference_metrics_{scenario_id}.csv\")\n",
    "merged_df.to_csv(merged_output_file, index=False)\n",
    "print(f\"Inference results with scenario info saved to {merged_output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# print all column names\n",
    "print(\"==== Columns: ====\")\n",
    "print(merged_df.columns)\n",
    "\n",
    "print(\"==== amount of rows: ====\")\n",
    "print(len(merged_df))\n",
    "\n",
    "print(\"==== Models: ====\")\n",
    "print(merged_df[\"Model\"].unique())\n",
    "\n",
    "print('===== len(seeds) =====')\n",
    "print(len(merged_df['Seed'].unique()))\n",
    "\n",
    "print('===== len(scenarios) =====')\n",
    "print(len(merged_df['Scenario'].unique()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1: Solution method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Reactive mean reward: 32888.746 (std=28506.482)\n",
      "DQN Reactive    mean reward: -4062.909 (std=23909.696)\n",
      "\n",
      "Shapiro–Wilk test on (DQN - Greedy):\n",
      "  statistic=0.9643, p=0.0000\n",
      "  => Non-normal difference (p < 0.05). We will use Wilcoxon because the data are not normally distributed.\n",
      "\n",
      "Wilcoxon test (DQN vs. Greedy) on TotalReward:\n",
      "  w-statistic=78450579.0000, p-value=0.000000\n",
      "\n",
      "Difference (DQN - Greedy): mean=-36951.655\n",
      "95% CI: [-37219.218, -36684.091]\n",
      "Cohen's d (paired) = -1.023\n",
      "\n",
      "p-value=0.000000 < 0.05, so we found a SIGNIFICANT DIFFERENCE.\n",
      "  => DQN is LOWER than Greedy. We reject H0 (no difference), but DQN < Greedy,\n",
      "     which CONTRADICTS the alt hypothesis that DQN is better.\n",
      "     So the difference is significant, but in the OPPOSITE direction of H_alt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv2/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 70000.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, shapiro, wilcoxon\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) FILTER, PIVOT\n",
    "# -------------------------------------------------\n",
    "df_two_methods = merged_df[merged_df['Model'].isin(['Greedy Reactive', 'DQN Reactive'])].copy()\n",
    "df_pivot = df_two_methods.pivot_table(\n",
    "    index=['Scenario', 'Seed'],\n",
    "    columns='Model',\n",
    "    values='TotalReward'\n",
    ").dropna()\n",
    "\n",
    "df_pivot.rename(\n",
    "    columns={'Greedy Reactive': 'Greedy', 'DQN Reactive': 'DQN'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) BASIC STATS\n",
    "# -------------------------------------------------\n",
    "mean_greedy = df_pivot['Greedy'].mean()\n",
    "std_greedy = df_pivot['Greedy'].std()\n",
    "mean_dqn    = df_pivot['DQN'].mean()\n",
    "std_dqn     = df_pivot['DQN'].std()\n",
    "\n",
    "print(f\"Greedy Reactive mean reward: {mean_greedy:.3f} (std={std_greedy:.3f})\")\n",
    "print(f\"DQN Reactive    mean reward: {mean_dqn:.3f} (std={std_dqn:.3f})\\n\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) NORMALITY CHECK (SHAPIRO–WILK)\n",
    "# -------------------------------------------------\n",
    "diff = df_pivot['DQN'] - df_pivot['Greedy']\n",
    "shapiro_stat, shapiro_p = shapiro(diff)\n",
    "print(\"Shapiro–Wilk test on (DQN - Greedy):\")\n",
    "print(f\"  statistic={shapiro_stat:.4f}, p={shapiro_p:.4f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) CHOOSE & RUN THE APPROPRIATE TEST\n",
    "# -------------------------------------------------\n",
    "if shapiro_p < ALPHA:\n",
    "    print(\"  => Non-normal difference (p < 0.05). We will use Wilcoxon because the data are not normally distributed.\\n\")\n",
    "\n",
    "    # Wilcoxon signed-rank test\n",
    "    w_stat, w_pval = wilcoxon(df_pivot['DQN'], df_pivot['Greedy'])\n",
    "    print(\"Wilcoxon test (DQN vs. Greedy) on TotalReward:\")\n",
    "    print(f\"  w-statistic={w_stat:.4f}, p-value={w_pval:.6f}\")\n",
    "\n",
    "    # Decision logic based on Wilcoxon\n",
    "    p_val = w_pval  # We’ll use `p_val` for consistency with the code below.\n",
    "\n",
    "else:\n",
    "    print(\"  => Difference does not deviate strongly from normality (p >= 0.05). We will use the paired t-test.\\n\")\n",
    "\n",
    "    # Paired t-test\n",
    "    t_stat, t_pval = ttest_rel(df_pivot['DQN'], df_pivot['Greedy'])\n",
    "    print(\"Paired t-test (DQN vs. Greedy) on TotalReward:\")\n",
    "    print(f\"  t-statistic={t_stat:.4f}, p-value={t_pval:.6f}\")\n",
    "\n",
    "    # Decision logic based on t-test\n",
    "    p_val = t_pval\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) 95% CI OF DIFFERENCE\n",
    "# -------------------------------------------------\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std(ddof=1)\n",
    "n         = len(diff)\n",
    "se_diff   = std_diff / np.sqrt(n)\n",
    "ci_lower  = mean_diff - 1.96 * se_diff\n",
    "ci_upper  = mean_diff + 1.96 * se_diff\n",
    "\n",
    "print(f\"\\nDifference (DQN - Greedy): mean={mean_diff:.3f}\")\n",
    "print(f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) EFFECT SIZE (Cohen's d)\n",
    "# -------------------------------------------------\n",
    "cohens_d = mean_diff / std_diff\n",
    "print(f\"Cohen's d (paired) = {cohens_d:.3f}\\n\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) DECISION LOGIC\n",
    "# -------------------------------------------------\n",
    "# H0: DQN == Greedy\n",
    "# H_alt: DQN > Greedy  (i.e., DQN is significantly better)\n",
    "\n",
    "if p_val < ALPHA:\n",
    "    print(f\"p-value={p_val:.6f} < {ALPHA}, so we found a SIGNIFICANT DIFFERENCE.\")\n",
    "    if mean_diff > 0:\n",
    "        print(\"  => DQN is HIGHER than Greedy. We reject H0 and ACCEPT the alt hypothesis\")\n",
    "        print(\"     that DQN is significantly better.\")\n",
    "    else:\n",
    "        print(\"  => DQN is LOWER than Greedy. We reject H0 (no difference), but DQN < Greedy,\")\n",
    "        print(\"     which CONTRADICTS the alt hypothesis that DQN is better.\")\n",
    "        print(\"     So the difference is significant, but in the OPPOSITE direction of H_alt.\")\n",
    "else:\n",
    "    print(f\"p-value={p_val:.6f} >= {ALPHA}, so we fail to reject H0.\")\n",
    "    print(\"  => We do NOT have significant evidence of a difference in reward.\")\n",
    "    print(\"     (Cannot claim DQN is better based on these data.)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2: Action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Proactive-N mean reward: 6198.375 (std=29111.820)\n",
      "DQN Reactive    mean reward: -4062.909 (std=23909.696)\n",
      "\n",
      "Shapiro–Wilk test on (ProactiveN - Reactive):\n",
      "  statistic=0.9917, p=0.0000\n",
      "  => Non-normal difference (p < 0.05). We will use the Wilcoxon test because the distribution is not normal.\n",
      "\n",
      "Wilcoxon (ProactiveN vs. Reactive) on TotalReward:\n",
      "  statistic=825400324.5000, p-value=0.000000\n",
      "\n",
      "Difference (ProactiveN - Reactive): mean=10261.284\n",
      "95% CI: [9995.512, 10527.055]\n",
      "Cohen's d (paired) = 0.286\n",
      "\n",
      "p-value=0.000000 < 0.05, so we found a SIGNIFICANT DIFFERENCE.\n",
      "  => ProactiveN > Reactive. Reject H0 and ACCEPT the alt hypothesis.\n",
      "     DQN Proactive-N has a higher mean reward than DQN Reactive.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv2/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 70000.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, shapiro, wilcoxon\n",
    "\n",
    "# Significance level\n",
    "ALPHA = 0.05\n",
    "\n",
    "# 1) FILTER AND PIVOT\n",
    "df_two_methods = merged_df[merged_df['Model'].isin(['DQN Proactive-N', 'DQN Reactive'])].copy()\n",
    "\n",
    "# Pivot so each (Scenario, Seed) becomes a single row with columns [ProactiveN, Reactive]\n",
    "df_pivot = df_two_methods.pivot_table(\n",
    "    index=['Scenario', 'Seed'],\n",
    "    columns='Model',\n",
    "    values='TotalReward'\n",
    ").dropna()\n",
    "\n",
    "# For convenience, rename columns\n",
    "df_pivot.rename(\n",
    "    columns={'DQN Proactive-N': 'ProactiveN', 'DQN Reactive': 'Reactive'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# 2) BASIC STATS\n",
    "mean_proactive = df_pivot['ProactiveN'].mean()\n",
    "std_proactive  = df_pivot['ProactiveN'].std()\n",
    "mean_reactive  = df_pivot['Reactive'].mean()\n",
    "std_reactive   = df_pivot['Reactive'].std()\n",
    "\n",
    "print(f\"DQN Proactive-N mean reward: {mean_proactive:.3f} (std={std_proactive:.3f})\")\n",
    "print(f\"DQN Reactive    mean reward: {mean_reactive:.3f} (std={std_reactive:.3f})\\n\")\n",
    "\n",
    "# 3) CHECK NORMALITY\n",
    "diff = df_pivot['ProactiveN'] - df_pivot['Reactive']\n",
    "shapiro_stat, shapiro_p = shapiro(diff)\n",
    "print(\"Shapiro–Wilk test on (ProactiveN - Reactive):\")\n",
    "print(f\"  statistic={shapiro_stat:.4f}, p={shapiro_p:.4f}\")\n",
    "\n",
    "# Decide which test to use (t-test vs Wilcoxon) based on normality\n",
    "if shapiro_p < ALPHA:\n",
    "    print(\"  => Non-normal difference (p < 0.05). We will use the Wilcoxon test because the distribution is not normal.\\n\")\n",
    "    test_name = \"Wilcoxon\"\n",
    "    test_stat, p_val = wilcoxon(df_pivot['ProactiveN'], df_pivot['Reactive'])\n",
    "else:\n",
    "    print(\"  => Difference does not deviate strongly from normality (p >= 0.05).\\n\")\n",
    "    test_name = \"Paired t-test\"\n",
    "    test_stat, p_val = ttest_rel(df_pivot['ProactiveN'], df_pivot['Reactive'])\n",
    "\n",
    "# 4) REPORT TEST RESULTS\n",
    "print(f\"{test_name} (ProactiveN vs. Reactive) on TotalReward:\")\n",
    "print(f\"  statistic={test_stat:.4f}, p-value={p_val:.6f}\")\n",
    "\n",
    "# 5) 95% CI OF THE DIFFERENCE\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std(ddof=1)\n",
    "n         = len(diff)\n",
    "se_diff   = std_diff / np.sqrt(n)\n",
    "ci_lower  = mean_diff - 1.96 * se_diff\n",
    "ci_upper  = mean_diff + 1.96 * se_diff\n",
    "\n",
    "print(f\"\\nDifference (ProactiveN - Reactive): mean={mean_diff:.3f}\")\n",
    "print(f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "# 6) EFFECT SIZE (Cohen's d for paired data)\n",
    "cohens_d = mean_diff / std_diff\n",
    "print(f\"Cohen's d (paired) = {cohens_d:.3f}\\n\")\n",
    "\n",
    "# 7) DECISION LOGIC\n",
    "#   H0: No difference between ProactiveN and Reactive\n",
    "#   H_alt: ProactiveN is significantly better (i.e., higher reward)\n",
    "if p_val < ALPHA:\n",
    "    print(f\"p-value={p_val:.6f} < {ALPHA}, so we found a SIGNIFICANT DIFFERENCE.\")\n",
    "    if mean_diff > 0:\n",
    "        print(\"  => ProactiveN > Reactive. Reject H0 and ACCEPT the alt hypothesis.\")\n",
    "        print(\"     DQN Proactive-N has a higher mean reward than DQN Reactive.\\n\")\n",
    "    else:\n",
    "        print(\"  => ProactiveN < Reactive. Reject H0, but ProactiveN is actually WORSE.\")\n",
    "        print(\"     This is significant but in the OPPOSITE direction of the alt hypothesis.\\n\")\n",
    "else:\n",
    "    print(f\"p-value={p_val:.6f} >= {ALPHA}, so we FAIL to reject H0.\")\n",
    "    print(\"  => No statistically significant difference in the mean reward.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 3: Uncertainty indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Proactive-U mean reward: 10186.831 (std=30907.585)\n",
      "DQN Proactive-N mean reward: 6198.375 (std=29111.820)\n",
      "\n",
      "Shapiro–Wilk test on (ProactiveU - ProactiveN):\n",
      "  statistic=0.9924, p=0.0000\n",
      "  => Non-normal difference (p < 0.05). We will use the Wilcoxon test.\n",
      "\n",
      "Wilcoxon (ProactiveU vs. ProactiveN) on TotalReward:\n",
      "  statistic=1116893103.0000, p-value=0.000000\n",
      "\n",
      "Difference (U - N): mean=3988.457\n",
      "95% CI: [3701.555, 4275.359]\n",
      "Cohen's d (paired) = 0.103\n",
      "\n",
      "p-value=0.000000 < 0.05, so we found a SIGNIFICANT DIFFERENCE.\n",
      "  => ProactiveU > ProactiveN. Reject H0 and ACCEPT H_alt.\n",
      "     DQN Proactive-U has a higher mean reward than DQN Proactive-N.\n",
      "\n",
      "Linear regression of (Proactive-U minus Proactive-N) vs. AvgUncertaintyProbability:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     36.40\n",
      "Date:                Thu, 02 Jan 2025   Prob (F-statistic):           1.62e-09\n",
      "Time:                        14:21:18   Log-Likelihood:            -8.3881e+05\n",
      "No. Observations:               70000   AIC:                         1.678e+06\n",
      "Df Residuals:                   69998   BIC:                         1.678e+06\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                      2464.1190    291.989      8.439      0.000    1891.821    3036.417\n",
      "AvgUncertaintyProbability  3524.0899    584.142      6.033      0.000    2379.173    4669.007\n",
      "==============================================================================\n",
      "Omnibus:                     1591.344   Durbin-Watson:                   0.529\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2143.278\n",
      "Skew:                           0.278   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.653   Cond. No.                         4.78\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv2/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 70000.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, shapiro, wilcoxon\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) FILTER THE TWO METHODS & PIVOT ON REWARD\n",
    "# ---------------------------------------------------------\n",
    "df_two_methods = merged_df[merged_df['Model'].isin(['DQN Proactive-U', 'DQN Proactive-N'])].copy()\n",
    "\n",
    "# Pivot so each (Scenario, Seed) is a row, columns = [ProactiveU, ProactiveN]\n",
    "df_pivot_reward = df_two_methods.pivot_table(\n",
    "    index=['Scenario', 'Seed'],\n",
    "    columns='Model',\n",
    "    values='TotalReward'\n",
    ").dropna()\n",
    "\n",
    "# Rename columns for convenience\n",
    "df_pivot_reward.rename(\n",
    "    columns={'DQN Proactive-U': 'ProactiveU', 'DQN Proactive-N': 'ProactiveN'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Get the average probability (if you want to analyze that relationship)\n",
    "df_prob = df_two_methods[['Scenario', 'Seed', 'AvgUncertaintyProbability']].drop_duplicates()\n",
    "df_final = df_pivot_reward.merge(df_prob, on=['Scenario', 'Seed'], how='left')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) DESCRIPTIVE STATS\n",
    "# ---------------------------------------------------------\n",
    "mean_u = df_final['ProactiveU'].mean()\n",
    "std_u  = df_final['ProactiveU'].std()\n",
    "mean_n = df_final['ProactiveN'].mean()\n",
    "std_n  = df_final['ProactiveN'].std()\n",
    "\n",
    "print(f\"DQN Proactive-U mean reward: {mean_u:.3f} (std={std_u:.3f})\")\n",
    "print(f\"DQN Proactive-N mean reward: {mean_n:.3f} (std={std_n:.3f})\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) NORMALITY CHECK\n",
    "# ---------------------------------------------------------\n",
    "diff = df_final['ProactiveU'] - df_final['ProactiveN']\n",
    "shapiro_stat, shapiro_p = shapiro(diff)\n",
    "print(\"Shapiro–Wilk test on (ProactiveU - ProactiveN):\")\n",
    "print(f\"  statistic={shapiro_stat:.4f}, p={shapiro_p:.4f}\")\n",
    "\n",
    "# Decide which test to use (t-test vs Wilcoxon) based on normality\n",
    "if shapiro_p < ALPHA:\n",
    "    print(\"  => Non-normal difference (p < 0.05). We will use the Wilcoxon test.\\n\")\n",
    "    test_name = \"Wilcoxon\"\n",
    "    test_stat, p_val = wilcoxon(df_final['ProactiveU'], df_final['ProactiveN'])\n",
    "else:\n",
    "    print(\"  => Difference does not deviate strongly from normality (p >= 0.05).\\n\")\n",
    "    test_name = \"Paired t-test\"\n",
    "    test_stat, p_val = ttest_rel(df_final['ProactiveU'], df_final['ProactiveN'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) REPORT TEST RESULTS\n",
    "# ---------------------------------------------------------\n",
    "print(f\"{test_name} (ProactiveU vs. ProactiveN) on TotalReward:\")\n",
    "print(f\"  statistic={test_stat:.4f}, p-value={p_val:.6f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) 95% CI OF THE DIFFERENCE\n",
    "# ---------------------------------------------------------\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std(ddof=1)\n",
    "n         = len(diff)\n",
    "se_diff   = std_diff / np.sqrt(n)\n",
    "ci_lower  = mean_diff - 1.96 * se_diff\n",
    "ci_upper  = mean_diff + 1.96 * se_diff\n",
    "\n",
    "print(f\"\\nDifference (U - N): mean={mean_diff:.3f}\")\n",
    "print(f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) EFFECT SIZE (Cohen's d)\n",
    "# ---------------------------------------------------------\n",
    "cohens_d = mean_diff / std_diff\n",
    "print(f\"Cohen's d (paired) = {cohens_d:.3f}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) DECISION LOGIC\n",
    "# ---------------------------------------------------------\n",
    "# H0: No difference between Proactive-U and Proactive-N\n",
    "# H_alt: Proactive-U is significantly better (i.e., higher reward)\n",
    "if p_val < ALPHA:\n",
    "    print(f\"p-value={p_val:.6f} < {ALPHA}, so we found a SIGNIFICANT DIFFERENCE.\")\n",
    "    if mean_diff > 0:\n",
    "        print(\"  => ProactiveU > ProactiveN. Reject H0 and ACCEPT H_alt.\")\n",
    "        print(\"     DQN Proactive-U has a higher mean reward than DQN Proactive-N.\\n\")\n",
    "    else:\n",
    "        print(\"  => ProactiveU < ProactiveN. Reject H0, but it's in the OPPOSITE direction.\")\n",
    "        print(\"     Proactive-U is actually WORSE than Proactive-N.\\n\")\n",
    "else:\n",
    "    print(f\"p-value={p_val:.6f} >= {ALPHA}, so we FAIL to reject H0.\")\n",
    "    print(\"  => No statistically significant difference in the mean reward.\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) OPTIONAL: REGRESSION OF (U - N) vs. Probability\n",
    "# ---------------------------------------------------------\n",
    "X = sm.add_constant(df_final['AvgUncertaintyProbability'])  # add intercept\n",
    "y = diff  # (ProactiveU - ProactiveN)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(\"Linear regression of (Proactive-U minus Proactive-N) vs. AvgUncertaintyProbability:\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 4: Disruption scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Two-Way ANOVA (Type II) ===\n",
      "                                    sum_sq        df             F  PR(>F)\n",
      "C(Method)                     5.116805e+13       3.0  26756.602458     0.0\n",
      "C(DisruptionScale)            3.082899e+13       6.0   8060.489489     0.0\n",
      "C(Method):C(DisruptionScale)  1.379437e+13      18.0   1202.216916     0.0\n",
      "Residual                      1.784683e+14  279972.0           NaN     NaN \n",
      "\n",
      "=== ANOVA Interpretation ===\n",
      "* p-value(Method)=0.000000 < 0.05.\n",
      "  => There's a significant main effect of Method (averaging across disruption scales).\n",
      "* p-value(DisruptionScale)=0.000000 < 0.05.\n",
      "  => There's a significant main effect of Disruption Scale (averaging across methods).\n",
      "* p-value(Interaction)=0.000000 < 0.05.\n",
      "  => There's a significant interaction between Method and Disruption Scale.\n",
      "     This suggests the effect of Method depends on disruption scale (and vice versa).\n",
      "\n",
      "** Hypothesis 4 Decision **\n",
      "We found a significant interaction, so the difference among methods changes\n",
      "as disruption scale grows => This likely REJECTS H0 (that there's no difference)\n",
      "and supports investigating which methods differ at higher scale.\n",
      "But to confirm that 'proactive DRL approaches perform better' at high scale,\n",
      "we typically do post-hoc tests or slice the data by scale.\n",
      "\n",
      "=== Post-hoc Tukey HSD for Method main effect ===\n",
      "              Multiple Comparison of Means - Tukey HSD, FWER=0.05               \n",
      "================================================================================\n",
      "     group1          group2       meandiff  p-adj    lower       upper    reject\n",
      "--------------------------------------------------------------------------------\n",
      "DQN Proactive-N DQN Proactive-U   3988.4567   0.0   3600.8411   4376.0723   True\n",
      "DQN Proactive-N    DQN Reactive -10261.2837   0.0 -10648.8993   -9873.668   True\n",
      "DQN Proactive-N Greedy Reactive  26690.3709   0.0  26302.7553  27077.9865   True\n",
      "DQN Proactive-U    DQN Reactive -14249.7404   0.0  -14637.356 -13862.1248   True\n",
      "DQN Proactive-U Greedy Reactive  22701.9141   0.0  22314.2985  23089.5297   True\n",
      "   DQN Reactive Greedy Reactive  36951.6545   0.0  36564.0389  37339.2701   True\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Hypothesis 4:\n",
    "#   H0: As disruption scale grows, no difference among the four approaches\n",
    "#   H_alt: As disruption scale grows, proactive DRL approaches perform better\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) PREPARE THE DATAFRAME\n",
    "# ------------------------------------------------\n",
    "# We'll assume merged_df has columns like:\n",
    "#   \"Model\" -> one of ['Greedy Reactive', 'DQN Reactive', 'DQN Proactive-N', 'DQN Proactive-U']\n",
    "#   \"TotalReward\" -> numeric outcome measure\n",
    "#   \"ActualDisruptedFlights\" (or some relevant column) -> integer count of disrupted flights\n",
    "\n",
    "# Make sure we only keep the 4 relevant models if your DataFrame might have more\n",
    "df_four_methods = merged_df[merged_df['Model'].isin([\n",
    "    'Greedy Reactive', \n",
    "    'DQN Reactive', \n",
    "    'DQN Proactive-N', \n",
    "    'DQN Proactive-U'\n",
    "])].copy()\n",
    "\n",
    "# For the two-way ANOVA, we need a categorical factor for 'Model' \n",
    "# and a factor for 'DisruptedFlights' (which could be treated as categorical \n",
    "# or numeric, depending on your experimental design).\n",
    "# If 'ActualDisruptedFlights' is numeric (0,1,2,3,...), we can:\n",
    "# - treat it as a categorical factor by converting to string or 'category'\n",
    "df_four_methods['Method'] = df_four_methods['Model'].astype('category')\n",
    "df_four_methods['DisruptionScale'] = df_four_methods['ActualDisruptedFlights'].astype('category')\n",
    "\n",
    "# We'll rename \"TotalReward\" to a shorter name for the formula\n",
    "df_four_methods['Reward'] = df_four_methods['TotalReward']\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) FIT A TWO-WAY ANOVA MODEL WITH INTERACTION\n",
    "# ------------------------------------------------\n",
    "# OLS formula: \"Reward ~ C(Method) * C(DisruptionScale)\"\n",
    "# This includes main effects for Method, DisruptionScale, and their interaction\n",
    "model = ols(\"Reward ~ C(Method) * C(DisruptionScale)\", data=df_four_methods).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)  # Type 2 sums of squares\n",
    "print(\"=== Two-Way ANOVA (Type II) ===\")\n",
    "print(anova_table, \"\\n\")\n",
    "\n",
    "# We'll extract p-values from the ANOVA table\n",
    "p_method = anova_table.loc['C(Method)', 'PR(>F)']\n",
    "p_scale  = anova_table.loc['C(DisruptionScale)', 'PR(>F)']\n",
    "p_inter  = anova_table.loc['C(Method):C(DisruptionScale)', 'PR(>F)']\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) INTERPRET ANOVA RESULTS WITH IF-ELSE\n",
    "# ------------------------------------------------\n",
    "# Because the hypothesis is specifically about \"no difference among methods as scale grows\" vs. \n",
    "# \"proactive methods do better as scale grows,\" we pay attention to the interaction mostly. \n",
    "# However, for completeness, let's interpret main effects too.\n",
    "print(\"=== ANOVA Interpretation ===\")\n",
    "\n",
    "# Main effect of Method\n",
    "if p_method < ALPHA:\n",
    "    print(f\"* p-value(Method)={p_method:.6f} < {ALPHA}.\")\n",
    "    print(\"  => There's a significant main effect of Method (averaging across disruption scales).\")\n",
    "else:\n",
    "    print(f\"* p-value(Method)={p_method:.6f} >= {ALPHA}.\")\n",
    "    print(\"  => No significant main effect of Method overall.\")\n",
    "\n",
    "# Main effect of DisruptionScale\n",
    "if p_scale < ALPHA:\n",
    "    print(f\"* p-value(DisruptionScale)={p_scale:.6f} < {ALPHA}.\")\n",
    "    print(\"  => There's a significant main effect of Disruption Scale (averaging across methods).\")\n",
    "else:\n",
    "    print(f\"* p-value(DisruptionScale)={p_scale:.6f} >= {ALPHA}.\")\n",
    "    print(\"  => No significant main effect of Disruption Scale overall.\")\n",
    "\n",
    "# Interaction effect\n",
    "if p_inter < ALPHA:\n",
    "    print(f\"* p-value(Interaction)={p_inter:.6f} < {ALPHA}.\")\n",
    "    print(\"  => There's a significant interaction between Method and Disruption Scale.\")\n",
    "    print(\"     This suggests the effect of Method depends on disruption scale (and vice versa).\")\n",
    "\n",
    "    # Now, let's map that to the hypothesis:\n",
    "    print(\"\\n** Hypothesis 4 Decision **\")\n",
    "    print(\"We found a significant interaction, so the difference among methods changes\")\n",
    "    print(\"as disruption scale grows => This likely REJECTS H0 (that there's no difference)\")\n",
    "    print(\"and supports investigating which methods differ at higher scale.\")\n",
    "    print(\"But to confirm that 'proactive DRL approaches perform better' at high scale,\")\n",
    "    print(\"we typically do post-hoc tests or slice the data by scale.\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"* p-value(Interaction)={p_inter:.6f} >= {ALPHA}.\")\n",
    "    print(\"  => No significant interaction. The difference among methods does not\")\n",
    "    print(\"     significantly change at different disruption scales under this model.\\n\")\n",
    "    print(\"** Hypothesis 4 Decision **\")\n",
    "    print(\"We FAIL to reject H0 for the interaction effect. There's no evidence that\")\n",
    "    print(\"the methods diverge more (or less) as disruption scale changes.\\n\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) POST-HOC TEST (Tukey HSD)\n",
    "# ------------------------------------------------\n",
    "# If you want to see specifically which pairs of (Method, Scale) differ, you can do:\n",
    "#  - pairwise Tukey on e.g. \"Method\" or on \"Method:Scale\" combinations\n",
    "# For demonstration, let's do it for Method only, ignoring scale. \n",
    "# (In practice, you might want a more advanced approach that accounts for scale or does separate tests by scale.)\n",
    "tukey_input = df_four_methods[['Reward', 'Method']].dropna()\n",
    "tukey = pairwise_tukeyhsd(endog=tukey_input['Reward'],\n",
    "                          groups=tukey_input['Method'],\n",
    "                          alpha=ALPHA)\n",
    "print(\"=== Post-hoc Tukey HSD for Method main effect ===\")\n",
    "print(tukey.summary(), \"\\n\")\n",
    "\n",
    "# If you specifically want pairs for each scale, you could create a combined factor:\n",
    "# df_four_methods['Method_Scale'] = df_four_methods['Method'].astype(str) + \"_\" + df_four_methods['DisruptionScale'].astype(str)\n",
    "# ... then do pairwise_tukeyhsd with that combined factor to see all pairwise comparisons.\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) ADDITIONAL NOTES\n",
    "# ------------------------------------------------\n",
    "# - For repeated-measures design, you'd typically need each scenario tested at each level \n",
    "#   of Method and DisruptionScale, so you'd consider a different model approach \n",
    "#   (e.g., \"mixed-effects\" with random factor = scenario).\n",
    "# - If normality or homoskedasticity assumptions fail, consider a Friedman test \n",
    "#   (non-parametric repeated-measures) or a more robust method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 5: Schedule robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=Greedy Reactive, Metric=TotalCancelledFlights\n",
      "  Slope=-1.6474, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalCancelledFlights' for Greedy Reactive.\n",
      "\n",
      "Model=Greedy Reactive, Metric=TotalDelays\n",
      "  Slope=25.3842, p-value=0.000615\n",
      "  => Slack does NOT show a significant negative effect on 'TotalDelays' for Greedy Reactive.\n",
      "\n",
      "Model=Greedy Reactive, Metric=TailSwaps\n",
      "  Slope=-0.2872, p-value=0.001123\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TailSwaps' for Greedy Reactive.\n",
      "\n",
      "Model=DQN Reactive, Metric=TotalCancelledFlights\n",
      "  Slope=-1.6316, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalCancelledFlights' for DQN Reactive.\n",
      "\n",
      "Model=DQN Reactive, Metric=TotalDelays\n",
      "  Slope=-111.9766, p-value=0.000140\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalDelays' for DQN Reactive.\n",
      "\n",
      "Model=DQN Reactive, Metric=TailSwaps\n",
      "  Slope=-2.5112, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TailSwaps' for DQN Reactive.\n",
      "\n",
      "Model=DQN Proactive-N, Metric=TotalCancelledFlights\n",
      "  Slope=-2.7962, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalCancelledFlights' for DQN Proactive-N.\n",
      "\n",
      "Model=DQN Proactive-N, Metric=TotalDelays\n",
      "  Slope=-304.8501, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalDelays' for DQN Proactive-N.\n",
      "\n",
      "Model=DQN Proactive-N, Metric=TailSwaps\n",
      "  Slope=1.3856, p-value=0.000000\n",
      "  => Slack does NOT show a significant negative effect on 'TailSwaps' for DQN Proactive-N.\n",
      "\n",
      "Model=DQN Proactive-U, Metric=TotalCancelledFlights\n",
      "  Slope=-1.8856, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalCancelledFlights' for DQN Proactive-U.\n",
      "\n",
      "Model=DQN Proactive-U, Metric=TotalDelays\n",
      "  Slope=-136.7149, p-value=0.000003\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TotalDelays' for DQN Proactive-U.\n",
      "\n",
      "Model=DQN Proactive-U, Metric=TailSwaps\n",
      "  Slope=-1.8661, p-value=0.000000\n",
      "  => Slack has a SIGNIFICANT NEGATIVE effect on 'TailSwaps' for DQN Proactive-U.\n",
      "\n",
      "=== Final Decision on Hypothesis 5 ===\n",
      "At least one model or metric does NOT show a significant negative slope.\n",
      "=> We FAIL to confirm that ALL approaches improve on ALL metrics.\n",
      "Hence, we do NOT accept the alternative hypothesis (H_alt).\n",
      "\n",
      "=== Detailed Summary ===\n",
      "Greedy Reactive    | TotalCancelledFlights     | slope=  -1.647 | p=0.00000 | significant_neg=True\n",
      "Greedy Reactive    | TotalDelays               | slope=  25.384 | p=0.00062 | significant_neg=False\n",
      "Greedy Reactive    | TailSwaps                 | slope=  -0.287 | p=0.00112 | significant_neg=True\n",
      "DQN Reactive       | TotalCancelledFlights     | slope=  -1.632 | p=0.00000 | significant_neg=True\n",
      "DQN Reactive       | TotalDelays               | slope=-111.977 | p=0.00014 | significant_neg=True\n",
      "DQN Reactive       | TailSwaps                 | slope=  -2.511 | p=0.00000 | significant_neg=True\n",
      "DQN Proactive-N    | TotalCancelledFlights     | slope=  -2.796 | p=0.00000 | significant_neg=True\n",
      "DQN Proactive-N    | TotalDelays               | slope=-304.850 | p=0.00000 | significant_neg=True\n",
      "DQN Proactive-N    | TailSwaps                 | slope=   1.386 | p=0.00000 | significant_neg=False\n",
      "DQN Proactive-U    | TotalCancelledFlights     | slope=  -1.886 | p=0.00000 | significant_neg=True\n",
      "DQN Proactive-U    | TotalDelays               | slope=-136.715 | p=0.00000 | significant_neg=True\n",
      "DQN Proactive-U    | TailSwaps                 | slope=  -1.866 | p=0.00000 | significant_neg=True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# Hypothesis 5:\n",
    "#   H0: Increasing schedule slack does NOT lead to significant differences \n",
    "#       among the 4 models across all 3 metrics (no consistent improvements).\n",
    "#   H_alt: As schedule slack increases, performance (fewer cancellations/delays/swaps)\n",
    "#          significantly improves for ALL 4 models (i.e., negative slope in all metrics).\n",
    "\n",
    "# -----------------------------\n",
    "# 1) PREPARE DATA\n",
    "# -----------------------------\n",
    "# We'll assume 'merged_df' has your data.\n",
    "df_all = merged_df.copy()\n",
    "\n",
    "# The 4 relevant models:\n",
    "models_of_interest = [\n",
    "    'Greedy Reactive', \n",
    "    'DQN Reactive', \n",
    "    'DQN Proactive-N', \n",
    "    'DQN Proactive-U'\n",
    "]\n",
    "\n",
    "# We'll check 3 metrics:\n",
    "metrics = [\n",
    "    'TotalCancelledFlights',\n",
    "    'TotalDelays',\n",
    "    'TailSwaps'\n",
    "]\n",
    "\n",
    "# We only keep rows from these 4 models to avoid confusion\n",
    "df_all = df_all[df_all['Model'].isin(models_of_interest)].copy()\n",
    "\n",
    "# Make sure 'ScenarioSlack' is numeric (0..1). If it's already float, that’s fine.\n",
    "df_all['ScenarioSlack'] = pd.to_numeric(df_all['ScenarioSlack'], errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# 2) FUNCTION TO RUN LINEAR REGRESSION AND INTERPRET\n",
    "# -----------------------------\n",
    "def check_slack_effect(df, metric):\n",
    "    \"\"\"\n",
    "    Runs a linear regression: metric ~ Slack\n",
    "    Returns (slope, p_value, is_significantly_negative)\n",
    "    \"\"\"\n",
    "    # Drop any missing rows\n",
    "    df = df.dropna(subset=[metric, 'ScenarioSlack'])\n",
    "    \n",
    "    if len(df) < 2:\n",
    "        # Not enough data for regression\n",
    "        return (None, None, False)\n",
    "    \n",
    "    # Prepare X with intercept\n",
    "    X = sm.add_constant(df['ScenarioSlack'])\n",
    "    y = df[metric]\n",
    "    \n",
    "    # Fit OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Extract slope & p-value for 'ScenarioSlack'\n",
    "    slope = model.params['ScenarioSlack']\n",
    "    p_val = model.pvalues['ScenarioSlack']\n",
    "    \n",
    "    # We want slope < 0 and p_val < ALPHA to say \"significant decrease\"\n",
    "    is_signif_neg = (slope < 0) and (p_val < ALPHA)\n",
    "    \n",
    "    return (slope, p_val, is_signif_neg)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LOOP THROUGH EACH MODEL AND METRIC\n",
    "# -----------------------------\n",
    "all_good = True  # Will remain True only if ALL model-metric combos show significant negative slope\n",
    "\n",
    "results_summary = []  # We'll collect data to print at the end\n",
    "\n",
    "for m in models_of_interest:\n",
    "    df_model = df_all[df_all['Model'] == m]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        slope, p_val, is_signif_neg = check_slack_effect(df_model, metric)\n",
    "        \n",
    "        # We'll store/print partial results now\n",
    "        results_summary.append((m, metric, slope, p_val, is_signif_neg))\n",
    "        \n",
    "        print(f\"Model={m}, Metric={metric}\")\n",
    "        if slope is None:\n",
    "            print(\"  Not enough data to run regression.\\n\")\n",
    "            all_good = False\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Slope={slope:.4f}, p-value={p_val:.6f}\")\n",
    "        if is_signif_neg:\n",
    "            print(f\"  => Slack has a SIGNIFICANT NEGATIVE effect on '{metric}' for {m}.\\n\")\n",
    "        else:\n",
    "            # slope >= 0 or p_val >= ALPHA\n",
    "            print(f\"  => Slack does NOT show a significant negative effect on '{metric}' for {m}.\\n\")\n",
    "            all_good = False\n",
    "\n",
    "# -----------------------------\n",
    "# 4) FINAL DECISION LOGIC\n",
    "# -----------------------------\n",
    "print(\"=== Final Decision on Hypothesis 5 ===\")\n",
    "\n",
    "if all_good:\n",
    "    print(\"All 4 models show a SIGNIFICANT reduction in all 3 metrics as slack increases.\")\n",
    "    print(\"We REJECT H0 and ACCEPT H_alt: Slack helps all approaches significantly.\")\n",
    "else:\n",
    "    print(\"At least one model or metric does NOT show a significant negative slope.\")\n",
    "    print(\"=> We FAIL to confirm that ALL approaches improve on ALL metrics.\")\n",
    "    print(\"Hence, we do NOT accept the alternative hypothesis (H_alt).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) (Optional) Detailed Summary\n",
    "# -----------------------------\n",
    "print(\"\\n=== Detailed Summary ===\")\n",
    "for (m, metric, slope, p_val, is_signif_neg) in results_summary:\n",
    "    if slope is not None:\n",
    "        print(f\"{m:<18} | {metric:<25} | slope={slope:8.3f} | p={p_val:7.5f} | significant_neg={is_signif_neg}\")\n",
    "    else:\n",
    "        print(f\"{m:<18} | {metric:<25} | NO DATA / REGRESSION FAILED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 6: Scenario size scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           ScenarioTime   R-squared:                       0.438\n",
      "Model:                            OLS   Adj. R-squared:                  0.438\n",
      "Method:                 Least Squares   F-statistic:                 3.123e+04\n",
      "Date:                Thu, 02 Jan 2025   Prob (F-statistic):               0.00\n",
      "Time:                        14:21:20   Log-Likelihood:             3.3558e+05\n",
      "No. Observations:              280000   AIC:                        -6.711e+05\n",
      "Df Residuals:                  279992   BIC:                        -6.711e+05\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================================\n",
      "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                    -0.1859      0.002    -84.620      0.000      -0.190      -0.182\n",
      "C(Method)[T.DQN Reactive]                     0.2132      0.003     68.643      0.000       0.207       0.219\n",
      "C(Method)[T.DQN Proactive-N]                  0.2126      0.003     68.447      0.000       0.207       0.219\n",
      "C(Method)[T.DQN Proactive-U]                  0.2126      0.003     68.425      0.000       0.206       0.219\n",
      "TotalFlights                                  0.0222      0.000    163.652      0.000       0.022       0.022\n",
      "C(Method)[T.DQN Reactive]:TotalFlights       -0.0219      0.000   -114.410      0.000      -0.022      -0.022\n",
      "C(Method)[T.DQN Proactive-N]:TotalFlights    -0.0219      0.000   -114.275      0.000      -0.022      -0.022\n",
      "C(Method)[T.DQN Proactive-U]:TotalFlights    -0.0219      0.000   -114.268      0.000      -0.022      -0.022\n",
      "==============================================================================\n",
      "Omnibus:                   188816.448   Durbin-Watson:                   0.410\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         26680697.443\n",
      "Skew:                           2.329   Prob(JB):                         0.00\n",
      "Kurtosis:                      50.594   Cond. No.                         621.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "--- DQN Reactive vs. Greedy Reactive ---\n",
      "Slope (Greedy)        = 0.022160\n",
      "Slope difference      = -0.021909 (p=0)\n",
      "  => Implied slope(DRL) = 0.000251\n",
      "  => This DRL method has a significantly LOWER growth rate than Greedy.\n",
      "\n",
      "--- DQN Proactive-N vs. Greedy Reactive ---\n",
      "Slope (Greedy)        = 0.022160\n",
      "Slope difference      = -0.021883 (p=0)\n",
      "  => Implied slope(DRL) = 0.000277\n",
      "  => This DRL method has a significantly LOWER growth rate than Greedy.\n",
      "\n",
      "--- DQN Proactive-U vs. Greedy Reactive ---\n",
      "Slope (Greedy)        = 0.022160\n",
      "Slope difference      = -0.021882 (p=0)\n",
      "  => Implied slope(DRL) = 0.000278\n",
      "  => This DRL method has a significantly LOWER growth rate than Greedy.\n",
      "\n",
      "=== Hypothesis 6 Decision ===\n",
      "All DRL-based methods have a significantly lower runtime slope than Greedy.\n",
      "We REJECT H0 and ACCEPT H_alt => DRL has slower growth in runtime with scenario size.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# Hypothesis 6:\n",
    "#   H0: No significant difference in how DRL vs. Greedy scale in runtime.\n",
    "#   H_alt: DRL-based methods have a SLOWER growth in runtime as #Flights increases.\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) PREPARE DATA\n",
    "# ------------------------------------------------\n",
    "df_methods = merged_df.copy()\n",
    "\n",
    "# We'll keep only the four relevant methods if needed\n",
    "df_methods = df_methods[df_methods['Model'].isin([\n",
    "    'Greedy Reactive',\n",
    "    'DQN Reactive',\n",
    "    'DQN Proactive-N',\n",
    "    'DQN Proactive-U'\n",
    "])].copy()\n",
    "\n",
    "# Suppose 'Runtime' is a column in seconds or minutes, and 'TotalFlights' is the scenario size\n",
    "# Convert them to numeric if necessary\n",
    "df_methods['ScenarioTime'] = pd.to_numeric(df_methods['ScenarioTime'], errors='coerce')\n",
    "df_methods['TotalFlights'] = pd.to_numeric(df_methods['TotalFlights'], errors='coerce')\n",
    "\n",
    "# Drop missing\n",
    "df_methods = df_methods.dropna(subset=['ScenarioTime', 'TotalFlights'])\n",
    "\n",
    "# Make 'Method' a categorical variable\n",
    "df_methods['Method'] = df_methods['Model'].astype('category')\n",
    "\n",
    "# Ensure \"Greedy Reactive\" is the REFERENCE category in the model\n",
    "df_methods['Method'] = df_methods['Method'].cat.reorder_categories([\n",
    "    'Greedy Reactive',\n",
    "    'DQN Reactive',\n",
    "    'DQN Proactive-N',\n",
    "    'DQN Proactive-U'\n",
    "], ordered=True)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) FIT LINEAR MODEL WITH INTERACTION\n",
    "# ------------------------------------------------\n",
    "# We'll do: Runtime ~ C(Method)*TotalFlights\n",
    "# This gives us a baseline slope for 'Greedy Reactive' \n",
    "# plus slope differences for the DRL methods.\n",
    "model = ols(\"ScenarioTime ~ C(Method)*TotalFlights\", data=df_methods).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# The key parameters of interest:\n",
    "#   - 'TotalFlights' => slope for the reference (Greedy Reactive)\n",
    "#   - 'C(Method)[T.DQN Reactive]:TotalFlights' => slope difference for DQN Reactive vs. Greedy\n",
    "#   - 'C(Method)[T.DQN Proactive-N]:TotalFlights' => slope difference for DQN Proactive-N vs. Greedy\n",
    "#   - 'C(Method)[T.DQN Proactive-U]:TotalFlights' => slope difference for DQN Proactive-U vs. Greedy\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) EXTRACT SLOPES AND TEST IF DRL < GREEDY\n",
    "# ------------------------------------------------\n",
    "params = model.params\n",
    "pvalues = model.pvalues\n",
    "\n",
    "# The slope for Greedy:\n",
    "greedy_slope = params['TotalFlights']\n",
    "greedy_slope_p = pvalues['TotalFlights']\n",
    "\n",
    "# For each DRL method, the total slope = (greedy_slope + difference_term)\n",
    "# We'll check if \"difference_term\" is negative enough to make the DRL slope < Greedy\n",
    "# and if that difference is statistically significant.\n",
    "methods_drl = ['DQN Reactive', 'DQN Proactive-N', 'DQN Proactive-U']\n",
    "\n",
    "# We'll track whether *all* DRL slopes are significantly lower\n",
    "all_drl_slopes_lower = True\n",
    "\n",
    "for m in methods_drl:\n",
    "    # The difference in slope for method m\n",
    "    diff_key = f\"C(Method)[T.{m}]:TotalFlights\"\n",
    "    if diff_key not in params:\n",
    "        # Just in case the design changed\n",
    "        print(f\"Method '{m}' not found in model parameters!\")\n",
    "        all_drl_slopes_lower = False\n",
    "        continue\n",
    "\n",
    "    slope_diff = params[diff_key]\n",
    "    p_val_diff = pvalues[diff_key]\n",
    "\n",
    "    # The DRL slope is:\n",
    "    slope_drl = greedy_slope + slope_diff\n",
    "\n",
    "    print(f\"\\n--- {m} vs. Greedy Reactive ---\")\n",
    "    print(f\"Slope (Greedy)        = {greedy_slope:.6f}\")\n",
    "    print(f\"Slope difference      = {slope_diff:.6f} (p={p_val_diff:.6g})\")\n",
    "    print(f\"  => Implied slope(DRL) = {slope_drl:.6f}\")\n",
    "\n",
    "    # We want slope_drl < greedy_slope AND p_val_diff < ALPHA \n",
    "    # to say \"significantly smaller slope\"\n",
    "    # Actually, because the reference slope can also be uncertain, \n",
    "    # a simpler approach is to see if slope_diff < 0 and p_val_diff < 0.05 \n",
    "    # to say the difference from Greedy is significant and negative.\n",
    "\n",
    "    if (slope_diff < 0) and (p_val_diff < ALPHA):\n",
    "        # This means the DRL slope is significantly lower than Greedy\n",
    "        print(\"  => This DRL method has a significantly LOWER growth rate than Greedy.\")\n",
    "    else:\n",
    "        print(\"  => This DRL method does NOT show a significantly lower slope than Greedy.\")\n",
    "        all_drl_slopes_lower = False\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) DECISION LOGIC ON HYPOTHESIS\n",
    "# ------------------------------------------------\n",
    "print(\"\\n=== Hypothesis 6 Decision ===\")\n",
    "if all_drl_slopes_lower:\n",
    "    print(\"All DRL-based methods have a significantly lower runtime slope than Greedy.\")\n",
    "    print(\"We REJECT H0 and ACCEPT H_alt => DRL has slower growth in runtime with scenario size.\")\n",
    "else:\n",
    "    print(\"At least one DRL method does NOT have a significantly lower slope than Greedy.\")\n",
    "    print(\"We FAIL to confirm that DRL-based methods scale better in all cases => FAIL to reject H0.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
