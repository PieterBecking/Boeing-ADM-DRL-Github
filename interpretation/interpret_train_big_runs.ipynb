{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "base_results_folder = \"../1-archive-runs/07-novel-run\"  # folder containing scenario subfolders\n",
    "cv = False\n",
    "\n",
    "\n",
    "def smooth(data, window=10):\n",
    "    if window > 1 and len(data) >= window:\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "    return data\n",
    "\n",
    "# Scan the base folder for scenario subdirectories\n",
    "scenario_folders = [\n",
    "    f for f in os.listdir(base_results_folder) \n",
    "    if os.path.isdir(os.path.join(base_results_folder, f))\n",
    "]\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "for scenario in scenario_folders:\n",
    "    # Path to the numpy folder inside this scenario\n",
    "    numpy_dir = os.path.join(base_results_folder, scenario, \"numpy\")\n",
    "    if not os.path.exists(numpy_dir):\n",
    "        # If there's no numpy folder, skip this scenario\n",
    "        continue\n",
    "    \n",
    "    # Load the arrays\n",
    "    try:\n",
    "        all_myopic_runs = np.load(os.path.join(numpy_dir, \"all_myopic_runs.npy\"))\n",
    "        all_myopic_steps_runs = np.load(os.path.join(numpy_dir, \"all_myopic_steps_runs.npy\"))\n",
    "        all_proactive_runs = np.load(os.path.join(numpy_dir, \"all_proactive_runs.npy\"))\n",
    "        all_proactive_steps_runs = np.load(os.path.join(numpy_dir, \"all_proactive_steps_runs.npy\"))\n",
    "        all_reactive_runs = np.load(os.path.join(numpy_dir, \"all_reactive_runs.npy\"))\n",
    "        all_reactive_steps_runs = np.load(os.path.join(numpy_dir, \"all_reactive_steps_runs.npy\"))\n",
    "        \n",
    "        # Test rewards - check individual seed files instead of aggregated files\n",
    "        test_rewards_myopic = []\n",
    "        test_rewards_proactive = []\n",
    "        test_rewards_reactive = []\n",
    "        \n",
    "        # Get list of seeds from file names\n",
    "        seed_pattern = re.compile(r'test_rewards_myopic_seed_(\\d+)\\.npy')\n",
    "        seed_files = [f for f in os.listdir(numpy_dir) if seed_pattern.match(f)]\n",
    "        seeds = [seed_pattern.match(f).group(1) for f in seed_files]\n",
    "        \n",
    "        for seed in seeds:\n",
    "            myopic_path = os.path.join(numpy_dir, f\"test_rewards_myopic_seed_{seed}.npy\")\n",
    "            proactive_path = os.path.join(numpy_dir, f\"test_rewards_proactive_seed_{seed}.npy\")\n",
    "            reactive_path = os.path.join(numpy_dir, f\"test_rewards_reactive_seed_{seed}.npy\")\n",
    "            \n",
    "            if os.path.exists(myopic_path):\n",
    "                test_rewards_myopic.append(np.load(myopic_path).tolist())\n",
    "            if os.path.exists(proactive_path):\n",
    "                test_rewards_proactive.append(np.load(proactive_path).tolist())\n",
    "            if os.path.exists(reactive_path):\n",
    "                test_rewards_reactive.append(np.load(reactive_path).tolist())\n",
    "\n",
    "        # Convert to numpy arrays if any data was loaded and pad sequences to same length\n",
    "        if test_rewards_myopic:\n",
    "            max_len = max(len(x) for x in test_rewards_myopic)\n",
    "            test_rewards_myopic = np.array([x + [np.nan]*(max_len-len(x)) for x in test_rewards_myopic])\n",
    "        if test_rewards_proactive:\n",
    "            max_len = max(len(x) for x in test_rewards_proactive)\n",
    "            test_rewards_proactive = np.array([x + [np.nan]*(max_len-len(x)) for x in test_rewards_proactive])\n",
    "        if test_rewards_reactive:\n",
    "            max_len = max(len(x) for x in test_rewards_reactive)\n",
    "            test_rewards_reactive = np.array([x + [np.nan]*(max_len-len(x)) for x in test_rewards_reactive])\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Missing files in {numpy_dir}, skipping scenario '{scenario}'. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Compute means and std\n",
    "    myopic_mean = all_myopic_runs.mean(axis=0)\n",
    "    myopic_std = all_myopic_runs.std(axis=0)\n",
    "    proactive_mean = all_proactive_runs.mean(axis=0)\n",
    "    proactive_std = all_proactive_runs.std(axis=0)\n",
    "    reactive_mean = all_reactive_runs.mean(axis=0)\n",
    "    reactive_std = all_reactive_runs.std(axis=0)\n",
    "\n",
    "    myopic_steps_mean = all_myopic_steps_runs.mean(axis=0).astype(int)\n",
    "    proactive_steps_mean = all_proactive_steps_runs.mean(axis=0).astype(int)\n",
    "    reactive_steps_mean = all_reactive_steps_runs.mean(axis=0).astype(int)\n",
    "\n",
    "    # Smooth data for plotting \n",
    "    smooth_window = 1\n",
    "    \n",
    "    myopic_mean_sm = smooth(myopic_mean, smooth_window)\n",
    "    myopic_std_sm = smooth(myopic_std, smooth_window)\n",
    "    myopic_steps_sm = myopic_steps_mean[:len(myopic_mean_sm)]\n",
    "\n",
    "    proactive_mean_sm = smooth(proactive_mean, smooth_window)\n",
    "    proactive_std_sm = smooth(proactive_std, smooth_window)\n",
    "    proactive_steps_sm = proactive_steps_mean[:len(proactive_mean_sm)]\n",
    "\n",
    "    reactive_mean_sm = smooth(reactive_mean, smooth_window)\n",
    "    reactive_std_sm = smooth(reactive_std, smooth_window)\n",
    "    reactive_steps_sm = reactive_steps_mean[:len(reactive_mean_sm)]\n",
    "\n",
    "    # Create training plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(proactive_steps_sm, proactive_mean_sm, label=\"DQN Proactive-U\", color='orange')\n",
    "    plt.fill_between(proactive_steps_sm, proactive_mean_sm - proactive_std_sm, proactive_mean_sm + proactive_std_sm, alpha=0.2, color='orange')\n",
    "    plt.plot(myopic_steps_sm, myopic_mean_sm, label=\"DQN Proactive-N\", color='blue')\n",
    "    plt.fill_between(myopic_steps_sm, myopic_mean_sm - myopic_std_sm, myopic_mean_sm + myopic_std_sm, alpha=0.2, color='blue')\n",
    "    plt.plot(reactive_steps_sm, reactive_mean_sm, label=\"DQN Reactive\", color='green')\n",
    "    plt.fill_between(reactive_steps_sm, reactive_mean_sm - reactive_std_sm, reactive_mean_sm + reactive_std_sm, alpha=0.2, color='green')\n",
    "    plt.xlabel(\"Environment Steps\")\n",
    "    plt.ylabel(\"Episode Reward\")\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    if cv:\n",
    "        # Create testing plot\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        if test_rewards_proactive is not None:\n",
    "            test_mean_proactive = np.nanmean(test_rewards_proactive, axis=0)\n",
    "            test_std_proactive = np.nanstd(test_rewards_proactive, axis=0)\n",
    "            plt.plot(test_mean_proactive, label=\"DQN Proactive-U\", color='orange')\n",
    "            plt.fill_between(range(len(test_mean_proactive)), test_mean_proactive - test_std_proactive, test_mean_proactive + test_std_proactive, alpha=0.2, color='orange')\n",
    "        \n",
    "        if test_rewards_myopic is not None:\n",
    "            test_mean_myopic = np.nanmean(test_rewards_myopic, axis=0)\n",
    "            test_std_myopic = np.nanstd(test_rewards_myopic, axis=0)\n",
    "            plt.plot(test_mean_myopic, label=\"DQN Proactive-N\", color='blue')\n",
    "            plt.fill_between(range(len(test_mean_myopic)), test_mean_myopic - test_std_myopic, test_mean_myopic + test_std_myopic, alpha=0.2, color='blue')\n",
    "        \n",
    "        if test_rewards_reactive is not None:\n",
    "            test_mean_reactive = np.nanmean(test_rewards_reactive, axis=0)\n",
    "            test_std_reactive = np.nanstd(test_rewards_reactive, axis=0)\n",
    "            plt.plot(test_mean_reactive, label=\"DQN Reactive\", color='green')\n",
    "            plt.fill_between(range(len(test_mean_reactive)), test_mean_reactive - test_std_reactive, test_mean_reactive + test_std_reactive, alpha=0.2, color='green')\n",
    "        \n",
    "        plt.xlabel(\"Test Episode\")\n",
    "        plt.ylabel(\"Episode Reward\")\n",
    "        plt.title(\"Testing Performance\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # # Combined plot\n",
    "    # ax3.plot(proactive_steps_sm, proactive_mean_sm, label=\"Train DQN Proactive-U\", color='orange')\n",
    "    # ax3.fill_between(proactive_steps_sm, proactive_mean_sm - proactive_std_sm, proactive_mean_sm + proactive_std_sm, alpha=0.2, color='orange')\n",
    "    # ax3.plot(myopic_steps_sm, myopic_mean_sm, label=\"Train DQN Proactive-N\", color='blue')\n",
    "    # ax3.fill_between(myopic_steps_sm, myopic_mean_sm - myopic_std_sm, myopic_mean_sm + myopic_std_sm, alpha=0.2, color='blue')\n",
    "    # ax3.plot(reactive_steps_sm, reactive_mean_sm, label=\"Train DQN Reactive\", color='green')\n",
    "    # ax3.fill_between(reactive_steps_sm, reactive_mean_sm - reactive_std_sm, reactive_mean_sm + reactive_std_sm, alpha=0.2, color='green')\n",
    "\n",
    "    # # Add test points\n",
    "    # if test_rewards_proactive is not None:\n",
    "    #     ax3.plot(proactive_steps_sm[-1], test_mean_proactive[-1], 'o', label=\"Test DQN Proactive-U\", color='orange', markersize=10)\n",
    "    # if test_rewards_myopic is not None:\n",
    "    #     ax3.plot(myopic_steps_sm[-1], test_mean_myopic[-1], 'o', label=\"Test DQN Proactive-N\", color='blue', markersize=10)\n",
    "    # if test_rewards_reactive is not None:\n",
    "    #     ax3.plot(reactive_steps_sm[-1], test_mean_reactive[-1], 'o', label=\"Test DQN Reactive\", color='green', markersize=10)\n",
    "\n",
    "    # ax3.set_xlabel(\"Environment Steps\")\n",
    "    # ax3.set_ylabel(\"Episode Reward\")\n",
    "    # ax3.set_title(\"Combined Training and Testing Performance\")\n",
    "    # ax3.legend()\n",
    "    # ax3.grid(True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np= <module 'numpy' from 'C:\\\\Users\\\\Admin\\\\Desktop\\\\Boeing-ADM-DRL-Github\\\\.venv\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'>\n",
      "MAX_AIRCRAFT= 6\n",
      "MAX_FLIGHTS_PER_AIRCRAFT= 20\n",
      "ROWS_STATE_SPACE= 7\n",
      "COLUMNS_STATE_SPACE= 63\n",
      "ACTION_SPACE_SIZE= 147\n",
      "DEPARTURE_AFTER_END_RECOVERY= 1\n",
      "TIMESTEP_HOURS= 1\n",
      "DUMMY_VALUE= -999\n",
      "RESOLVED_CONFLICT_REWARD= 5000\n",
      "DELAY_MINUTE_PENALTY= 11.5\n",
      "MAX_DELAY_PENALTY= 2500\n",
      "NO_ACTION_PENALTY= 0\n",
      "CANCELLED_FLIGHT_PENALTY= 5000\n",
      "LAST_MINUTE_THRESHOLD= 120\n",
      "LAST_MINUTE_FLIGHT_PENALTY= 455\n",
      "AHEAD_BONUS_PER_MINUTE= 0.1\n",
      "TIME_MINUTE_PENALTY= 1\n",
      "TERMINATION_REWARD= 500\n",
      "MIN_TURN_TIME= 0\n",
      "MIN_BREAKDOWN_PROBABILITY= 0\n",
      "DEBUG_MODE= False\n",
      "DEBUG_MODE_TRAINING= False\n",
      "DEBUG_MODE_REWARD= False\n",
      "DEBUG_MODE_PRINT_STATE= False\n",
      "DEBUG_MODE_CANCELLED_FLIGHT= False\n",
      "DEBUG_MODE_VISUALIZATION= False\n",
      "DEBUG_MODE_BREAKDOWN= False\n",
      "DEBUG_MODE_ACTION= False\n",
      "DEBUG_MODE_STOPPING_CRITERIA= False\n",
      "DEBUG_MODE_SCHEDULING= False\n",
      "DEBUG_MODE_REWARD_LAST_MINUTE_PENALTY= False\n",
      "DEBUG_MODE_REWARD_RESOLVED_CONFLICTS= False\n",
      "MAX_TOTAL_TIMESTEPS= 5000000\n",
      "SEEDS= ['1023', '1024', '1025', '1026', '1027']\n",
      "brute_force_flag= False\n",
      "cross_val_flag= True\n",
      "early_stopping_flag= False\n",
      "CROSS_VAL_INTERVAL= 10\n",
      "printing_intermediate_results= False\n"
     ]
    }
   ],
   "source": [
    "# Load and print config values from config.csv\n",
    "config_df = pd.read_csv(base_results_folder + \"/config.csv\")\n",
    "for column in config_df.columns:\n",
    "    print(f\"{column}= {config_df[column].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Testing/6ac-350-diverse/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m SCENARIO_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/Testing/6ac-350-diverse/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Extract all scenario folders\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m scenario_folders \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SCENARIO_FOLDER, d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCENARIO_FOLDER\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SCENARIO_FOLDER, d))]\n\u001b[1;32m    105\u001b[0m scenario_folders\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_env_type\u001b[39m(model_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Testing/6ac-350-diverse/'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.visualizations import StatePlotter\n",
    "from scripts.utils import load_scenario_data\n",
    "from src.config import *\n",
    "\n",
    "def run_inference_dqn(model_path, scenario_folder, env_type, seed):\n",
    "    # Load scenario data\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Initialize environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, \n",
    "        flights_dict, \n",
    "        rotations_dict, \n",
    "        alt_aircraft_dict, \n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    # Start timing the inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        # Convert observation to float32\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "        # Get action mask\n",
    "        action_mask = obs.get('action_mask', None)\n",
    "        if action_mask is None:\n",
    "            raise ValueError(\"Action mask is missing in the observation!\")\n",
    "\n",
    "        # Get Q-values\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        # Mask invalid actions\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "        action = np.argmax(masked_q_values)\n",
    "\n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done_flag = terminated or truncated\n",
    "        step_num += 1\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    return total_reward, step_num, runtime\n",
    "\n",
    "# Example seed and training IDs\n",
    "seed = 26\n",
    "training_id_myopic = 506\n",
    "training_id_proactive = 513\n",
    "training_id_reactive = 518\n",
    "\n",
    "model_to_compare = [\n",
    "    f\"../trained_models/dqn/6ac-350-diverse/{seed}/training_{training_id_myopic}/myopic-training_{training_id_myopic}.zip\",\n",
    "    f\"../trained_models/dqn/6ac-350-diverse/{seed}/training_{training_id_proactive}/proactive-training_{training_id_proactive}.zip\",\n",
    "    f\"../trained_models/dqn/6ac-350-diverse/{seed}/training_{training_id_reactive}/reactive-training_{training_id_reactive}.zip\",\n",
    "]\n",
    "\n",
    "# Define the scenario folder set\n",
    "SCENARIO_FOLDER = \"../data/Testing/6ac-350-diverse/\"\n",
    "\n",
    "# Extract all scenario folders\n",
    "scenario_folders = [os.path.join(SCENARIO_FOLDER, d) for d in os.listdir(SCENARIO_FOLDER) \n",
    "                    if os.path.isdir(os.path.join(SCENARIO_FOLDER, d))]\n",
    "scenario_folders.sort()\n",
    "\n",
    "def extract_env_type(model_path):\n",
    "    match = re.search(r'/(myopic|proactive|reactive)', model_path)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Model path {model_path} does not contain a valid environment type!\")\n",
    "    return match.group(1)\n",
    "\n",
    "# Number of runs\n",
    "N_RUNS = 5\n",
    "\n",
    "num_scenarios = len(scenario_folders)\n",
    "num_models = len(model_to_compare)\n",
    "\n",
    "results_all_runs = np.zeros((N_RUNS, num_scenarios, num_models))\n",
    "runtime_all_runs = np.zeros((N_RUNS, num_scenarios, num_models))\n",
    "\n",
    "base_seed = 12345\n",
    "\n",
    "for run_idx in range(N_RUNS):\n",
    "    current_seed = base_seed + run_idx\n",
    "    \n",
    "    for m_idx, model_path in enumerate(model_to_compare):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "            \n",
    "        env_type = extract_env_type(model_path)\n",
    "        \n",
    "        for s_idx, scenario_folder in enumerate(scenario_folders):\n",
    "            if not os.path.exists(scenario_folder):\n",
    "                raise FileNotFoundError(f\"Scenario folder not found: {scenario_folder}\")\n",
    "            \n",
    "            total_reward, steps, runtime = run_inference_dqn(model_path, scenario_folder, env_type, current_seed)\n",
    "            # Store results\n",
    "            results_all_runs[run_idx, s_idx, m_idx] = total_reward\n",
    "            runtime_all_runs[run_idx, s_idx, m_idx] = runtime\n",
    "\n",
    "# Compute run-level means for rewards and runtime\n",
    "run_mean_rewards = np.mean(results_all_runs, axis=1)  # shape: (N_RUNS, num_models)\n",
    "run_mean_runtime = np.mean(runtime_all_runs, axis=1)  # shape: (N_RUNS, num_models)\n",
    "\n",
    "# Compute final mean and std across runs\n",
    "final_mean_rewards = np.mean(run_mean_rewards, axis=0)\n",
    "final_std_rewards = np.std(run_mean_rewards, axis=0)\n",
    "\n",
    "final_mean_runtime = np.mean(run_mean_runtime, axis=0)\n",
    "final_std_runtime = np.std(run_mean_runtime, axis=0)\n",
    "\n",
    "model_names = [os.path.basename(m) for m in model_to_compare]\n",
    "\n",
    "# Print final results\n",
    "print(\"Final Average Rewards (averaged over scenarios and then over runs):\")\n",
    "for m_name, val in zip(model_names, final_mean_rewards):\n",
    "    print(f\"{m_name}: {val}\")\n",
    "\n",
    "print(\"\\nFinal Std of Rewards (across runs):\")\n",
    "for m_name, val in zip(model_names, final_std_rewards):\n",
    "    print(f\"{m_name}: {val}\")\n",
    "\n",
    "print(\"\\nFinal Average Runtime (averaged over scenarios and then over runs):\")\n",
    "for m_name, val in zip(model_names, final_mean_runtime):\n",
    "    print(f\"{m_name}: {val}\")\n",
    "\n",
    "print(\"\\nFinal Std of Runtime (across runs):\")\n",
    "for m_name, val in zip(model_names, final_std_runtime):\n",
    "    print(f\"{m_name}: {val}\")\n",
    "\n",
    "# Pairwise t-tests for rewards\n",
    "print(\"\\nPairwise p-values for REWARDS:\")\n",
    "p_value_rewards = pd.DataFrame(index=model_names, columns=model_names)\n",
    "for i in range(num_models):\n",
    "    for j in range(num_models):\n",
    "        if i == j:\n",
    "            p_value_rewards.iloc[i, j] = np.nan\n",
    "        else:\n",
    "            stat, p_val = ttest_rel(run_mean_rewards[:, i], run_mean_rewards[:, j])\n",
    "            p_value_rewards.iloc[i, j] = p_val\n",
    "print(p_value_rewards)\n",
    "\n",
    "# Pairwise t-tests for runtime\n",
    "print(\"\\nPairwise p-values for RUNTIME:\")\n",
    "p_value_runtime = pd.DataFrame(index=model_names, columns=model_names)\n",
    "for i in range(num_models):\n",
    "    for j in range(num_models):\n",
    "        if i == j:\n",
    "            p_value_runtime.iloc[i, j] = np.nan\n",
    "        else:\n",
    "            stat, p_val = ttest_rel(run_mean_runtime[:, i], run_mean_runtime[:, j])\n",
    "            p_value_runtime.iloc[i, j] = p_val\n",
    "print(p_value_runtime)\n",
    "\n",
    "# Plotting using the actual calculated values\n",
    "labels = model_names\n",
    "bar_colors = ['orange', 'blue', 'green'][:len(labels)]  # In case you have fewer or more models\n",
    "\n",
    "y = np.arange(len(labels))  # the label locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure(figsize=(7, 2))\n",
    "plt.barh(y, final_mean_rewards, xerr=final_std_rewards, capsize=5, color=bar_colors, alpha=0.3)\n",
    "plt.xlabel('Mean Cumulative Reward', fontsize=12)\n",
    "plt.yticks(y, labels, fontsize=12)\n",
    "plt.axvline(x=0, color='gray', linewidth=1, linestyle='--')\n",
    "plt.title('Mean Cumulative Reward by DQN Approach')\n",
    "plt.grid(alpha=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot runtime\n",
    "plt.figure(figsize=(7, 2))\n",
    "plt.barh(y, final_mean_runtime, xerr=final_std_runtime, capsize=5, color=bar_colors, alpha=0.3)\n",
    "plt.xlabel('Mean Runtime (seconds)', fontsize=12)\n",
    "plt.yticks(y, labels, fontsize=12)\n",
    "plt.title('Mean Runtime by DQN Approach')\n",
    "plt.grid(alpha=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
