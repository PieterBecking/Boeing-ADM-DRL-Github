{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>PREP: </b>Scenario analysis\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "# def time_to_minutes(timestr):\n",
    "#     # Handle '+1' suffix by removing it before parsing\n",
    "#     timestr = timestr.split('+')[0]  # Remove '+1' if present\n",
    "#     hh, mm = timestr.split(':')\n",
    "#     return int(hh) * 60 + int(mm)\n",
    "\n",
    "# def calculate_slack_for_scenario(scenario_data):\n",
    "#     \"\"\"\n",
    "#     Calculate the slack metric for the given scenario.\n",
    "    \n",
    "#     Slack is defined as:\n",
    "#         Slack = 1 - (total flight minutes in recovery period / total recovery period aircraft-minutes)\n",
    "    \n",
    "#     A slack of 1 means no flights during recovery period.\n",
    "#     A slack of 0 means flights occupy the entire recovery period.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def time_to_minutes(timestr):\n",
    "#         # Handle '+1' suffix by removing it before parsing\n",
    "#         timestr = timestr.split('+')[0]  # Remove '+1' if present\n",
    "#         hh, mm = timestr.split(':')\n",
    "#         return int(hh) * 60 + int(mm)\n",
    "#     # Extract scenario start/end times\n",
    "#     # We assume the same date for start and end for simplicity.\n",
    "#     recovery_start_time_str = scenario_data[\"recovery_start_time\"]  \n",
    "#     recovery_end_time_str = scenario_data[\"recovery_end_time\"]      \n",
    "    \n",
    "#     recovery_start_minutes = time_to_minutes(recovery_start_time_str)\n",
    "#     recovery_end_minutes = time_to_minutes(recovery_end_time_str)\n",
    "#     total_recovery_period_minutes = recovery_end_minutes - recovery_start_minutes\n",
    "    \n",
    "#     total_aircraft = scenario_data[\"total_aircraft\"]\n",
    "    \n",
    "#     # Calculate total flight minutes within the recovery period\n",
    "#     flights = scenario_data[\"flights\"]\n",
    "#     total_flights = len(flights)\n",
    "#     total_flight_minutes_in_recovery = 0\n",
    "#     total_flight_minutes_total = 0\n",
    "    \n",
    "#     for flight_id, flight_data in flights.items():\n",
    "#         dep_time_str = flight_data[\"DepTime\"]  \n",
    "#         arr_time_str = flight_data[\"ArrTime\"] \n",
    "        \n",
    "#         dep_minutes = time_to_minutes(dep_time_str)\n",
    "#         arr_minutes = time_to_minutes(arr_time_str)\n",
    "        \n",
    "#         total_flight_minutes_total += arr_minutes - dep_minutes\n",
    "#         overlap_start = max(dep_minutes, recovery_start_minutes)\n",
    "#         overlap_end = min(arr_minutes, recovery_end_minutes)\n",
    "        \n",
    "#         if overlap_end > overlap_start:\n",
    "#             flight_overlap = overlap_end - overlap_start\n",
    "#         else:\n",
    "#             flight_overlap = 0\n",
    "        \n",
    "#         total_flight_minutes_in_recovery += flight_overlap\n",
    "    \n",
    "#     # Calculate total aircraft-minutes available during the recovery period\n",
    "#     total_recovery_aircraft_minutes = total_recovery_period_minutes * total_aircraft\n",
    "    \n",
    "#     # Slack calculation\n",
    "#     if total_recovery_aircraft_minutes == 0:\n",
    "#         slack = 1.0\n",
    "#     else:\n",
    "#         slack = 1 - (total_flight_minutes_in_recovery / total_recovery_aircraft_minutes)\n",
    "    \n",
    "#     return slack, total_flights, total_flight_minutes_total\n",
    "\n",
    "\n",
    "# def extract_disruption_stats(scenario_data):\n",
    "#     \"\"\"\n",
    "#     Extract disruption statistics:\n",
    "#     - Count of fully disrupted (prob = 1.0)\n",
    "#     - Count of uncertain disruptions (0 < prob < 1.0)\n",
    "#     - Average probability across all aircraft (where an aircraft's probability is the max disruption probability it faces, \n",
    "#       with 1.0 for fully disrupted and 0.0 if no disruption)\n",
    "#     - Average uncertainty probability (average of all disruptions where 0<prob<1.0, excluding 0 and 1)\n",
    "#     \"\"\"\n",
    "#     disruptions_info = scenario_data.get('disruptions', {})\n",
    "#     disruptions_list = disruptions_info.get('disruptions', [])\n",
    "#     total_aircraft = disruptions_info.get('total_aircraft', 0)\n",
    "\n",
    "#     if total_aircraft == 0:\n",
    "#         # No aircraft or no disruptions\n",
    "#         return 0, 0, 0.0, 0.0\n",
    "\n",
    "#     fully_disrupted_count = sum(1 for d in disruptions_list if d.get('probability', 0.0) == 1.0)\n",
    "#     uncertain_disruptions = [d for d in disruptions_list if 0.0 < d.get('probability', 0.0) < 1.0]\n",
    "#     uncertain_count = len(uncertain_disruptions)\n",
    "\n",
    "#     aircraft_ids = scenario_data.get('aircraft_ids', [])\n",
    "#     ac_prob_map = {ac: 0.0 for ac in aircraft_ids}  \n",
    "    \n",
    "#     for d in disruptions_list:\n",
    "#         ac_id = d.get('aircraft_id')\n",
    "#         p = d.get('probability', 0.0)\n",
    "#         # Keep the max probability for that aircraft\n",
    "#         if ac_id in ac_prob_map:\n",
    "#             ac_prob_map[ac_id] = max(ac_prob_map[ac_id], p)\n",
    "\n",
    "#     avg_ac_prob = sum(ac_prob_map.values()) / total_aircraft if total_aircraft > 0 else 0.0\n",
    "\n",
    "#     # Average uncertainty probability (only consider disruptions where 0<prob<1)\n",
    "#     if len(uncertain_disruptions) > 0:\n",
    "#         avg_uncertainty_prob = np.mean([d['probability'] for d in uncertain_disruptions])\n",
    "#     else:\n",
    "#         avg_uncertainty_prob = 0.0\n",
    "\n",
    "#     return fully_disrupted_count, uncertain_count, avg_ac_prob, avg_uncertainty_prob, total_aircraft\n",
    "\n",
    "# # Path to the scenarios folder\n",
    "# scenario_folder_path = \"../logs/scenarios/\"\n",
    "# latest_folder = max(\n",
    "#     [f for f in os.listdir(scenario_folder_path) if f.startswith(\"scenario_folder_\")],\n",
    "#     key=lambda x: int(x.split('_')[-1].replace('.json', ''))\n",
    "# )\n",
    "\n",
    "# latest_folder = \"scenario_folder_scenario_84.json\"\n",
    "\n",
    "# file_path = os.path.join(scenario_folder_path, latest_folder)\n",
    "\n",
    "# # Extract scenario ID\n",
    "# scenario_id = file_path.split('_')[-1].split('.')[0]\n",
    "# print(f\"Scenario ID: {scenario_id}\")\n",
    "\n",
    "# # Load the JSON data\n",
    "# with open(file_path, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Extract the scenarios from the JSON data\n",
    "# scenarios = data['outputs']\n",
    "\n",
    "\n",
    "# # Extract the data_folder (not strictly necessary for slack calculation, but we print it for context)\n",
    "# data_folder = data['data_folder']\n",
    "# print(f\"Data Folder: {data_folder}\")\n",
    "\n",
    "# # Calculate slack and disruption stats for each scenario and store in a list of dicts\n",
    "# results = []\n",
    "# for scenario_name, scenario_data in scenarios.items():\n",
    "#     scenario_slack, total_flights, total_flight_minutes_total = calculate_slack_for_scenario(scenario_data)\n",
    "#     fully_disrupted_count, uncertain_count, avg_ac_prob, avg_uncertain_prob, total_aircraft = extract_disruption_stats(scenario_data)\n",
    "#     results.append({\n",
    "#         \"Scenario\": scenario_name,\n",
    "#         \"ScenarioSlack\": scenario_slack,\n",
    "#         \"TotalFlights\": total_flights,\n",
    "#         \"TotalFlightMinutes\": total_flight_minutes_total,\n",
    "#         \"FullyDisruptedCount\": fully_disrupted_count,\n",
    "#         \"UncertainCount\": uncertain_count,\n",
    "#         \"AvgAircraftProbability\": avg_ac_prob,\n",
    "#         \"AvgUncertaintyProbability\": avg_uncertain_prob,\n",
    "#         \"TotalAircraft\": total_aircraft\n",
    "#     })\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# scenarios_df = pd.DataFrame(results)\n",
    "# print(scenarios_df)\n",
    "\n",
    "# # Save the slack results to CSV\n",
    "# # output_file = os.path.join(scenario_folder_path, f\"scenario_slack_metrics_{scenario_id}.csv\")\n",
    "# # scenarios_df.to_csv(output_file, index=False)\n",
    "# # print(f\"Slack metrics saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>RUN: </b>Inferencing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "import os\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from stable_baselines3 import DQN\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from scripts.utils import load_scenario_data, NumpyEncoder, get_training_metadata\n",
    "from scripts.logger import create_new_id, log_inference_metadata, find_corresponding_training_id, convert_to_serializable\n",
    "\n",
    "def run_inference_dqn_single(model_path, scenario_folder, env_type, seed):\n",
    "    \"\"\"\n",
    "    Runs inference on a single scenario and returns the total reward.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # print(f\"*** initializing env with env_type: {env_type}\")\n",
    "\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, flights_dict, rotations_dict, alt_aircraft_dict, config_dict, env_type=env_type\n",
    "    )\n",
    "\n",
    "    model = DQN.load(model_path)\n",
    "    model.set_env(env)\n",
    "    model.policy.set_training_mode(False)\n",
    "    model.exploration_rate = 0.0\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    done_flag = False\n",
    "    total_reward = 0\n",
    "    step_num = 0\n",
    "    max_steps = 1000\n",
    "\n",
    "    while not done_flag and step_num < max_steps:\n",
    "        action_mask = obs['action_mask']\n",
    "        obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "        obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "        q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        masked_q_values = q_values.copy()\n",
    "        masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "        # If no valid actions remain, break out\n",
    "        if np.all(np.isinf(masked_q_values)):\n",
    "            break\n",
    "        valid_actions = np.where(masked_q_values != -np.inf)[0]\n",
    "        action = np.random.choice(valid_actions)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done_flag = terminated or truncated\n",
    "        step_num += 1\n",
    "\n",
    "    total_delays = env.scenario_wide_delay_minutes\n",
    "    total_cancelled_flights = env.scenario_wide_cancelled_flights\n",
    "    end_time = time.time()\n",
    "    scenario_time = end_time - start_time\n",
    "    scenario_steps = env.scenario_wide_steps\n",
    "    scenario_resolved_conflicts = env.scenario_wide_resolved_conflicts\n",
    "    solution_slack = env.scenario_wide_solution_slack\n",
    "    scenario_wide_tail_swaps = env.scenario_wide_tail_swaps\n",
    "    scenario_wide_actual_disrupted_flights = env.scenario_wide_actual_disrupted_flights\n",
    "    scenario_wide_reward_components = env.scenario_wide_reward_components\n",
    "\n",
    "    return total_reward, total_delays, total_cancelled_flights, scenario_time, scenario_steps, scenario_resolved_conflicts, solution_slack, scenario_wide_tail_swaps, scenario_wide_actual_disrupted_flights, scenario_wide_reward_components\n",
    "\n",
    "\n",
    "import os\n",
    "from scripts.utils import load_scenario_data\n",
    "from src.environment import AircraftDisruptionOptimizer\n",
    "from scripts.visualizations import StatePlotter\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def run_exact_single(scenario_folder):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load scenario data first\n",
    "    data_dict = load_scenario_data(scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "    \n",
    "    # Parse recovery period times\n",
    "    recovery_period = config_dict['RecoveryPeriod']\n",
    "    start_datetime = datetime.strptime(f\"{recovery_period['StartDate']} {recovery_period['StartTime']}\", '%d/%m/%y %H:%M')\n",
    "    end_datetime = datetime.strptime(f\"{recovery_period['EndDate']} {recovery_period['EndTime']}\", '%d/%m/%y %H:%M')\n",
    "    \n",
    "    # Initialize optimizer with loaded data\n",
    "    optimizer = AircraftDisruptionOptimizer(\n",
    "        aircraft_dict=aircraft_dict,\n",
    "        flights_dict=flights_dict,\n",
    "        rotations_dict=rotations_dict,\n",
    "        alt_aircraft_dict=alt_aircraft_dict,\n",
    "        config_dict=config_dict\n",
    "    )\n",
    "    \n",
    "    solution = optimizer.solve()\n",
    "    \n",
    "    # Convert optimizer solution to state plotter format\n",
    "    swapped_flights = [(flight_id, new_aircraft) for flight_id, new_aircraft in solution['assignments'].items()]\n",
    "    environment_delayed_flights = set(solution['delays'].keys())\n",
    "    cancelled_flights = set(solution['cancellations'])\n",
    "    \n",
    "    # Update flights dict with delays\n",
    "    updated_flights_dict = flights_dict.copy()\n",
    "    for flight_id, delay in solution['delays'].items():\n",
    "        if flight_id in updated_flights_dict:\n",
    "            flight_info = updated_flights_dict[flight_id]\n",
    "            flight_info['Delay'] = delay\n",
    "\n",
    "    total_reward = optimizer.scenario_wide_reward_total\n",
    "    total_delays = optimizer.scenario_wide_delay_minutes\n",
    "    total_cancelled_flights = optimizer.scenario_wide_cancelled_flights\n",
    "    scenario_steps = optimizer.scenario_wide_steps\n",
    "    scenario_resolved_conflicts = optimizer.scenario_wide_resolved_conflicts\n",
    "    solution_slack = optimizer.scenario_wide_solution_slack\n",
    "    scenario_wide_tail_swaps = optimizer.scenario_wide_tail_swaps\n",
    "    scenario_wide_actual_disrupted_flights = optimizer.scenario_wide_actual_disrupted_flights\n",
    "    scenario_wide_reward_components = optimizer.scenario_wide_reward_components\n",
    "\n",
    "    end_time = time.time()\n",
    "    scenario_time = end_time - start_time\n",
    "\n",
    "    return total_reward, total_delays, total_cancelled_flights, scenario_time, scenario_steps, scenario_resolved_conflicts, solution_slack, scenario_wide_tail_swaps, scenario_wide_actual_disrupted_flights, scenario_wide_reward_components\n",
    "\n",
    "\n",
    "def run_inference_on_data_folder(model_paths, data_folder, seeds):\n",
    "    \"\"\"\n",
    "    Runs inference on all scenarios found in 'data_folder', for each model in 'model_paths' and each seed in 'seeds'.\n",
    "\n",
    "    Args:\n",
    "        model_paths (list): List of tuples containing (model_path, env_type).\n",
    "        data_folder (str): Path to the folder containing scenario subfolders.\n",
    "        seeds (list): List of seeds for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing scenario, model, seed, and total reward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all scenario folders within data_folder\n",
    "    scenario_folders = [\n",
    "        os.path.join(data_folder, folder)\n",
    "        for folder in os.listdir(data_folder)\n",
    "        if os.path.isdir(os.path.join(data_folder, folder))\n",
    "    ]\n",
    "\n",
    "    num_combinations = len(model_paths) * len(seeds) * len(scenario_folders)\n",
    "    print(f\"Number of combinations: {num_combinations}\")\n",
    "    current_combination = 0\n",
    "\n",
    "    results = []\n",
    "    for scenario_folder in scenario_folders:\n",
    "        scenario_name = os.path.basename(scenario_folder)\n",
    "        for model_tuple in model_paths:\n",
    "            model_path, env_type = model_tuple  # Unpack the tuple correctly\n",
    "            for seed in seeds:\n",
    "                current_combination += 1\n",
    "                print(f\"({current_combination}/{num_combinations} - {round(current_combination/num_combinations*100, 2)}%)\")\n",
    "                print(f\"env_type: {env_type}, model_path: {model_path}, scenario_folder: {scenario_folder}, seed: {seed}\")\n",
    "                if env_type == \"exact\":\n",
    "                    total_reward, total_delays, total_cancelled_flights, scenario_time, scenario_steps, scenario_resolved_conflicts, solution_slack, scenario_wide_tail_swaps, scenario_wide_actual_disrupted_flights, scenario_wide_reward_components = run_exact_single(scenario_folder)\n",
    "                else:\n",
    "                    total_reward, total_delays, total_cancelled_flights, scenario_time, scenario_steps, scenario_resolved_conflicts, solution_slack, scenario_wide_tail_swaps, scenario_wide_actual_disrupted_flights, scenario_wide_reward_components = run_inference_dqn_single(model_path, scenario_folder, env_type, seed)\n",
    "                results.append({\n",
    "                    \"Scenario\": scenario_name,\n",
    "                    \"Model\": os.path.basename(model_path),\n",
    "                    \"Seed\": seed,\n",
    "                    \"TotalReward\": total_reward,\n",
    "                    \"TotalDelays\": total_delays,\n",
    "                    \"TotalCancelledFlights\": total_cancelled_flights,\n",
    "                    \"ScenarioTime\": scenario_time,\n",
    "                    \"ScenarioSteps\": scenario_steps,\n",
    "                    \"ScenarioResolvedConflicts\": scenario_resolved_conflicts,\n",
    "                    \"SolutionSlack\": solution_slack,\n",
    "                    \"TailSwaps\": scenario_wide_tail_swaps,\n",
    "                    \"ActualDisruptedFlights\": scenario_wide_actual_disrupted_flights,\n",
    "                    \"Reward_delay_penalty_total\": scenario_wide_reward_components[\"delay_penalty_total\"],\n",
    "                    \"Reward_cancel_penalty\": scenario_wide_reward_components[\"cancel_penalty\"],\n",
    "                    \"Reward_inaction_penalty\": scenario_wide_reward_components[\"inaction_penalty\"],\n",
    "                    \"Reward_proactive_bonus\": scenario_wide_reward_components[\"proactive_bonus\"],\n",
    "                    \"Reward_time_penalty\": scenario_wide_reward_components[\"time_penalty\"],\n",
    "                    \"Reward_final_conflict_resolution_reward\": scenario_wide_reward_components[\"final_conflict_resolution_reward\"]\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "ac = \"20\"\n",
    "\n",
    "# Define models and seeds \n",
    "model_paths = [\n",
    "    # (\"../22-run/6ac-10000-deterministic/myopic_2023.zip\", \"myopic\"),\n",
    "    # (\"../22-run/6ac-10000-deterministic/proactive_2023.zip\", \"proactive\"),\n",
    "    # (\"../22-run/6ac-10000-deterministic/reactive_2023.zip\", \"reactive\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1023/myopic-89.zip\", \"myopic\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1023/proactive-93.zip\", \"proactive\"), \n",
    "    # # (\"../trained_models/dqn/6ac-700-diverse/1023/reactive-96.zip\", \"reactive\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1024/myopic-90.zip\", \"myopic\"),\n",
    "    # # (\"../trained_models/dqn/6ac-700-diverse/1024/proactive-92.zip\", \"proactive\"), \n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1024/reactive-95.zip\", \"reactive\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1025/myopic-91.zip\", \"myopic\"),\n",
    "    #     # (\"../trained_models/dqn/6ac-700-diverse/1023/myopic-89.zip\", \"myopic\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1023/proactive-93.zip\", \"proactive\"), \n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1023/reactive-96.zip\", \"reactive\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1024/myopic-90.zip\", \"myopic\"),\n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1024/proactive-92.zip\", \"proactive\"), \n",
    "    # (\"../trained_models/dqn/6ac-700-diverse/1024/reactive-95.zip\", \"reactive\"), \n",
    "    (f\"../1-bbbb-multi-ac-6ac-run/{ac}ac-10-multi-ac-deterministic/myopic_2023.zip\", \"myopic\"),\n",
    "    (f\"../1-bbbb-multi-ac-6ac-run/{ac}ac-10-multi-ac-deterministic/proactive_2023.zip\", \"proactive\"),\n",
    "    (f\"../1-bbbb-multi-ac-6ac-run/{ac}ac-10-multi-ac-deterministic/reactive_2023.zip\", \"reactive\"),\n",
    "    (\"exact\", \"exact\")\n",
    "]\n",
    "\n",
    "# seeds = [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
    "seeds = []\n",
    "for i in range(1, 2):\n",
    "    seeds.append(i)\n",
    "\n",
    "data_folder = f\"../data/TEMPOOOOOO/{ac}ac-10-multi-ac-deterministic/\"\n",
    "\n",
    "print(seeds)\n",
    "# approx 2,5min per seed with 4 models\n",
    "\n",
    "results_df = run_inference_on_data_folder(model_paths, data_folder, seeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "<b>DONE: </b>MERGED DATASET\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(results_df.columns)\n",
    "# Assuming results_df is already defined and contains the necessary data\n",
    "# Create a new DataFrame to store the runtimes and runtime per step\n",
    "runtimes_df = pd.DataFrame(columns=['ac', 'model', 'runtime', 'runtime_per_step'])\n",
    "\n",
    "\n",
    "\n",
    "# Get unique models from results_df\n",
    "unique_models = results_df['Model'].unique()\n",
    "\n",
    "# For each unique model, add two rows to runtimes_df with the runtime and runtime per step\n",
    "for model in unique_models:\n",
    "    # Assuming 'ScenarioTime' represents the runtime and 'ScenarioSteps' represents the number of steps\n",
    "    runtime = results_df[results_df['Model'] == model]['ScenarioTime'].mean()\n",
    "    scenario_steps = results_df[results_df['Model'] == model]['ScenarioSteps'].mean()\n",
    "    runtime_per_step = runtime / scenario_steps if scenario_steps > 0 else 0  # Avoid division by zero\n",
    "    new_row = {'ac': ac, 'model': model, 'runtime': runtime, 'runtime_per_step': runtime_per_step}\n",
    "    runtimes_df = pd.concat([runtimes_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "import os\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists('runtimes_abc.csv'):\n",
    "    # If not, create a new file\n",
    "    runtimes_df.to_csv('runtimes_abc.csv', index=False)\n",
    "else:\n",
    "    # If the file exists, append new rows\n",
    "    runtimes_df.to_csv('runtimes_abc.csv', mode='a', index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
