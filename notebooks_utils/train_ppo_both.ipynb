{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "CUDA available: False\n",
      "Number of GPUs available: 0\n",
      "cuDNN enabled: True\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Device info: {'device_type': 'MacBook M1'}\n",
      "Training folders: ['Scenario_03', 'Scenario_04', 'Scenario_05', 'Scenario_02', 'Scenario_10', 'Scenario_09', 'Scenario_07', 'Scenario_01', 'Scenario_06', 'Scenario_08']\n",
      "Training on 500 days of data (50 episodes of 10 scenarios)\n",
      "Getting model version for 3ac-10\n",
      "Getting model version for 3ac-10\n",
      "Models will be saved to:\n",
      "   ../trained_models/ppo/myopic_3ac-10-500-1.zip\n",
      "   ../trained_models/ppo/proactive_3ac-10-500-1.zip\n",
      "Results directory created at: ../results/ppo/20241205-20-19\n",
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to /var/folders/m6/gwyqzldd12bg_s3mrl40tp6r0000gn/T/SB3-2024-12-05-20-19-47-320085\n",
      "myopic: Episode 1/50 started.\n",
      "Type of processed state: <class 'dict'>\n",
      "{'state': array([ 0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,\n",
      "        0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,\n",
      "        0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,  0.00e+00,  1.02e+03,\n",
      "        0.00e+00,  1.02e+03, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,  1.00e+00, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02,  1.00e+00,  7.50e+01,  2.44e+02,  2.00e+00,\n",
      "        2.89e+02,  3.58e+02,  3.00e+00,  3.78e+02,  5.10e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02,  2.00e+00, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "        4.00e+00,  1.65e+02,  2.83e+02,  5.00e+00,  4.49e+02,  6.18e+02,\n",
      "        6.00e+00,  7.45e+02,  9.50e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "        3.00e+00, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02,\n",
      "       -9.99e+02, -9.99e+02, -9.99e+02, -9.99e+02]), 'action_mask': array([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)}\n",
      "*** Mask: [array([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]\n",
      "*** Predicted Action: [17]\n",
      "*** Mask: [array([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]\n",
      "*** Predicted Action: [8]\n",
      "*** Mask: [array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]\n",
      "*** Predicted Action: [12]\n",
      "*** Mask: [array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]\n",
      "*** Predicted Action: [20]\n",
      "*** Mask: [array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]\n",
      "*** Predicted Action: [17]\n",
      "Type of processed state: <class 'dict'>\n",
      "{'state': array([ 0.000e+00,  9.900e+02,  0.000e+00,  9.900e+02,  0.000e+00,\n",
      "        9.900e+02,  0.000e+00,  9.900e+02,  0.000e+00,  9.900e+02,\n",
      "        0.000e+00,  9.900e+02,  0.000e+00,  9.900e+02,  0.000e+00,\n",
      "        9.900e+02,  0.000e+00,  9.900e+02,  0.000e+00,  9.900e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        1.000e+00, -9.990e+02, -9.990e+02, -9.990e+02,  1.000e+00,\n",
      "        1.500e+02,  3.460e+02,  2.000e+00,  3.960e+02,  4.640e+02,\n",
      "        3.000e+00,  5.010e+02,  7.680e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        2.000e+00, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        3.000e+00, -9.990e+02, -9.990e+02, -9.990e+02,  8.000e+00,\n",
      "        1.500e+02,  2.350e+02,  9.000e+00,  3.150e+02,  4.490e+02,\n",
      "        1.000e+01,  4.670e+02,  6.310e+02,  1.100e+01,  7.680e+02,\n",
      "        1.067e+03, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02]), 'action_mask': array([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0], dtype=uint8)}\n",
      "Episode 1: Reward: [-6544.901]\n",
      "Type of processed state: <class 'dict'>\n",
      "{'state': array([ 0.000e+00,  1.050e+03,  0.000e+00,  1.050e+03,  0.000e+00,\n",
      "        1.050e+03,  0.000e+00,  1.050e+03,  0.000e+00,  1.050e+03,\n",
      "        0.000e+00,  1.050e+03,  0.000e+00,  1.050e+03,  0.000e+00,\n",
      "        1.050e+03,  0.000e+00,  1.050e+03,  0.000e+00,  1.050e+03,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        1.000e+00, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        2.000e+00, -9.990e+02, -9.990e+02, -9.990e+02,  5.000e+00,\n",
      "        7.500e+01,  3.450e+02,  6.000e+00,  4.400e+02,  6.320e+02,\n",
      "        7.000e+00,  7.040e+02,  8.770e+02,  8.000e+00,  1.055e+03,\n",
      "        1.120e+03, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "        3.000e+00, -9.990e+02, -9.990e+02, -9.990e+02,  9.000e+00,\n",
      "        1.050e+02,  1.880e+02,  1.000e+01,  2.160e+02,  5.080e+02,\n",
      "        1.100e+01,  6.110e+02,  7.280e+02,  1.200e+01,  8.900e+02,\n",
      "        1.048e+03, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02,\n",
      "       -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02, -9.990e+02]), 'action_mask': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pieterbecking/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/sb3_contrib/common/maskable/distributions.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  self.masks = th.as_tensor(masks, dtype=th.bool, device=device).reshape(self.logits.shape)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rewards, total_timesteps\n\u001b[1;32m    198\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 200\u001b[0m results_myopic \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m results_proactive \u001b[38;5;241m=\u001b[39m train_ppo_agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproactive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    203\u001b[0m rewards_myopic, total_timesteps_myopic \u001b[38;5;241m=\u001b[39m results_myopic\n",
      "Cell \u001b[0;32mIn[1], line 187\u001b[0m, in \u001b[0;36mtrain_ppo_agent\u001b[0;34m(env_type)\u001b[0m\n\u001b[1;32m    184\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps_per_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m total_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timesteps_per_episode\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_EPISODES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:442\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfMaskablePPO,\n\u001b[1;32m    432\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    439\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfMaskablePPO:\n\u001b[1;32m    440\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 442\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:78\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds[env_idx], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmaybe_options)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_seeds()\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:110\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scripts.utils import (\n",
    "    load_scenario_data,\n",
    "    verify_training_folders,\n",
    "    create_results_directory,\n",
    "    get_model_version,\n",
    "    format_days,\n",
    "    calculate_training_days,\n",
    "    initialize_device,\n",
    "    check_device_capabilities,\n",
    "    get_device_info,\n",
    ")\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "\n",
    "# Constants and Training Settings\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 50000 * 2\n",
    "BATCH_SIZE = 64 * 4\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "MAX_TIMESTEPS = 50000\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256 * 2, 256])\n",
    "LEARNING_STARTS = 0\n",
    "TRAIN_FREQ = 4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.025\n",
    "PERCENTAGE_MIN = 95\n",
    "N_EPISODES = 50\n",
    "TRAINING_FOLDERS_PATH = \"../data/Training/high-prob/3ac-10/\"\n",
    "TRAINING_FOLDERS_PATH = \"../data/Training/high-prob/3ac-10/\"\n",
    "\n",
    "# Initialize device\n",
    "device = initialize_device()\n",
    "check_device_capabilities()\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "# Verify training folders and gather training data\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "print(f\"Training folders: {training_folders}\")\n",
    "\n",
    "# Calculate training days and model naming\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "      f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "last_folder = os.path.basename(os.path.normpath(TRAINING_FOLDERS_PATH))\n",
    "model_name = last_folder\n",
    "model_version_myopic = get_model_version(model_name, myopic_proactive=\"myopic\", drl_type=\"ppo\")\n",
    "model_version_proactive = get_model_version(model_name, myopic_proactive=\"proactive\", drl_type=\"ppo\")\n",
    "MODEL_SAVE_PATH = f'../trained_models/ppo/'\n",
    "MODEL_SAVE_NAME_MYOPIC = f'{model_name}-{formatted_days}-{model_version_myopic}.zip'\n",
    "MODEL_SAVE_NAME_PROACTIVE = f'{model_name}-{formatted_days}-{model_version_proactive}.zip'\n",
    "print(f\"Models will be saved to:\")\n",
    "print(f\"   {MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\")\n",
    "print(f\"   {MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = create_results_directory(append_to_name='ppo')\n",
    "print(f\"Results directory created at: {results_dir}\")\n",
    "\n",
    "def my_get_action_masks(env):\n",
    "    mask = [e.get_action_mask() for e in env.envs]\n",
    "    print(\"*** Mask:\", mask)\n",
    "    return mask\n",
    "\n",
    "def train_ppo_agent(env_type):\n",
    "    rewards = []\n",
    "    total_timesteps = 0\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    class ScenarioEnvWrapper(gym.Env):\n",
    "        def __init__(self, scenario_folders, env_type):\n",
    "            super(ScenarioEnvWrapper, self).__init__()\n",
    "            self.scenario_folders = scenario_folders\n",
    "            self.env_type = env_type\n",
    "            self.current_scenario_idx = -1\n",
    "            self.load_next_scenario()\n",
    "\n",
    "            self.observation_space = self.env.observation_space\n",
    "            self.action_space = self.env.action_space\n",
    "\n",
    "        def load_next_scenario(self):\n",
    "            self.current_scenario_idx = (self.current_scenario_idx + 1) % len(self.scenario_folders)\n",
    "            scenario_folder = self.scenario_folders[self.current_scenario_idx]\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            self.env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=self.env_type\n",
    "            )\n",
    "\n",
    "        def reset(self, seed=None, options=None):\n",
    "            self.load_next_scenario()\n",
    "            result = self.env.reset()\n",
    "            return result\n",
    "\n",
    "        def step(self, action):\n",
    "            return self.env.step(action)\n",
    "\n",
    "        def render(self, mode='human'):\n",
    "            return self.env.render(mode=mode)\n",
    "\n",
    "        def close(self):\n",
    "            return self.env.close()\n",
    "\n",
    "        def get_action_mask(self):\n",
    "            return self.env.get_action_mask()\n",
    "\n",
    "    env = ScenarioEnvWrapper(scenario_folders, env_type)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = ActionMasker(env, my_get_action_masks)\n",
    "\n",
    "    model = MaskablePPO(\n",
    "        'MultiInputPolicy',\n",
    "        env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./ppo_aircraft_tensorboard_{env_type}/\",\n",
    "        device=device,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    timesteps_per_episode = MAX_TIMESTEPS // N_EPISODES\n",
    "\n",
    "    for episode in range(N_EPISODES):\n",
    "        print(f\"{env_type}: Episode {episode + 1}/{N_EPISODES} started.\")\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = []\n",
    "\n",
    "        while not done:\n",
    "            action_masks = env.action_masks()\n",
    "\n",
    "            # Pass the masks to `model.predict()` so that the model only chooses valid actions.\n",
    "            action, _states = model.predict(obs, deterministic=False, action_masks=action_masks)\n",
    "            print(f\"*** Predicted Action: {action}\")\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "        # Extract the values from the lists\n",
    "        reward = reward[0]\n",
    "        done = done[0]\n",
    "        info = info[0]\n",
    "\n",
    "        total_reward = sum(episode_rewards)\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward: {total_reward}\")\n",
    "\n",
    "        model.learn(total_timesteps=timesteps_per_episode, reset_num_timesteps=False)\n",
    "        total_timesteps += timesteps_per_episode\n",
    "\n",
    "        print(f\"{env_type}: Episode {episode + 1}/{N_EPISODES} completed.\")\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"{MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\")\n",
    "    else:\n",
    "        model.save(f\"{MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "    return rewards, total_timesteps\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "results_myopic = train_ppo_agent('myopic')\n",
    "results_proactive = train_ppo_agent('proactive')\n",
    "\n",
    "rewards_myopic, total_timesteps_myopic = results_myopic\n",
    "rewards_proactive, total_timesteps_proactive = results_proactive\n",
    "\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = (end_time - start_time).total_seconds()\n",
    "print(f\"Total training time: {runtime} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Save the myopic rewards\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "\n",
    "# Save the proactive rewards\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "runtime_in_seconds = runtime.total_seconds()\n",
    "print(f\"Total training time: {runtime_in_seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "\n",
    "# Create a dictionary of all hyperparameters and system information\n",
    "hyperparameters = {\n",
    "    \"Parameter\": [\n",
    "        \"MODEL_TYPE\",\n",
    "        \"LEARNING_RATE\", \"GAMMA\", \"BUFFER_SIZE\", \"BATCH_SIZE\",\n",
    "        \"TARGET_UPDATE_INTERVAL\", \"MAX_TIMESTEPS\", \"LEARNING_STARTS\",\n",
    "        \"TRAIN_FREQ\", \"N_EPISODES\", \"NEURAL_NET_STRUCTURE\",\n",
    "        \"TRAINING_FOLDERS_PATH\", \"TESTING_FOLDERS_PATH\", \"MODEL_SAVE_PATH\",\n",
    "        \"MODEL_SAVE_NAME\", \"runtime_in_seconds\", \"runtime_in_hh:mm:ss\",\n",
    "        \"total_timesteps_myopic\", \"total_timesteps_proactive\", \"device\",\n",
    "        \"device_info\", \"MAX_AIRCRAFT\", \"MAX_FLIGHTS_PER_AIRCRAFT\",\n",
    "        \"TIMESTEP_HOURS\", \"DUMMY_VALUE\", \"RESOLVED_CONFLICT_REWARD\",\n",
    "        \"DELAY_MINUTE_PENALTY\", \"MAX_DELAY_PENALTY\", \"NO_ACTION_PENALTY\",\n",
    "        \"CANCELLED_FLIGHT_PENALTY\", \"MIN_TURN_TIME\", \"CROSS_VAL_INTERVAL\",\n",
    "        \"PERCENTAGE_MIN\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        \"PPO\", LEARNING_RATE, GAMMA, BUFFER_SIZE, BATCH_SIZE,\n",
    "        TARGET_UPDATE_INTERVAL, MAX_TIMESTEPS, LEARNING_STARTS,\n",
    "        TRAIN_FREQ, N_EPISODES, str(NEURAL_NET_STRUCTURE),\n",
    "        TRAINING_FOLDERS_PATH, TESTING_FOLDERS_PATH, MODEL_SAVE_PATH,\n",
    "        MODEL_SAVE_NAME, runtime_in_seconds, str(runtime),\n",
    "        total_timesteps_myopic, total_timesteps_proactive, device,\n",
    "        str(device_info), MAX_AIRCRAFT, MAX_FLIGHTS_PER_AIRCRAFT,\n",
    "        TIMESTEP_HOURS, DUMMY_VALUE, RESOLVED_CONFLICT_REWARD,\n",
    "        DELAY_MINUTE_PENALTY, MAX_DELAY_PENALTY, NO_ACTION_PENALTY,\n",
    "        CANCELLED_FLIGHT_PENALTY, MIN_TURN_TIME, CROSS_VAL_INTERVAL,\n",
    "        PERCENTAGE_MIN\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "hyperparameters_df = pd.DataFrame(hyperparameters)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(results_dir, \"hyperparameters.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "hyperparameters_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"All hyperparameters saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training rewards per episode for myopic\n",
    "print(rewards_myopic)\n",
    "episodes_myopic = [ep for ep, _ in rewards_myopic]\n",
    "avg_rewards_per_episode_myopic = [reward for _, reward in rewards_myopic]\n",
    "\n",
    "# Process training rewards per episode for proactive\n",
    "episodes_proactive = [ep for ep, _ in rewards_proactive]\n",
    "avg_rewards_per_episode_proactive = [reward for _, reward in rewards_proactive]\n",
    "\n",
    "# Extract test rewards for myopic\n",
    "# test_episodes_myopic = [ep for ep, _ in test_rewards_myopic]\n",
    "# test_avg_rewards_myopic = [reward for _, reward in test_rewards_myopic]\n",
    "\n",
    "# # Extract test rewards for proactive\n",
    "# test_episodes_proactive = [ep for ep, _ in test_rewards_proactive]\n",
    "# test_avg_rewards_proactive = [reward for _, reward in test_rewards_proactive]\n",
    "\n",
    "# Plot the average rewards over the episodes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training rewards\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "\n",
    "\n",
    "# plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "# plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_episode.png'))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
