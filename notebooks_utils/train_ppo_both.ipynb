{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "CUDA available: False\n",
      "Number of GPUs available: 0\n",
      "cuDNN enabled: True\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Device info: {'device_type': 'MacBook M1'}\n",
      "Training folders: ['Scenario_03', 'Scenario_04', 'Scenario_05', 'Scenario_02', 'Scenario_10', 'Scenario_09', 'Scenario_07', 'Scenario_01', 'Scenario_06', 'Scenario_08']\n",
      "Training on 500 days of data (50 episodes of 10 scenarios)\n",
      "Getting model version for 3ac-10\n",
      "Getting model version for 3ac-10\n",
      "Models will be saved to:\n",
      "   ../trained_models/ppo/myopic_3ac-10-500-1.zip\n",
      "   ../trained_models/ppo/proactive_3ac-10-500-1.zip\n",
      "Results directory created at: ../results/ppo/20241205-20-40\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rewards, total_timesteps\n\u001b[1;32m    198\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 200\u001b[0m results_myopic \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmyopic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m results_proactive \u001b[38;5;241m=\u001b[39m train_ppo_agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproactive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    203\u001b[0m rewards_myopic, total_timesteps_myopic \u001b[38;5;241m=\u001b[39m results_myopic\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mtrain_ppo_agent\u001b[0;34m(env_type)\u001b[0m\n\u001b[1;32m    143\u001b[0m env \u001b[38;5;241m=\u001b[39m ScenarioEnvWrapper(scenario_folders, env_type)\n\u001b[1;32m    144\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env])\n\u001b[0;32m--> 145\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mActionMasker\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_get_action_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mNEURAL_NET_STRUCTURE\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    158\u001b[0m logger \u001b[38;5;241m=\u001b[39m configure()\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/sb3_contrib/common/wrappers/action_masker.py:20\u001b[0m, in \u001b[0;36mActionMasker.__init__\u001b[0;34m(self, env, action_mask_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: gym\u001b[38;5;241m.\u001b[39mEnv, action_mask_fn: Union[\u001b[38;5;28mstr\u001b[39m, Callable[[gym\u001b[38;5;241m.\u001b[39mEnv], np\u001b[38;5;241m.\u001b[39mndarray]]):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_mask_fn, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     23\u001b[0m         found_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action_mask_fn)\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/gymnasium/core.py:310\u001b[0m, in \u001b[0;36mWrapper.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wraps an environment to allow a modular transformation of the :meth:`step` and :meth:`reset` methods.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    env: The environment to wrap\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, Env)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space: spaces\u001b[38;5;241m.\u001b[39mSpace[WrapperActType] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation_space: spaces\u001b[38;5;241m.\u001b[39mSpace[WrapperObsType] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scripts.utils import (\n",
    "    load_scenario_data,\n",
    "    verify_training_folders,\n",
    "    create_results_directory,\n",
    "    get_model_version,\n",
    "    format_days,\n",
    "    calculate_training_days,\n",
    "    initialize_device,\n",
    "    check_device_capabilities,\n",
    "    get_device_info,\n",
    ")\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from src.environment_ppo import AircraftDisruptionEnv\n",
    "\n",
    "# Constants and Training Settings\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 50000 * 2\n",
    "BATCH_SIZE = 64 * 4\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "MAX_TIMESTEPS = 50000\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256 * 2, 256])\n",
    "LEARNING_STARTS = 0\n",
    "TRAIN_FREQ = 4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.025\n",
    "PERCENTAGE_MIN = 95\n",
    "N_EPISODES = 50\n",
    "TRAINING_FOLDERS_PATH = \"../data/Training/high-prob/3ac-10/\"\n",
    "TRAINING_FOLDERS_PATH = \"../data/Training/high-prob/3ac-10/\"\n",
    "\n",
    "# Initialize device\n",
    "device = initialize_device()\n",
    "check_device_capabilities()\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "# Verify training folders and gather training data\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "print(f\"Training folders: {training_folders}\")\n",
    "\n",
    "# Calculate training days and model naming\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "      f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "last_folder = os.path.basename(os.path.normpath(TRAINING_FOLDERS_PATH))\n",
    "model_name = last_folder\n",
    "model_version_myopic = get_model_version(model_name, myopic_proactive=\"myopic\", drl_type=\"ppo\")\n",
    "model_version_proactive = get_model_version(model_name, myopic_proactive=\"proactive\", drl_type=\"ppo\")\n",
    "MODEL_SAVE_PATH = f'../trained_models/ppo/'\n",
    "MODEL_SAVE_NAME_MYOPIC = f'{model_name}-{formatted_days}-{model_version_myopic}.zip'\n",
    "MODEL_SAVE_NAME_PROACTIVE = f'{model_name}-{formatted_days}-{model_version_proactive}.zip'\n",
    "print(f\"Models will be saved to:\")\n",
    "print(f\"   {MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\")\n",
    "print(f\"   {MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = create_results_directory(append_to_name='ppo')\n",
    "print(f\"Results directory created at: {results_dir}\")\n",
    "\n",
    "def my_get_action_masks(env):\n",
    "    mask = [e.get_action_mask() for e in env.envs]\n",
    "    print(\"*** Mask:\", mask)\n",
    "    return mask\n",
    "\n",
    "def train_ppo_agent(env_type):\n",
    "    rewards = []\n",
    "    total_timesteps = 0\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    class ScenarioEnvWrapper(gym.Env):\n",
    "        def __init__(self, scenario_folders, env_type):\n",
    "            super(ScenarioEnvWrapper, self).__init__()\n",
    "            self.scenario_folders = scenario_folders\n",
    "            self.env_type = env_type\n",
    "            self.current_scenario_idx = -1\n",
    "            self.load_next_scenario()\n",
    "\n",
    "            self.observation_space = self.env.observation_space\n",
    "            self.action_space = self.env.action_space\n",
    "\n",
    "        def load_next_scenario(self):\n",
    "            self.current_scenario_idx = (self.current_scenario_idx + 1) % len(self.scenario_folders)\n",
    "            scenario_folder = self.scenario_folders[self.current_scenario_idx]\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            self.env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=self.env_type\n",
    "            )\n",
    "\n",
    "        def reset(self, seed=None, options=None):\n",
    "            self.load_next_scenario()\n",
    "            obs, info = self.env.reset()  # Make sure self.env.reset() returns (obs, info)\n",
    "            return obs, info\n",
    "\n",
    "        def step(self, action):\n",
    "            return self.env.step(action)\n",
    "\n",
    "        def render(self, mode='human'):\n",
    "            return self.env.render(mode=mode)\n",
    "\n",
    "        def close(self):\n",
    "            return self.env.close()\n",
    "\n",
    "        def get_action_mask(self):\n",
    "            return self.env.get_action_mask()\n",
    "\n",
    "    env = ScenarioEnvWrapper(scenario_folders, env_type)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = ActionMasker(env, my_get_action_masks)\n",
    "\n",
    "    model = MaskablePPO(\n",
    "        'MultiInputPolicy',\n",
    "        env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./ppo_aircraft_tensorboard_{env_type}/\",\n",
    "        device=device,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    timesteps_per_episode = MAX_TIMESTEPS // N_EPISODES\n",
    "\n",
    "    for episode in range(N_EPISODES):\n",
    "        print(f\"{env_type}: Episode {episode + 1}/{N_EPISODES} started.\")\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = []\n",
    "\n",
    "        while not done:\n",
    "            action_masks = env.action_masks()\n",
    "\n",
    "            # Pass the masks to `model.predict()` so that the model only chooses valid actions.\n",
    "            action, _states = model.predict(obs, deterministic=False, action_masks=action_masks)\n",
    "            print(f\"*** Predicted Action: {action}\")\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "        # Extract the values from the lists\n",
    "        reward = reward[0]\n",
    "        done = done[0]\n",
    "        info = info[0]\n",
    "\n",
    "        total_reward = sum(episode_rewards)\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward: {total_reward}\")\n",
    "\n",
    "        model.learn(total_timesteps=timesteps_per_episode, reset_num_timesteps=False)\n",
    "        total_timesteps += timesteps_per_episode\n",
    "\n",
    "        print(f\"{env_type}: Episode {episode + 1}/{N_EPISODES} completed.\")\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"{MODEL_SAVE_PATH}myopic_{MODEL_SAVE_NAME_MYOPIC}\")\n",
    "    else:\n",
    "        model.save(f\"{MODEL_SAVE_PATH}proactive_{MODEL_SAVE_NAME_PROACTIVE}\")\n",
    "    return rewards, total_timesteps\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "results_myopic = train_ppo_agent('myopic')\n",
    "results_proactive = train_ppo_agent('proactive')\n",
    "\n",
    "rewards_myopic, total_timesteps_myopic = results_myopic\n",
    "rewards_proactive, total_timesteps_proactive = results_proactive\n",
    "\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = (end_time - start_time).total_seconds()\n",
    "print(f\"Total training time: {runtime} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Save the myopic rewards\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "\n",
    "# Save the proactive rewards\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "runtime_in_seconds = runtime.total_seconds()\n",
    "print(f\"Total training time: {runtime_in_seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "\n",
    "# Create a dictionary of all hyperparameters and system information\n",
    "hyperparameters = {\n",
    "    \"Parameter\": [\n",
    "        \"MODEL_TYPE\",\n",
    "        \"LEARNING_RATE\", \"GAMMA\", \"BUFFER_SIZE\", \"BATCH_SIZE\",\n",
    "        \"TARGET_UPDATE_INTERVAL\", \"MAX_TIMESTEPS\", \"LEARNING_STARTS\",\n",
    "        \"TRAIN_FREQ\", \"N_EPISODES\", \"NEURAL_NET_STRUCTURE\",\n",
    "        \"TRAINING_FOLDERS_PATH\", \"TESTING_FOLDERS_PATH\", \"MODEL_SAVE_PATH\",\n",
    "        \"MODEL_SAVE_NAME\", \"runtime_in_seconds\", \"runtime_in_hh:mm:ss\",\n",
    "        \"total_timesteps_myopic\", \"total_timesteps_proactive\", \"device\",\n",
    "        \"device_info\", \"MAX_AIRCRAFT\", \"MAX_FLIGHTS_PER_AIRCRAFT\",\n",
    "        \"TIMESTEP_HOURS\", \"DUMMY_VALUE\", \"RESOLVED_CONFLICT_REWARD\",\n",
    "        \"DELAY_MINUTE_PENALTY\", \"MAX_DELAY_PENALTY\", \"NO_ACTION_PENALTY\",\n",
    "        \"CANCELLED_FLIGHT_PENALTY\", \"MIN_TURN_TIME\", \"CROSS_VAL_INTERVAL\",\n",
    "        \"PERCENTAGE_MIN\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        \"PPO\", LEARNING_RATE, GAMMA, BUFFER_SIZE, BATCH_SIZE,\n",
    "        TARGET_UPDATE_INTERVAL, MAX_TIMESTEPS, LEARNING_STARTS,\n",
    "        TRAIN_FREQ, N_EPISODES, str(NEURAL_NET_STRUCTURE),\n",
    "        TRAINING_FOLDERS_PATH, TESTING_FOLDERS_PATH, MODEL_SAVE_PATH,\n",
    "        MODEL_SAVE_NAME, runtime_in_seconds, str(runtime),\n",
    "        total_timesteps_myopic, total_timesteps_proactive, device,\n",
    "        str(device_info), MAX_AIRCRAFT, MAX_FLIGHTS_PER_AIRCRAFT,\n",
    "        TIMESTEP_HOURS, DUMMY_VALUE, RESOLVED_CONFLICT_REWARD,\n",
    "        DELAY_MINUTE_PENALTY, MAX_DELAY_PENALTY, NO_ACTION_PENALTY,\n",
    "        CANCELLED_FLIGHT_PENALTY, MIN_TURN_TIME, CROSS_VAL_INTERVAL,\n",
    "        PERCENTAGE_MIN\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "hyperparameters_df = pd.DataFrame(hyperparameters)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(results_dir, \"hyperparameters.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "hyperparameters_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"All hyperparameters saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training rewards per episode for myopic\n",
    "print(rewards_myopic)\n",
    "episodes_myopic = [ep for ep, _ in rewards_myopic]\n",
    "avg_rewards_per_episode_myopic = [reward for _, reward in rewards_myopic]\n",
    "\n",
    "# Process training rewards per episode for proactive\n",
    "episodes_proactive = [ep for ep, _ in rewards_proactive]\n",
    "avg_rewards_per_episode_proactive = [reward for _, reward in rewards_proactive]\n",
    "\n",
    "# Extract test rewards for myopic\n",
    "# test_episodes_myopic = [ep for ep, _ in test_rewards_myopic]\n",
    "# test_avg_rewards_myopic = [reward for _, reward in test_rewards_myopic]\n",
    "\n",
    "# # Extract test rewards for proactive\n",
    "# test_episodes_proactive = [ep for ep, _ in test_rewards_proactive]\n",
    "# test_avg_rewards_proactive = [reward for _, reward in test_rewards_proactive]\n",
    "\n",
    "# Plot the average rewards over the episodes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training rewards\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "\n",
    "\n",
    "# plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "# plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_episode.png'))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
