{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in scenario ../data/Testing/1k-3ac-12f-1dis-F/Scenario_201: No valid flights found in the scenario Scenario_201. Skipping this scenario.\n",
      "Results saved to: ../results/evaluations/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from scripts.utils import load_scenario_data\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize the environment\n",
    "def initialize_environment(data_dict):\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    # Check if there are any flights in the flights_dict for the current day\n",
    "    this_day_flights = [flight_info for flight_info in flights_dict.values() if '+' not in flight_info['DepTime']]\n",
    "    \n",
    "    if not this_day_flights:\n",
    "        # Handle empty flight list\n",
    "        raise ValueError(f\"No valid flights found in the scenario {data_dict['scenario_name']}. Skipping this scenario.\")\n",
    "    \n",
    "    # Initialize the environment\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict, \n",
    "        flights_dict, \n",
    "        rotations_dict, \n",
    "        alt_aircraft_dict, \n",
    "        config_dict\n",
    "    )\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Load the model and evaluate on all scenarios in the folder\n",
    "def evaluate_model_on_scenarios(model_path, scenarios_folder, output_csv):\n",
    "    # Verify that the model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    # Initialize a list to store rewards for all scenarios\n",
    "    results = []\n",
    "\n",
    "    # Load the trained model\n",
    "    model = DQN.load(model_path)\n",
    "\n",
    "    # Iterate over all scenario folders in the specified directory\n",
    "    scenario_folders = [os.path.join(scenarios_folder, folder) for folder in os.listdir(scenarios_folder)\n",
    "                        if os.path.isdir(os.path.join(scenarios_folder, folder))]\n",
    "\n",
    "    for scenario_folder in scenario_folders:\n",
    "        try:\n",
    "            # print(f\"Evaluating scenario: {scenario_folder}\")\n",
    "            \n",
    "            # Load the scenario data\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            data_dict['scenario_name'] = os.path.basename(scenario_folder)  # Add scenario name for error handling\n",
    "            \n",
    "            # Initialize the environment with validation for empty flights\n",
    "            env = initialize_environment(data_dict)\n",
    "\n",
    "            # Set the environment for the model\n",
    "            model.set_env(env)\n",
    "\n",
    "            # Reset the environment\n",
    "            obs, _ = env.reset()\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            step_num = 0\n",
    "            max_steps = 1000  # Set a maximum number of steps to prevent infinite loops\n",
    "\n",
    "            # Run the environment loop\n",
    "            while not done_flag and step_num < max_steps:\n",
    "                # Predict the action using the trained model\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # Accumulate the reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Combine terminated and truncated flags\n",
    "                done_flag = terminated or truncated\n",
    "                step_num += 1\n",
    "            \n",
    "            # Append the results (scenario name, total reward, and steps)\n",
    "            results.append({\n",
    "                \"scenario\": os.path.basename(scenario_folder),\n",
    "                \"total_reward\": total_reward,\n",
    "                \"steps\": step_num\n",
    "            })\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error in scenario {scenario_folder}: {ve}\")\n",
    "            continue\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Results saved to: {output_csv}\")\n",
    "\n",
    "# Specify the model path and scenarios folder\n",
    "MODEL_PATH = \"../trained_models/dqn_100000d_1000u-1.zip\"\n",
    "SCENARIOS_FOLDER = \"../data/Testing/1k-3ac-12f-1dis-F/\"\n",
    "OUTPUT_CSV = \"../results/evaluations/evaluation_results.csv\"\n",
    "\n",
    "# Run the evaluation on all scenarios\n",
    "evaluate_model_on_scenarios(MODEL_PATH, SCENARIOS_FOLDER, OUTPUT_CSV)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
