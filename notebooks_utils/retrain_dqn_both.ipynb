{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scripts.utils import *\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure  # Import the configure function\n",
    "from stable_baselines3.common.utils import polyak_update, set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Parameter': 'LEARNING_RATE', 'Value': '0.001'}\n",
      "{'Parameter': 'GAMMA', 'Value': '0.99'}\n",
      "{'Parameter': 'BUFFER_SIZE', 'Value': '400000'}\n",
      "{'Parameter': 'BATCH_SIZE', 'Value': '512'}\n",
      "{'Parameter': 'TARGET_UPDATE_INTERVAL', 'Value': '100'}\n",
      "{'Parameter': 'EPSILON_START', 'Value': '1.0'}\n",
      "{'Parameter': 'EPSILON_MIN', 'Value': '0.001'}\n",
      "{'Parameter': 'EPSILON_DECAY_RATE', 'Value': '0.0006049352201578192'}\n",
      "{'Parameter': 'MAX_TIMESTEPS', 'Value': '50000'}\n",
      "{'Parameter': 'LEARNING_STARTS', 'Value': '1000'}\n",
      "{'Parameter': 'TRAIN_FREQ', 'Value': '4'}\n",
      "{'Parameter': 'N_EPISODES', 'Value': '2000'}\n",
      "{'Parameter': 'NEURAL_NET_STRUCTURE', 'Value': \"{'net_arch': [256, 512, 512, 256]}\"}\n",
      "{'Parameter': 'TRAINING_FOLDERS_PATH', 'Value': '../data/Training/3ac-single-cleared/'}\n",
      "{'Parameter': 'TESTING_FOLDERS_PATH', 'Value': '../data/Training/3ac-single-cleared/'}\n",
      "{'Parameter': 'MODEL_SAVE_PATH', 'Value': '../trained_models/'}\n",
      "{'Parameter': 'myopic_model_name', 'Value': 'myopic_3ac'}\n",
      "{'Parameter': 'proactive_model_name', 'Value': 'proactive_3ac'}\n",
      "{'Parameter': 'runtime_in_seconds', 'Value': '60.869984'}\n",
      "{'Parameter': 'runtime_in_hh:mm:ss', 'Value': '0:01:00.869984'}\n",
      "{'Parameter': 'total_timesteps_myopic', 'Value': '8523'}\n",
      "{'Parameter': 'total_timesteps_proactive', 'Value': '8538'}\n",
      "{'Parameter': 'device', 'Value': 'mps'}\n",
      "{'Parameter': 'device_info', 'Value': \"{'device_type': 'MacBook M1'}\"}\n",
      "{'Parameter': 'MAX_AIRCRAFT', 'Value': '3'}\n",
      "{'Parameter': 'MAX_FLIGHTS_PER_AIRCRAFT', 'Value': '12'}\n",
      "{'Parameter': 'TIMESTEP_HOURS', 'Value': '1'}\n",
      "{'Parameter': 'DUMMY_VALUE', 'Value': '-999'}\n",
      "{'Parameter': 'RESOLVED_CONFLICT_REWARD', 'Value': '2000'}\n",
      "{'Parameter': 'DELAY_MINUTE_PENALTY', 'Value': '6'}\n",
      "{'Parameter': 'MAX_DELAY_PENALTY', 'Value': '750'}\n",
      "{'Parameter': 'NO_ACTION_PENALTY', 'Value': '0'}\n",
      "{'Parameter': 'CANCELLED_FLIGHT_PENALTY', 'Value': '1000'}\n",
      "{'Parameter': 'MIN_TURN_TIME', 'Value': '0'}\n",
      "{'Parameter': 'CROSS_VAL_INTERVAL', 'Value': '10'}\n",
      "{'Parameter': 'PERCENTAGE_MIN', 'Value': '95'}\n",
      "{'Parameter': 'wallclock_seconds_per_timestep_myopic', 'Value': '0.007141849583479996'}\n",
      "{'Parameter': 'wallclock_seconds_per_timestep_proactive', 'Value': '0.007129302412743031'}\n"
     ]
    }
   ],
   "source": [
    "# Extract hyperparams from csv file \n",
    "results_folder = \"../results/20241123-17-34/\"\n",
    "hyperparams_df = pd.read_csv(os.path.join(results_folder, \"hyperparameters.csv\"))\n",
    "\n",
    "# print(hyperparams_df)\n",
    "\n",
    "\n",
    "# Extract the hyperparameters from the dataframe\n",
    "hyperparams_dict = hyperparams_df.to_dict(orient=\"records\")[1]\n",
    "# print(hyperparams_dict)\n",
    "\n",
    "for i in hyperparams_df.to_dict(orient=\"records\"):\n",
    "    print(i)\n",
    "\n",
    "    # save them as variables\n",
    "    globals()[i[\"Parameter\"]] = i[\"Value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constants and Training Settings\n",
    "# LEARNING_RATE = 0.0001\n",
    "# GAMMA = 0.99\n",
    "# BUFFER_SIZE = 50000*2\n",
    "# BATCH_SIZE = 64*2\n",
    "# TARGET_UPDATE_INTERVAL = 50\n",
    "# MAX_TIMESTEPS = 50000\n",
    "\n",
    "# NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256 * 2, 256])\n",
    "# LEARNING_STARTS = 0\n",
    "# TRAIN_FREQ = 1\n",
    "\n",
    "# EPSILON_START = 1.0\n",
    "# EPSILON_MIN = 0.05\n",
    "# PERCENTAGE_MIN = 95\n",
    "\n",
    "# EPSILON_TYPE = \"exponential\"\n",
    "# # EPSILON_TYPE = \"linear\"\n",
    "\n",
    "\n",
    "# if EPSILON_TYPE == \"linear\":\n",
    "#     EPSILON_MIN = 0\n",
    "\n",
    "# N_EPISODES = 2500\n",
    "# CROSS_VAL_INTERVAL = 10\n",
    "\n",
    "# TRAINING_FOLDERS_PATH = \"../data/Training/3ac-single-cleared/\"\n",
    "# TESTING_FOLDERS_PATH = \"../data/Training/3ac-single-cleared/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESTIMATED VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate total timesteps 10 times and take the average\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m total_timesteps_list \u001b[38;5;241m=\u001b[39m [calculate_total_training_timesteps(TRAINING_FOLDERS_PATH, N_EPISODES) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m      8\u001b[0m estimated_total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(total_timesteps_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(total_timesteps_list)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated total timesteps: \u001b[39m\u001b[38;5;124m\"\u001b[39m, estimated_total_timesteps)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate total timesteps 10 times and take the average\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m total_timesteps_list \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_total_training_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAINING_FOLDERS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPISODES\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n\u001b[1;32m      8\u001b[0m estimated_total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(total_timesteps_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(total_timesteps_list)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated total timesteps: \u001b[39m\u001b[38;5;124m\"\u001b[39m, estimated_total_timesteps)\n",
      "File \u001b[0;32m~/Desktop/Boeing-ADM-DRL-Github/scripts/visualizations.py:657\u001b[0m, in \u001b[0;36mcalculate_total_training_timesteps\u001b[0;34m(training_folders_path, n_episodes)\u001b[0m\n\u001b[1;32m    655\u001b[0m average_time_per_scenario \u001b[38;5;241m=\u001b[39m batch_time \u001b[38;5;241m/\u001b[39m scenario_count\n\u001b[1;32m    656\u001b[0m total_timesteps_estimate \u001b[38;5;241m=\u001b[39m total_timesteps_per_batch \u001b[38;5;241m*\u001b[39m n_episodes\n\u001b[0;32m--> 657\u001b[0m estimated_training_time \u001b[38;5;241m=\u001b[39m \u001b[43maverage_time_per_timestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps_estimate\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Print timing information\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# print(f\"Estimated Total Training Time: {estimated_training_time / 3600:.2f} hours\")\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# print(f\"Estimated Total Training Time: {estimated_training_time / 60:.2f} minutes\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# print(\"\")\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# print(f\"Total Timesteps Estimate: {total_timesteps_estimate}\")\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_timesteps_estimate\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.environment import AircraftDisruptionEnv\n",
    "\n",
    "print(\"Calculating...\")\n",
    "# Calculate total timesteps 10 times and take the average\n",
    "total_timesteps_list = [calculate_total_training_timesteps(TRAINING_FOLDERS_PATH, N_EPISODES) for _ in range(100)]\n",
    "estimated_total_timesteps = sum(total_timesteps_list) / len(total_timesteps_list)\n",
    "\n",
    "print(\"Estimated total timesteps: \", estimated_total_timesteps)\n",
    "\n",
    "# Calculate the decay rate or linear rate\n",
    "EPSILON_DECAY_RATE = calculate_epsilon_decay_rate(\n",
    "    estimated_total_timesteps,\n",
    "    EPSILON_START,\n",
    "    EPSILON_MIN,\n",
    "    PERCENTAGE_MIN,\n",
    "    EPSILON_TYPE\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Testing...\")\n",
    "simulate_and_plot_epsilon_decay(\n",
    "    EPSILON_START, EPSILON_MIN, EPSILON_DECAY_RATE, estimated_total_timesteps, EPSILON_TYPE\n",
    ")\n",
    "\n",
    "print(\"EPSILON DECAY RATE: \", EPSILON_DECAY_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = initialize_device()\n",
    "\n",
    "# Check device capabilities\n",
    "check_device_capabilities()\n",
    "\n",
    "# Get device-specific information\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "# Verify training folders and gather training data\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "# print(f\"Training folders: {training_folders}\")\n",
    "\n",
    "# Calculate training days and model naming\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "        f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "MODEL_SAVE_PATH = f'../trained_models/'\n",
    "\n",
    "myopic_model_name = \"myopic_3ac\"\n",
    "proactive_model_name = \"proactive_3ac\"\n",
    "myopic_model_version = get_model_version(myopic_model_name)\n",
    "proactive_model_version = get_model_version(proactive_model_name)\n",
    "print(f\"Models will be saved to:\")\n",
    "print(f\"   {MODEL_SAVE_PATH}{myopic_model_name}-{myopic_model_version}.zip\")\n",
    "print(f\"   {MODEL_SAVE_PATH}{proactive_model_name}-{proactive_model_version}.zip\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = create_results_directory()\n",
    "print(f\"Results directory created at: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training both agents: myopic and proactive\n",
    "\n",
    "def train_dqn_agent(env_type):\n",
    "    # Initialize variables\n",
    "    rewards = []\n",
    "    test_rewards = []\n",
    "    epsilon_values = []\n",
    "    total_timesteps = 0  # Added to track total timesteps\n",
    "    consecutive_drops = 0  # Track consecutive performance drops\n",
    "    best_test_reward = float('-inf')  # Track best test performance\n",
    "    action_sequences = {\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder): {\n",
    "            \"best_actions\": [],\n",
    "            \"best_reward\": float('-inf'),\n",
    "            \"worst_actions\": [],\n",
    "            \"worst_reward\": float('inf')\n",
    "        }\n",
    "        for folder in training_folders\n",
    "    }\n",
    "    \n",
    "\n",
    "    def cross_validate_on_test_data(model, current_episode):\n",
    "        test_scenario_folders = [\n",
    "            os.path.join(TESTING_FOLDERS_PATH, folder)\n",
    "            for folder in os.listdir(TESTING_FOLDERS_PATH)\n",
    "            if os.path.isdir(os.path.join(TESTING_FOLDERS_PATH, folder))\n",
    "        ]\n",
    "        total_test_reward = 0\n",
    "        for test_scenario_folder in test_scenario_folders:\n",
    "            # Load data\n",
    "            data_dict = load_scenario_data(test_scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            # Update the environment with the new scenario (by reinitializing it)\n",
    "            from src.environment import AircraftDisruptionEnv\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "            # Evaluate the model on the test scenario without training\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            timesteps = 0\n",
    "\n",
    "            while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "                # Get the action mask from the environment\n",
    "                action_mask = obs['action_mask']\n",
    "\n",
    "                # Convert observation to float32\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "                # Preprocess observation and get Q-values\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "                # Apply the action mask (set invalid actions to -inf)\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "                # Select the action with the highest masked Q-value\n",
    "                action = np.argmax(masked_q_values)\n",
    "\n",
    "                # Take the selected action in the environment\n",
    "                result = env.step(action)\n",
    "\n",
    "                # Unpack the result\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "                done_flag = terminated or truncated\n",
    "\n",
    "                # Accumulate the reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the current observation\n",
    "                obs = obs_next\n",
    "\n",
    "                timesteps += 1\n",
    "\n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            total_test_reward += total_reward\n",
    "\n",
    "        # Compute average test reward\n",
    "        avg_test_reward = total_test_reward / len(test_scenario_folders)\n",
    "        test_rewards.append((current_episode, avg_test_reward))\n",
    "        print(f\"CV at episode {current_episode}: {avg_test_reward}\")\n",
    "        \n",
    "        return avg_test_reward  # Return the average test reward\n",
    "\n",
    "    # List all the scenario folders in Data/Training\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    total_timesteps = 0  # Reset total_timesteps for each agent\n",
    "\n",
    "    # Initialize the DQN\n",
    "    dummy_scenario_folder = scenario_folders[0]\n",
    "    data_dict = load_scenario_data(dummy_scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    from src.environment import AircraftDisruptionEnv\n",
    "\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict,\n",
    "        flights_dict,\n",
    "        rotations_dict,\n",
    "        alt_aircraft_dict,\n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        policy='MultiInputPolicy',\n",
    "        env=env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "        verbose=0,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    logger = configure()\n",
    "    model._logger = logger\n",
    "\n",
    "    # Training loop over the number of episodes\n",
    "    for episode in range(N_EPISODES):\n",
    "        # Cycle through all the scenario folders\n",
    "        for scenario_folder in scenario_folders:\n",
    "            # Load the data for the current scenario\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            # Update the environment with the new scenario (by reinitializing it)\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)  # Update the model's environment with the new instance\n",
    "\n",
    "            # Reset the environment\n",
    "            obs, _ = env.reset()  # Extract the observation (obs) and ignore the info (_)\n",
    "\n",
    "            done_flag = False\n",
    "            total_reward = 0\n",
    "            timesteps = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            while not done_flag and timesteps < MAX_TIMESTEPS:\n",
    "                model.exploration_rate = epsilon\n",
    "\n",
    "                # Get the action mask from the environment\n",
    "                action_mask = obs['action_mask']\n",
    "\n",
    "                # Convert observation to float32\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "\n",
    "                # Preprocess observation and get Q-values\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "\n",
    "                # Apply the action mask (set invalid actions to -inf)\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "\n",
    "                current_seed = int(time.time() * 1e9) % (2**32 - 1)\n",
    "                # print(current_seed)\n",
    "                np.random.seed(current_seed)\n",
    "                \n",
    "                # Select an action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    # Exploration: choose a random valid action\n",
    "                    valid_actions = np.where(action_mask == 1)[0]\n",
    "                    action = np.random.choice(valid_actions)\n",
    "                else:\n",
    "                    # Exploitation: choose the action with the highest masked Q-value\n",
    "                    action = np.argmax(masked_q_values)\n",
    "\n",
    "                # Take the selected action in the environment\n",
    "                result = env.step(action)\n",
    "\n",
    "                # Unpack the result (5 values)\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "\n",
    "                # Combine the terminated and truncated flags into a single done flag\n",
    "                done_flag = terminated or truncated\n",
    "\n",
    "                # Store the action\n",
    "                action_sequence.append(action)\n",
    "\n",
    "                # print(f\"Reward: {reward} for action: {env.map_index_to_action(action)} in scenario: {scenario_folder}\")\n",
    "\n",
    "                # Accumulate the reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Add the transition to the replay buffer\n",
    "                model.replay_buffer.add(\n",
    "                    obs=obs,\n",
    "                    next_obs=obs_next,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    done=done_flag,\n",
    "                    infos=[info]\n",
    "                )\n",
    "\n",
    "                # Update the current observation\n",
    "                obs = obs_next\n",
    "\n",
    "                # Update epsilon (exploration rate)\n",
    "                if EPSILON_TYPE == \"exponential\":\n",
    "                    epsilon = max(EPSILON_MIN, epsilon * (1 - EPSILON_DECAY_RATE))\n",
    "                elif EPSILON_TYPE == \"linear\":\n",
    "                    epsilon = max(EPSILON_MIN, epsilon - EPSILON_DECAY_RATE)\n",
    "                epsilon_values.append(epsilon)\n",
    "\n",
    "                timesteps += 1\n",
    "                total_timesteps += 1  # Update total_timesteps\n",
    "\n",
    "                # Training\n",
    "                if total_timesteps > model.learning_starts and total_timesteps % TRAIN_FREQ == 0:\n",
    "                    # Perform a training step\n",
    "                    model.train(gradient_steps=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "                # Update target network\n",
    "                if total_timesteps % model.target_update_interval == 0:\n",
    "                    polyak_update(model.q_net.parameters(), model.q_net_target.parameters(), model.tau)\n",
    "                    # Copy batch norm stats\n",
    "                    polyak_update(model.batch_norm_stats, model.batch_norm_stats_target, 1.0)\n",
    "\n",
    "                # Check if the episode is done\n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            # Store the total reward for the episode with the scenario specified\n",
    "            rewards.append((episode, scenario_folder, total_reward))\n",
    "\n",
    "            # Update the worst and best action sequences\n",
    "            if total_reward < action_sequences[scenario_folder][\"worst_reward\"]:\n",
    "                action_sequences[scenario_folder][\"worst_actions\"] = action_sequence\n",
    "                action_sequences[scenario_folder][\"worst_reward\"] = total_reward\n",
    "\n",
    "            if total_reward > action_sequences[scenario_folder][\"best_reward\"]:\n",
    "                action_sequences[scenario_folder][\"best_actions\"] = action_sequence\n",
    "                action_sequences[scenario_folder][\"best_reward\"] = total_reward\n",
    "\n",
    "        # Perform cross-validation at specified intervals\n",
    "        if (episode + 1) % CROSS_VAL_INTERVAL == 0 or episode == 0:\n",
    "            current_test_reward = cross_validate_on_test_data(model, episode + 1)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if current_test_reward < best_test_reward:\n",
    "                consecutive_drops += 1\n",
    "                print(f\"Performance drop {consecutive_drops}/5 (current: {current_test_reward:.2f}, best: {best_test_reward:.2f})\")\n",
    "                if consecutive_drops >= 50000:\n",
    "                    print(f\"Early stopping triggered at episode {episode + 1} due to 5 consecutive drops in test performance\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_drops = 0\n",
    "                best_test_reward = current_test_reward\n",
    "\n",
    "        print(f\"{env_type}: {epsilon:.2f} ({episode + 1}/{N_EPISODES}) rewards: {total_reward}\")\n",
    "\n",
    "    # Save the model after training\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"{MODEL_SAVE_PATH}{myopic_model_name}-{myopic_model_version}.zip\")\n",
    "    else:\n",
    "        model.save(f\"{MODEL_SAVE_PATH}{proactive_model_name}-{proactive_model_version}.zip\")\n",
    "\n",
    "    # Return collected data\n",
    "    return rewards, test_rewards, total_timesteps, epsilon_values  # Added total_timesteps and epsilon_values to the return\n",
    "\n",
    "# Main code to train both agents and plot results\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train the myopic DQN agents\n",
    "results_myopic = train_dqn_agent('myopic')\n",
    "results_proactive = train_dqn_agent('proactive')\n",
    "\n",
    "# Unpack the results\n",
    "rewards_myopic, test_rewards_myopic, total_timesteps_myopic, epsilon_values_myopic = results_myopic\n",
    "rewards_proactive, test_rewards_proactive, total_timesteps_proactive, epsilon_values_proactive = results_proactive\n",
    "\n",
    "\n",
    "# Save the myopic rewards\n",
    "myopic_rewards_file = os.path.join(results_dir, \"rewards_myopic.pkl\")\n",
    "with open(myopic_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_myopic, file)\n",
    "print(f\"Myopic rewards saved to {myopic_rewards_file}\")\n",
    "\n",
    "# Save the proactive rewards\n",
    "proactive_rewards_file = os.path.join(results_dir, \"rewards_proactive.pkl\")\n",
    "with open(proactive_rewards_file, \"wb\") as file:\n",
    "    pickle.dump(rewards_proactive, file)\n",
    "print(f\"Proactive rewards saved to {proactive_rewards_file}\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "runtime_in_seconds = runtime.total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import *\n",
    "\n",
    "wallclock_seconds_per_timestep_myopic = runtime_in_seconds / total_timesteps_myopic\n",
    "wallclock_seconds_per_timestep_proactive = runtime_in_seconds / total_timesteps_proactive\n",
    "\n",
    "# Create a dictionary of all hyperparameters and system information\n",
    "hyperparameters = {\n",
    "    \"Parameter\": [\n",
    "        \"LEARNING_RATE\", \"GAMMA\", \"BUFFER_SIZE\", \"BATCH_SIZE\",\n",
    "        \"TARGET_UPDATE_INTERVAL\", \"EPSILON_START\", \"EPSILON_MIN\",\n",
    "        \"EPSILON_DECAY_RATE\", \"MAX_TIMESTEPS\", \"LEARNING_STARTS\",\n",
    "        \"TRAIN_FREQ\", \"N_EPISODES\", \"NEURAL_NET_STRUCTURE\",\n",
    "        \"TRAINING_FOLDERS_PATH\", \"TESTING_FOLDERS_PATH\", \"MODEL_SAVE_PATH\",\n",
    "        \"myopic_model_name\", \"proactive_model_name\", \"runtime_in_seconds\", \"runtime_in_hh:mm:ss\",\n",
    "        \"total_timesteps_myopic\", \"total_timesteps_proactive\", \"device\",\n",
    "        \"device_info\", \"MAX_AIRCRAFT\", \"MAX_FLIGHTS_PER_AIRCRAFT\",\n",
    "        \"TIMESTEP_HOURS\", \"DUMMY_VALUE\", \"RESOLVED_CONFLICT_REWARD\",\n",
    "        \"DELAY_MINUTE_PENALTY\", \"MAX_DELAY_PENALTY\", \"NO_ACTION_PENALTY\",\n",
    "        \"CANCELLED_FLIGHT_PENALTY\", \"MIN_TURN_TIME\", \"CROSS_VAL_INTERVAL\",\n",
    "        \"PERCENTAGE_MIN\", \"wallclock_seconds_per_timestep_myopic\", \"wallclock_seconds_per_timestep_proactive\",\n",
    "        \"EPSILON_TYPE\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        LEARNING_RATE, GAMMA, BUFFER_SIZE, BATCH_SIZE,\n",
    "        TARGET_UPDATE_INTERVAL, EPSILON_START, EPSILON_MIN,\n",
    "        EPSILON_DECAY_RATE, MAX_TIMESTEPS, LEARNING_STARTS,\n",
    "        TRAIN_FREQ, N_EPISODES, str(NEURAL_NET_STRUCTURE),\n",
    "        TRAINING_FOLDERS_PATH, TESTING_FOLDERS_PATH, MODEL_SAVE_PATH,\n",
    "        myopic_model_name, proactive_model_name, runtime_in_seconds, str(runtime),\n",
    "        total_timesteps_myopic, total_timesteps_proactive, device,\n",
    "        str(device_info), MAX_AIRCRAFT, MAX_FLIGHTS_PER_AIRCRAFT,\n",
    "        TIMESTEP_HOURS, DUMMY_VALUE, RESOLVED_CONFLICT_REWARD,\n",
    "        DELAY_MINUTE_PENALTY, MAX_DELAY_PENALTY, NO_ACTION_PENALTY,\n",
    "        CANCELLED_FLIGHT_PENALTY, MIN_TURN_TIME, CROSS_VAL_INTERVAL,\n",
    "        PERCENTAGE_MIN, wallclock_seconds_per_timestep_myopic, wallclock_seconds_per_timestep_proactive,\n",
    "        EPSILON_TYPE\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "hyperparameters_df = pd.DataFrame(hyperparameters)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(results_dir, \"hyperparameters.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "hyperparameters_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"All hyperparameters saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training rewards per episode for myopic\n",
    "episode_rewards_myopic = {}\n",
    "for episode, scenario_folder, total_reward in rewards_myopic:\n",
    "    if episode not in episode_rewards_myopic:\n",
    "        episode_rewards_myopic[episode] = []\n",
    "    episode_rewards_myopic[episode].append(total_reward)\n",
    "\n",
    "avg_rewards_per_episode_myopic = []\n",
    "episodes_myopic = []\n",
    "for episode in sorted(episode_rewards_myopic.keys()):\n",
    "    avg_reward = np.mean(episode_rewards_myopic[episode])\n",
    "    avg_rewards_per_episode_myopic.append(avg_reward)\n",
    "    episodes_myopic.append(episode + 1)  # episode numbers start from 0, so add 1\n",
    "\n",
    "# Process training rewards per episode for proactive\n",
    "episode_rewards_proactive = {}\n",
    "for episode, scenario_folder, total_reward in rewards_proactive:\n",
    "    if episode not in episode_rewards_proactive:\n",
    "        episode_rewards_proactive[episode] = []\n",
    "    episode_rewards_proactive[episode].append(total_reward)\n",
    "\n",
    "avg_rewards_per_episode_proactive = []\n",
    "episodes_proactive = []\n",
    "for episode in sorted(episode_rewards_proactive.keys()):\n",
    "    avg_reward = np.mean(episode_rewards_proactive[episode])\n",
    "    avg_rewards_per_episode_proactive.append(avg_reward)\n",
    "    episodes_proactive.append(episode + 1)  # episode numbers start from 0, so add 1\n",
    "\n",
    "# Extract test rewards for myopic\n",
    "test_episodes_myopic = [ep for ep, _ in test_rewards_myopic]\n",
    "test_avg_rewards_myopic = [reward for _, reward in test_rewards_myopic]\n",
    "\n",
    "# Extract test rewards for proactive\n",
    "test_episodes_proactive = [ep for ep, _ in test_rewards_proactive]\n",
    "test_avg_rewards_proactive = [reward for _, reward in test_rewards_proactive]\n",
    "\n",
    "# Plot the average rewards over the episodes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training rewards\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "\n",
    "\n",
    "plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_episode.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Process training rewards per episode for myopic\n",
    "episode_rewards_myopic = {}\n",
    "for episode, scenario_folder, total_reward in rewards_myopic:\n",
    "    if episode not in episode_rewards_myopic:\n",
    "        episode_rewards_myopic[episode] = []\n",
    "    episode_rewards_myopic[episode].append(total_reward)\n",
    "\n",
    "avg_rewards_per_episode_myopic = []\n",
    "episodes_myopic = []\n",
    "for episode in sorted(episode_rewards_myopic.keys()):\n",
    "    avg_reward = np.mean(episode_rewards_myopic[episode])\n",
    "    avg_rewards_per_episode_myopic.append(avg_reward)\n",
    "    episodes_myopic.append(episode + 1)  # episode numbers start from 0, so add 1\n",
    "\n",
    "# Process training rewards per episode for proactive\n",
    "episode_rewards_proactive = {}\n",
    "for episode, scenario_folder, total_reward in rewards_proactive:\n",
    "    if episode not in episode_rewards_proactive:\n",
    "        episode_rewards_proactive[episode] = []\n",
    "    episode_rewards_proactive[episode].append(total_reward)\n",
    "\n",
    "avg_rewards_per_episode_proactive = []\n",
    "episodes_proactive = []\n",
    "for episode in sorted(episode_rewards_proactive.keys()):\n",
    "    avg_reward = np.mean(episode_rewards_proactive[episode])\n",
    "    avg_rewards_per_episode_proactive.append(avg_reward)\n",
    "    episodes_proactive.append(episode + 1)  # episode numbers start from 0, so add 1\n",
    "\n",
    "# Extract test rewards for myopic\n",
    "test_episodes_myopic = [ep + 1 for ep, _ in test_rewards_myopic]  # episode numbers start from 0, so add 1\n",
    "test_avg_rewards_myopic = [reward for _, reward in test_rewards_myopic]\n",
    "\n",
    "# Extract test rewards for proactive\n",
    "test_episodes_proactive = [ep + 1 for ep, _ in test_rewards_proactive]  # episode numbers start from 0, so add 1\n",
    "test_avg_rewards_proactive = [reward for _, reward in test_rewards_proactive]\n",
    "\n",
    "# Plot 1: Myopic (Training and Testing)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Myopic Rewards')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'myopic_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Proactive (Training and Testing)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Proactive Rewards')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'proactive_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Training Rewards (Myopic and Proactive)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episodes_myopic, avg_rewards_per_episode_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive, avg_rewards_per_episode_proactive, label='Proactive Training Reward', color='C1')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Training Rewards (Myopic vs Proactive)')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'training_only.png'))\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Testing Rewards (Myopic and Proactive)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_episodes_myopic, test_avg_rewards_myopic, label='Myopic Test Reward', color='C0', linestyle='--')\n",
    "plt.plot(test_episodes_proactive, test_avg_rewards_proactive, label='Proactive Test Reward', color='C1', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Testing Rewards (Myopic vs Proactive)')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'testing_only.png'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# Smoothening the lines using a sliding average with a window size of 10\n",
    "window_size = 30\n",
    "smoothed_avg_rewards_myopic = np.convolve(avg_rewards_per_episode_myopic, np.ones(window_size)/window_size, mode='valid')\n",
    "smoothed_avg_rewards_proactive = np.convolve(avg_rewards_per_episode_proactive, np.ones(window_size)/window_size, mode='valid')\n",
    "plt.plot(episodes_myopic[:-window_size+1], smoothed_avg_rewards_myopic, label='Myopic Training Reward', color='C0')\n",
    "plt.plot(episodes_proactive[:-window_size+1], smoothed_avg_rewards_proactive, label='Proactive Training Reward', color='C1')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Training Rewards (Myopic vs Proactive)')\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'training_only_smoothed.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionaries to store rewards for each scenario\n",
    "scenario_rewards_myopic = defaultdict(list)\n",
    "scenario_rewards_proactive = defaultdict(list)\n",
    "\n",
    "# Iterate through the rewards list and group by scenario (last two characters) for myopic model\n",
    "for _, scenario, reward in rewards_myopic:\n",
    "    scenario_id = scenario[-2:]  # Get the last two characters (e.g., '01', '02', etc.)\n",
    "    scenario_rewards_myopic[scenario_id].append(reward)\n",
    "\n",
    "# Iterate through the rewards list and group by scenario (last two characters) for proactive model\n",
    "for _, scenario, reward in rewards_proactive:\n",
    "    scenario_id = scenario[-2:]  # Get the last two characters (e.g., '01', '02', etc.)\n",
    "    scenario_rewards_proactive[scenario_id].append(reward)\n",
    "\n",
    "# Calculate the average reward for each scenario for both models\n",
    "avg_rewards_per_scenario_myopic = {scenario: np.mean(rewards) for scenario, rewards in scenario_rewards_myopic.items()}\n",
    "avg_rewards_per_scenario_proactive = {scenario: np.mean(rewards) for scenario, rewards in scenario_rewards_proactive.items()}\n",
    "\n",
    "# Get all unique scenarios\n",
    "all_scenarios = set(avg_rewards_per_scenario_myopic.keys()).union(avg_rewards_per_scenario_proactive.keys())\n",
    "sorted_scenarios = sorted(all_scenarios)\n",
    "\n",
    "# Extract the sorted average rewards for both models\n",
    "sorted_avg_rewards_myopic = [avg_rewards_per_scenario_myopic.get(scenario, 0) for scenario in sorted_scenarios]\n",
    "sorted_avg_rewards_proactive = [avg_rewards_per_scenario_proactive.get(scenario, 0) for scenario in sorted_scenarios]\n",
    "\n",
    "# Plot a grouped bar chart with scenarios on the x-axis and average rewards on the y-axis\n",
    "x = np.arange(len(sorted_scenarios))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width/2, sorted_avg_rewards_myopic, width, label='Myopic', color='blue')\n",
    "plt.bar(x + width/2, sorted_avg_rewards_proactive, width, label='Proactive', color='orange')\n",
    "\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Scenario')\n",
    "plt.xticks(x, sorted_scenarios)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'average_reward_per_scenario.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the new folder within results_dir\n",
    "scenario_results_dir = os.path.join(results_dir, 'plots', 'reward-plots-per-scenario')\n",
    "os.makedirs(scenario_results_dir, exist_ok=True)\n",
    "\n",
    "# Get all unique scenario IDs from both models\n",
    "all_scenario_ids = set(scenario_rewards_myopic.keys()).union(scenario_rewards_proactive.keys())\n",
    "\n",
    "# Iterate over all scenarios\n",
    "for scenario_id in all_scenario_ids:\n",
    "    # Get rewards lists for both models\n",
    "    rewards_list_myopic = scenario_rewards_myopic.get(scenario_id, [])\n",
    "    rewards_list_proactive = scenario_rewards_proactive.get(scenario_id, [])\n",
    "    \n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot for Myopic model\n",
    "    if rewards_list_myopic:\n",
    "        window_myopic = min(100, len(rewards_list_myopic))\n",
    "        smoothed_rewards_myopic = np.convolve(rewards_list_myopic, np.ones(window_myopic) / window_myopic, mode='same')\n",
    "        plt.plot(rewards_list_myopic, label='Myopic Reward', color='blue', alpha=0.3)\n",
    "        plt.plot(smoothed_rewards_myopic, label='Myopic Smoothed Reward', color='blue')\n",
    "    \n",
    "    # Plot for Proactive model\n",
    "    if rewards_list_proactive:\n",
    "        window_proactive = min(100, len(rewards_list_proactive)) \n",
    "        smoothed_rewards_proactive = np.convolve(rewards_list_proactive, np.ones(window_proactive) / window_proactive, mode='same')\n",
    "        plt.plot(rewards_list_proactive, label='Proactive Reward', color='orange', alpha=0.3)\n",
    "        plt.plot(smoothed_rewards_proactive, label='Proactive Smoothed Reward', color='orange')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Reward per Episode for Scenario {scenario_id}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = f'average_reward_for_scenario_{scenario_id}.png'\n",
    "    plt.savefig(os.path.join(scenario_results_dir, plot_filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total_timesteps_proactive\", total_timesteps_proactive)\n",
    "print(\"total_timesteps_myopic\", total_timesteps_myopic)\n",
    "\n",
    "# Plot the epsilon values over the episodes for both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for Proactive model\n",
    "plt.plot(range(len(epsilon_values_proactive)), epsilon_values_proactive, label='Proactive Epsilon', color='orange')\n",
    "plt.plot(range(len(epsilon_values_myopic)), epsilon_values_myopic, label='Myopic Epsilon', color='blue')\n",
    "\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Epsilon Value')\n",
    "plt.title('Epsilon Value over Timesteps for Both Models')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_dir, 'plots', 'epsilon_value_over_timesteps_both_models.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
