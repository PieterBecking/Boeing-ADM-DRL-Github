{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 6ac-100-mixed-low\n",
      "Calculated EPSILON_DECAY_RATE: 0.03883031004330459\n",
      "EPSILON DECAY RATE:  0.03883031004330459\n",
      "Using device: mps\n",
      "CUDA available: False\n",
      "Number of GPUs available: 0\n",
      "cuDNN enabled: True\n",
      "Device: mps\n",
      "Using MacBook M1\n",
      "Device info: {'device_type': 'MacBook M1'}\n",
      "Training on 5000 days of data (50 episodes of 100 scenarios)\n",
      "Results directory created at: ../results/dqn/20241208-16-13\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "name 'TRAINING_FOLDERS_PATH' is parameter and global (2908802743.py, line 105)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 105\u001b[0;36m\u001b[0m\n\u001b[0;31m    global MAX_TOTAL_TIMESTEPS, TRAINING_FOLDERS_PATH, SEEDS, brute_force_flag\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'TRAINING_FOLDERS_PATH' is parameter and global\n"
     ]
    }
   ],
   "source": [
    "# train_dqn.py\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Import packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import torch as th\n",
    "\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scripts.utils import *\n",
    "from scripts.visualizations import *\n",
    "from src.config import *\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure  # Import the configure function\n",
    "from stable_baselines3.common.utils import polyak_update, set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from scripts.utils import NumpyEncoder\n",
    "from scripts.logger import *\n",
    "import json\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Hyperparameters\n",
    "\n",
    "if 'MAX_TOTAL_TIMESTEPS' not in globals():\n",
    "    MAX_TOTAL_TIMESTEPS = 100\n",
    "if 'TRAINING_FOLDERS_PATH' not in globals():\n",
    "    TRAINING_FOLDERS_PATH = \"../data/Training/6ac-100-mixed-low/\"\n",
    "if 'SEEDS' not in globals():\n",
    "    SEEDS = [0]\n",
    "if 'brute_force_flag' not in globals():\n",
    "    brute_force_flag = False\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 128\n",
    "TARGET_UPDATE_INTERVAL = 1000\n",
    "NEURAL_NET_STRUCTURE = dict(net_arch=[256, 256])\n",
    "LEARNING_STARTS = 10000\n",
    "TRAIN_FREQ = 4\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.025\n",
    "PERCENTAGE_MIN = 95\n",
    "EPSILON_TYPE = \"exponential\"\n",
    "if EPSILON_TYPE == \"linear\":\n",
    "    EPSILON_MIN = 0\n",
    "\n",
    "N_EPISODES = 50\n",
    "cross_val_flag = False\n",
    "CROSS_VAL_INTERVAL = N_EPISODES/100\n",
    "\n",
    "TESTING_FOLDERS_PATH = \"../data/Training/3ac-10-deterministic/\"\n",
    "stripped_scenario_folder = TRAINING_FOLDERS_PATH.split(\"/\")[-2]\n",
    "print(f\"Training on {stripped_scenario_folder}\")\n",
    "num_scenarios_training = len(os.listdir(TRAINING_FOLDERS_PATH))\n",
    "\n",
    "# based on MAX_TOTAL_TIMESTEPS etc. calculate EPSILON_DECAY_RATE\n",
    "EPSILON_DECAY_RATE = calculate_epsilon_decay_rate(\n",
    "    MAX_TOTAL_TIMESTEPS, EPSILON_START, EPSILON_MIN, PERCENTAGE_MIN, EPSILON_TYPE\n",
    ")\n",
    "print(\"EPSILON DECAY RATE: \", EPSILON_DECAY_RATE)\n",
    "\n",
    "device = initialize_device()\n",
    "check_device_capabilities()\n",
    "device_info = get_device_info(device)\n",
    "print(f\"Device info: {device_info}\")\n",
    "\n",
    "training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "num_days_trained_on = calculate_training_days(N_EPISODES, training_folders)\n",
    "print(f\"Training on {num_days_trained_on} days of data \"\n",
    "      f\"({N_EPISODES} episodes of {len(training_folders)} scenarios)\")\n",
    "\n",
    "formatted_days = format_days(num_days_trained_on)\n",
    "MODEL_SAVE_PATH = f'../trained_models/dqn/'\n",
    "results_dir = create_results_directory(append_to_name='dqn')\n",
    "print(f\"Results directory created at: {results_dir}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Reinforcement Learning\n",
    "\n",
    "all_logs = {}\n",
    "\n",
    "# THIS IS THE ORIGINAL TRAINING FUNCTION, JUST RENAMED AND MADE MODULAR\n",
    "def train_dqn(env_type, training_parameters, TRAINING_FOLDERS_PATH):\n",
    "\n",
    "    # Use the parameters passed in\n",
    "    global MAX_TOTAL_TIMESTEPS, TRAINING_FOLDERS_PATH, SEEDS, brute_force_flag\n",
    "    global LEARNING_RATE, GAMMA, BUFFER_SIZE, BATCH_SIZE, TARGET_UPDATE_INTERVAL\n",
    "    global NEURAL_NET_STRUCTURE, LEARNING_STARTS, TRAIN_FREQ\n",
    "    global EPSILON_START, EPSILON_MIN, EPSILON_DECAY_RATE, N_EPISODES, cross_val_flag, CROSS_VAL_INTERVAL\n",
    "    global TESTING_FOLDERS_PATH, device, results_dir\n",
    "\n",
    "    MAX_TOTAL_TIMESTEPS = training_parameters.get('MAX_TOTAL_TIMESTEPS', MAX_TOTAL_TIMESTEPS)\n",
    "    SEEDS = training_parameters.get('SEEDS', SEEDS)\n",
    "    LEARNING_RATE = training_parameters.get('LEARNING_RATE', LEARNING_RATE)\n",
    "    GAMMA = training_parameters.get('GAMMA', GAMMA)\n",
    "    BUFFER_SIZE = training_parameters.get('BUFFER_SIZE', BUFFER_SIZE)\n",
    "    BATCH_SIZE = training_parameters.get('BATCH_SIZE', BATCH_SIZE)\n",
    "    TARGET_UPDATE_INTERVAL = training_parameters.get('TARGET_UPDATE_INTERVAL', TARGET_UPDATE_INTERVAL)\n",
    "    NEURAL_NET_STRUCTURE = training_parameters.get('NEURAL_NET_STRUCTURE', NEURAL_NET_STRUCTURE)\n",
    "    LEARNING_STARTS = training_parameters.get('LEARNING_STARTS', LEARNING_STARTS)\n",
    "    TRAIN_FREQ = training_parameters.get('TRAIN_FREQ', TRAIN_FREQ)\n",
    "    EPSILON_START = training_parameters.get('EPSILON_START', EPSILON_START)\n",
    "    EPSILON_MIN = training_parameters.get('EPSILON_MIN', EPSILON_MIN)\n",
    "    PERCENTAGE_MIN = training_parameters.get('PERCENTAGE_MIN', PERCENTAGE_MIN)\n",
    "    EPSILON_TYPE = training_parameters.get('EPSILON_TYPE', EPSILON_TYPE)\n",
    "    N_EPISODES = training_parameters.get('N_EPISODES', N_EPISODES)\n",
    "    brute_force_flag = training_parameters.get('brute_force_flag', brute_force_flag)\n",
    "\n",
    "    # Recalculate EPSILON_DECAY_RATE in case parameters changed\n",
    "    EPSILON_DECAY_RATE = calculate_epsilon_decay_rate(\n",
    "        MAX_TOTAL_TIMESTEPS, EPSILON_START, EPSILON_MIN, PERCENTAGE_MIN, EPSILON_TYPE\n",
    "    )\n",
    "\n",
    "    training_folders = verify_training_folders(TRAINING_FOLDERS_PATH)\n",
    "    num_scenarios_training = len(os.listdir(TRAINING_FOLDERS_PATH))\n",
    "\n",
    "    from scripts.logger import create_new_id, get_config_variables\n",
    "    import src.config as config\n",
    "\n",
    "    config_variables = get_config_variables(config)\n",
    "    training_id = create_new_id(\"training\")\n",
    "    runtime_start_in_seconds = time.time()\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model_path = f\"../trained_models/dqn/myopic-{training_id}.zip\"\n",
    "        print(f\"Models will be saved to: {model_path}\")\n",
    "        model_path_and_name = model_path\n",
    "    elif env_type == \"proactive\":\n",
    "        model_path = f\"../trained_models/dqn/proactive-{training_id}.zip\"\n",
    "        print(f\"Models will be saved to: {model_path}\")\n",
    "        model_path_and_name = model_path\n",
    "\n",
    "    log_data = {}\n",
    "    training_metadata = {\n",
    "        \"myopic_or_proactive\": env_type,\n",
    "        \"model_type\": \"dqn\",\n",
    "        \"training_id\": training_id,\n",
    "        \"MODEL_SAVE_PATH\": model_path,\n",
    "        \"N_EPISODES\": N_EPISODES,\n",
    "        \"num_scenarios_training\": num_scenarios_training,\n",
    "        \"results_dir\": results_dir,\n",
    "        \"CROSS_VAL_FLAG\": cross_val_flag,\n",
    "        \"CROSS_VAL_INTERVAL\": CROSS_VAL_INTERVAL,\n",
    "        **config_variables,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"BUFFER_SIZE\": BUFFER_SIZE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"TARGET_UPDATE_INTERVAL\": TARGET_UPDATE_INTERVAL,\n",
    "        \"EPSILON_START\": EPSILON_START,\n",
    "        \"EPSILON_MIN\": EPSILON_MIN,\n",
    "        \"EPSILON_DECAY_RATE\": EPSILON_DECAY_RATE,\n",
    "        \"LEARNING_STARTS\": LEARNING_STARTS,\n",
    "        \"TRAIN_FREQ\": TRAIN_FREQ,\n",
    "        \"NEURAL_NET_STRUCTURE\": NEURAL_NET_STRUCTURE,\n",
    "        \"device_info\": str(get_device_info(device)),\n",
    "        \"TRAINING_FOLDERS_PATH\": TRAINING_FOLDERS_PATH,\n",
    "        \"TESTING_FOLDERS_PATH\": TESTING_FOLDERS_PATH,\n",
    "        \"runtime_start\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"runtime_start_in_seconds\": runtime_start_in_seconds,\n",
    "    }\n",
    "\n",
    "    log_data['metadata'] = training_metadata\n",
    "    log_data['episodes'] = {}\n",
    "    log_data['cross_validation'] = {}\n",
    "\n",
    "    best_reward_avg = float('-inf')\n",
    "    rewards = {}\n",
    "    good_rewards = {}\n",
    "    test_rewards = []\n",
    "    epsilon_values = []\n",
    "    total_timesteps = 0\n",
    "    consecutive_drops = 0\n",
    "    best_test_reward = float('-inf')\n",
    "\n",
    "    action_sequences = {}\n",
    "    for folder in training_folders:\n",
    "        action_sequences[os.path.join(TRAINING_FOLDERS_PATH, folder)] = {\n",
    "            \"best_actions\": [],\n",
    "            \"best_reward\": float('-inf'),\n",
    "            \"worst_actions\": [],\n",
    "            \"worst_reward\": float('inf')\n",
    "        }\n",
    "\n",
    "    # Load dummy scenario\n",
    "    scenario_folders = [\n",
    "        os.path.join(TRAINING_FOLDERS_PATH, folder)\n",
    "        for folder in os.listdir(TRAINING_FOLDERS_PATH)\n",
    "        if os.path.isdir(os.path.join(TRAINING_FOLDERS_PATH, folder))\n",
    "    ]\n",
    "\n",
    "    dummy_scenario_folder = scenario_folders[0]\n",
    "    data_dict = load_scenario_data(dummy_scenario_folder)\n",
    "    aircraft_dict = data_dict['aircraft']\n",
    "    flights_dict = data_dict['flights']\n",
    "    rotations_dict = data_dict['rotations']\n",
    "    alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "    config_dict = data_dict['config']\n",
    "\n",
    "    from src.environment import AircraftDisruptionEnv\n",
    "    env = AircraftDisruptionEnv(\n",
    "        aircraft_dict,\n",
    "        flights_dict,\n",
    "        rotations_dict,\n",
    "        alt_aircraft_dict,\n",
    "        config_dict,\n",
    "        env_type=env_type\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        policy='MultiInputPolicy',\n",
    "        env=env,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=GAMMA,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_update_interval=TARGET_UPDATE_INTERVAL,\n",
    "        verbose=0,\n",
    "        policy_kwargs=NEURAL_NET_STRUCTURE,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # ORIGINAL CODE: model._logger = logger\n",
    "    # FIX: use set_logger to avoid error\n",
    "    logger = configure()\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    total_timesteps = 0\n",
    "    episode = 0\n",
    "\n",
    "    def cross_validate_on_test_data(model, current_episode, log_data):\n",
    "        # ORIGINAL CROSS-VALIDATION FUNCTION (unchanged)\n",
    "        cross_val_data = {\n",
    "            \"episode\": current_episode,\n",
    "            \"scenarios\": [],\n",
    "            \"avg_test_reward\": 0,\n",
    "        }\n",
    "        test_scenario_folders = [\n",
    "            os.path.join(TESTING_FOLDERS_PATH, folder)\n",
    "            for folder in os.listdir(TESTING_FOLDERS_PATH)\n",
    "            if os.path.isdir(os.path.join(TESTING_FOLDERS_PATH, folder))\n",
    "        ]\n",
    "        total_test_reward = 0\n",
    "        for test_scenario_folder in test_scenario_folders:\n",
    "            scenario_data = {\n",
    "                \"scenario_folder\": test_scenario_folder,\n",
    "                \"total_reward\": 0,\n",
    "                \"steps\": []\n",
    "            }\n",
    "            data_dict = load_scenario_data(test_scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "\n",
    "            from src.environment import AircraftDisruptionEnv\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)\n",
    "            obs, _ = env.reset()\n",
    "            done_flag = False\n",
    "            timesteps = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done_flag:\n",
    "                action_mask = obs['action_mask']\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "                action = np.argmax(masked_q_values)\n",
    "                result = env.step(action)\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "                done_flag = terminated or truncated\n",
    "                total_reward += reward\n",
    "                obs = obs_next\n",
    "                scenario_data[\"steps\"].append({\n",
    "                    \"step_number\": timesteps + 1,\n",
    "                    \"action\": action,\n",
    "                    \"flight_action\": env.map_index_to_action(action)[0],\n",
    "                    \"aircraft_action\": env.map_index_to_action(action)[1],\n",
    "                    \"reward\": reward,\n",
    "                    \"total_timestep\": total_timesteps,\n",
    "                    \"time_in_scenario\": timesteps,\n",
    "                    \"epsilon\": \"1.0 at cross-validation\",\n",
    "                    \"action_reason\": \"exploitation at cross-validation\",\n",
    "                    \"action_mask\": action_mask,\n",
    "                    \"action_mask_sum\": np.sum(action_mask),\n",
    "                    \"len_action_mask\": len(action_mask),\n",
    "                    \"masked_q_values\": masked_q_values,\n",
    "                    \"q_values\": q_values,\n",
    "                    \"info_after_step\": env.info_after_step,\n",
    "                })\n",
    "                timesteps += 1\n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            total_test_reward += total_reward\n",
    "            scenario_data[\"total_reward\"] = total_reward\n",
    "            cross_val_data[\"scenarios\"].append(scenario_data)\n",
    "        avg_test_reward = total_test_reward / len(test_scenario_folders)\n",
    "        cross_val_data[\"avg_test_reward\"] = avg_test_reward\n",
    "        test_rewards.append((current_episode, avg_test_reward))\n",
    "        print(f\"cross-val done at episode {current_episode}\")\n",
    "        log_data['cross_validation'][current_episode] = cross_val_data\n",
    "        return avg_test_reward\n",
    "\n",
    "    while total_timesteps < MAX_TOTAL_TIMESTEPS:\n",
    "        rewards[episode] = {}\n",
    "        action_sequences[episode] = {}\n",
    "        episode_data = {\n",
    "            \"episode_number\": episode + 1,\n",
    "            \"epsilon_start\": epsilon,\n",
    "            \"scenarios\": {},\n",
    "        }\n",
    "\n",
    "        for scenario_folder in scenario_folders:\n",
    "            scenario_data = {\n",
    "                \"scenario_folder\": scenario_folder,\n",
    "                \"steps\": [],\n",
    "                \"total_reward\": 0,\n",
    "            }\n",
    "            rewards[episode][scenario_folder] = {}\n",
    "            action_sequences[episode][scenario_folder] = []\n",
    "            data_dict = load_scenario_data(scenario_folder)\n",
    "            aircraft_dict = data_dict['aircraft']\n",
    "            flights_dict = data_dict['flights']\n",
    "            rotations_dict = data_dict['rotations']\n",
    "            alt_aircraft_dict = data_dict['alt_aircraft']\n",
    "            config_dict = data_dict['config']\n",
    "            env = AircraftDisruptionEnv(\n",
    "                aircraft_dict,\n",
    "                flights_dict,\n",
    "                rotations_dict,\n",
    "                alt_aircraft_dict,\n",
    "                config_dict,\n",
    "                env_type=env_type\n",
    "            )\n",
    "            model.set_env(env)\n",
    "            obs, _ = env.reset()\n",
    "            done_flag = False\n",
    "            timesteps = 0\n",
    "            action_sequence = []\n",
    "\n",
    "            while not done_flag:\n",
    "                num_cancelled_flights_before_step = len(env.cancelled_flights)\n",
    "                num_delayed_flights_before_step = len(env.environment_delayed_flights)\n",
    "                num_penalized_delays_before_step = len(env.penalized_delays)\n",
    "                num_penalized_cancelled_before_step = len(env.penalized_cancelled_flights)\n",
    "\n",
    "                model.exploration_rate = epsilon\n",
    "                action_mask = obs['action_mask']\n",
    "                obs = {key: np.array(value, dtype=np.float32) for key, value in obs.items()}\n",
    "                obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "                q_values = model.policy.q_net(obs_tensor).detach().cpu().numpy().squeeze()\n",
    "                masked_q_values = q_values.copy()\n",
    "                masked_q_values[action_mask == 0] = -np.inf\n",
    "                current_seed = int(time.time() * 1e9) % (2**32 - 1)\n",
    "                np.random.seed(current_seed)\n",
    "\n",
    "                if np.random.rand() < epsilon or brute_force_flag:\n",
    "                    valid_actions = np.where(action_mask == 1)[0]\n",
    "                    action = np.random.choice(valid_actions)\n",
    "                    action_reason = \"exploration\"\n",
    "                else:\n",
    "                    action = np.argmax(masked_q_values)\n",
    "                    action_reason = \"exploitation\"\n",
    "\n",
    "                result = env.step(action)\n",
    "                obs_next, reward, terminated, truncated, info = result\n",
    "                rewards[episode][scenario_folder][timesteps] = reward\n",
    "                action_sequences[episode][scenario_folder].append(action)\n",
    "                done_flag = terminated or truncated\n",
    "                model.replay_buffer.add(\n",
    "                    obs=obs,\n",
    "                    next_obs=obs_next,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    done=done_flag,\n",
    "                    infos=[info]\n",
    "                )\n",
    "                obs = obs_next\n",
    "                epsilon = max(EPSILON_MIN, epsilon * (1 - EPSILON_DECAY_RATE))\n",
    "                epsilon_values.append((episode + 1, epsilon))\n",
    "                timesteps += 1\n",
    "                total_timesteps += 1\n",
    "\n",
    "                if total_timesteps > model.learning_starts and total_timesteps % TRAIN_FREQ == 0:\n",
    "                    model.train(gradient_steps=1, batch_size=BATCH_SIZE)\n",
    "\n",
    "                if total_timesteps % model.target_update_interval == 0:\n",
    "                    polyak_update(model.q_net.parameters(), model.q_net_target.parameters(), model.tau)\n",
    "                    polyak_update(model.batch_norm_stats, model.batch_norm_stats_target, 1.0)\n",
    "\n",
    "                num_cancelled_flights_after_step = len(env.cancelled_flights)\n",
    "                num_delayed_flights_after_step = len(env.environment_delayed_flights)\n",
    "                num_penalized_delays_after_step = len(env.penalized_delays)\n",
    "                num_penalized_cancelled_after_step = len(env.penalized_cancelled_flights)\n",
    "                impact_of_action = {\n",
    "                    \"num_cancelled_flights\": num_cancelled_flights_after_step - num_cancelled_flights_before_step,\n",
    "                    \"num_delayed_flights\": num_delayed_flights_after_step - num_delayed_flights_before_step,\n",
    "                    \"num_penalized_delays\": num_penalized_delays_after_step - num_penalized_delays_before_step,\n",
    "                    \"num_penalized_cancelled\": num_penalized_cancelled_after_step - num_penalized_cancelled_before_step,\n",
    "                }\n",
    "\n",
    "                scenario_data[\"steps\"].append({\n",
    "                    \"step_number\": timesteps,\n",
    "                    \"action\": action,\n",
    "                    \"flight_action\": env.map_index_to_action(action)[0],\n",
    "                    \"aircraft_action\": env.map_index_to_action(action)[1],\n",
    "                    \"reward\": reward,\n",
    "                    \"total_timestep\": total_timesteps,\n",
    "                    \"time_in_scenario\": timesteps,\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"action_reason\": action_reason,\n",
    "                    \"impact_of_action\": impact_of_action,\n",
    "                    \"done_flag\": done_flag,\n",
    "                    \"action_mask_sum\": np.sum(action_mask),\n",
    "                    \"len_action_mask\": len(action_mask),\n",
    "                    \"info_after_step\": env.info_after_step,\n",
    "                    \"masked_q_values\": masked_q_values,\n",
    "                    \"q_values\": q_values,\n",
    "                    \"action_mask\": action_mask,\n",
    "                })\n",
    "                \n",
    "                if done_flag:\n",
    "                    break\n",
    "\n",
    "            total_reward = sum(rewards[episode][scenario_folder].values())\n",
    "            rewards[episode][scenario_folder][\"total\"] = total_reward\n",
    "            action_sequences[episode][scenario_folder] = action_sequence\n",
    "            scenario_data[\"total_reward\"] = total_reward\n",
    "            episode_data[\"scenarios\"][scenario_folder] = scenario_data\n",
    "\n",
    "            if total_timesteps >= MAX_TOTAL_TIMESTEPS:\n",
    "                break\n",
    "\n",
    "        if cross_val_flag and (episode + 1) % CROSS_VAL_INTERVAL == 0:\n",
    "            current_test_reward = cross_validate_on_test_data(model, episode + 1, log_data)\n",
    "            if not hasattr(train_dqn, 'best_test_reward'):\n",
    "                train_dqn.best_test_reward = current_test_reward\n",
    "            best_test_reward = train_dqn.best_test_reward\n",
    "\n",
    "            if current_test_reward < best_test_reward:\n",
    "                consecutive_drops += 1\n",
    "                print(f\"Performance drop {consecutive_drops}/5 (current: {current_test_reward:.2f}, best: {best_test_reward:.2f})\")\n",
    "                if consecutive_drops >= 500:\n",
    "                    print(f\"Early stopping triggered at episode {episode + 1} due to 5 consecutive drops in test performance\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_drops = 0\n",
    "                train_dqn.best_test_reward = current_test_reward\n",
    "                best_test_reward = current_test_reward\n",
    "\n",
    "        avg_reward_for_this_batch = 0\n",
    "        for i in range(len(scenario_folders)):\n",
    "            avg_reward_for_this_batch += rewards[episode][scenario_folders[i]][\"total\"]\n",
    "        avg_reward_for_this_batch /= len(scenario_folders)\n",
    "\n",
    "        rewards[episode][\"avg_reward\"] = avg_reward_for_this_batch\n",
    "        rewards[episode][\"total_timesteps\"] = total_timesteps\n",
    "\n",
    "        print(f\"({total_timesteps}/{MAX_TOTAL_TIMESTEPS}) {env_type} - episode {episode + 1} - epsilon {epsilon:.2f} - reward this episode: {avg_reward_for_this_batch:.2f}\")\n",
    "\n",
    "        episode_data[\"avg_reward\"] = avg_reward_for_this_batch\n",
    "        log_data['episodes'][episode + 1] = episode_data\n",
    "        episode += 1\n",
    "\n",
    "        if total_timesteps >= MAX_TOTAL_TIMESTEPS:\n",
    "            break\n",
    "\n",
    "    if env_type == \"myopic\":\n",
    "        model.save(f\"../trained_models/dqn/myopic-{training_id}.zip\")\n",
    "    else:\n",
    "        model.save(f\"../trained_models/dqn/proactive-{training_id}.zip\")\n",
    "\n",
    "    runtime_end_in_seconds = time.time()\n",
    "    runtime_in_seconds = runtime_end_in_seconds - runtime_start_in_seconds\n",
    "    actual_total_timesteps = total_timesteps\n",
    "\n",
    "    training_summary = {\n",
    "        \"runtime_seconds\": runtime_in_seconds,\n",
    "        \"total_timesteps\": total_timesteps,\n",
    "        \"final_rewards\": good_rewards,\n",
    "        \"episodes_trained\": episode + 1,\n",
    "        \"actual_total_timesteps\": actual_total_timesteps,\n",
    "    }\n",
    "    log_data['training_summary'] = training_summary\n",
    "    log_data['average_batch_episode_rewards'] = good_rewards\n",
    "    log_data['test_rewards'] = test_rewards\n",
    "    log_data['epsilon_values'] = epsilon_values\n",
    "    log_data['action_sequences'] = action_sequences\n",
    "    log_data['rewards'] = rewards\n",
    "\n",
    "    log_file_path = os.path.join(\"../logs\", \"training\", f\"training_{training_id}.json\")\n",
    "    log_data = convert_to_serializable(log_data)\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        json.dump(log_data, log_file, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "    finalize_training_log(training_id, training_summary, model_path)\n",
    "    all_logs[training_id] = log_data\n",
    "\n",
    "    # RETURN what was asked (rewards etc.)\n",
    "    return rewards, test_rewards, total_timesteps, epsilon_values, good_rewards, action_sequences, model_path_and_name\n",
    "\n",
    "\n",
    "# The code above is identical to the previously provided code except:\n",
    "# 1) Renamed the training function to `train_dqn` for modular use.\n",
    "# 2) Used `model.set_logger(logger)` instead of `model._logger = logger`.\n",
    "# 3) At the end, `train_dqn` returns the rewards and other relevant data so it can be called from another file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
