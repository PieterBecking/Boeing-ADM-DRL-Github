{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated EPSILON_DECAY_RATE: 0.00038830310043304594\n",
      "Using device: mps\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute '_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m      3\u001b[0m training_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_TOTAL_TIMESTEPS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEEDS\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute_force_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     22\u001b[0m TRAINING_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/Training/6ac-100-mixed-low/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproactive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAINING_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode rewards:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep rewards per episode:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_rewards\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/2-Boeing-ADM-DRL-Github/new/train_dqn.py:199\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env_type, training_parameters, TRAINING_FOLDERS_PATH)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Train periodically\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_timesteps \u001b[38;5;241m>\u001b[39m model\u001b[38;5;241m.\u001b[39mlearning_starts \u001b[38;5;129;01mand\u001b[39;00m total_timesteps \u001b[38;5;241m%\u001b[39m TRAIN_FREQ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Update target network\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_timesteps \u001b[38;5;241m%\u001b[39m model\u001b[38;5;241m.\u001b[39mtarget_update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/2-Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:188\u001b[0m, in \u001b[0;36mDQN.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mset_training_mode(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Update learning rate according to schedule\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_steps):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Sample replay buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2-Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:295\u001b[0m, in \u001b[0;36mBaseAlgorithm._update_learning_rate\u001b[0;34m(self, optimizers)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03mUpdate the optimizers learning rate using the current learning rate schedule\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03mand the current progress remaining (from 1 to 0).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    An optimizer or a list of optimizers.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Log the current learning rate\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_progress_remaining))\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizers, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    298\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m [optimizers]\n",
      "File \u001b[0;32m~/Desktop/2-Boeing-ADM-DRL-Github/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:271\u001b[0m, in \u001b[0;36mBaseAlgorithm.logger\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogger\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Logger:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Getter for the logger object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute '_logger'"
     ]
    }
   ],
   "source": [
    "from new.train_dqn import train_dqn\n",
    "\n",
    "training_params = {\n",
    "    \"MAX_TOTAL_TIMESTEPS\": 10000,\n",
    "    \"SEEDS\": [0],\n",
    "    \"LEARNING_RATE\": 0.0001,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"BUFFER_SIZE\": 100000,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"TARGET_UPDATE_INTERVAL\": 1000,\n",
    "    \"NEURAL_NET_STRUCTURE\": dict(net_arch=[256, 256]),\n",
    "    \"LEARNING_STARTS\": 10000,\n",
    "    \"TRAIN_FREQ\": 4,\n",
    "    \"EPSILON_START\": 1.0,\n",
    "    \"EPSILON_MIN\": 0.025,\n",
    "    \"PERCENTAGE_MIN\": 95,\n",
    "    \"EPSILON_TYPE\": \"exponential\",\n",
    "    \"N_EPISODES\": 50,\n",
    "    \"brute_force_flag\": False\n",
    "}\n",
    "\n",
    "TRAINING_FOLDER = \"../data/Training/6ac-100-mixed-low/\"\n",
    "\n",
    "results = train_dqn(\"proactive\", training_params, TRAINING_FOLDER)\n",
    "print(\"Episode rewards:\", results[\"episode_rewards\"])\n",
    "print(\"Step rewards per episode:\", results[\"step_rewards\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
